{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1waErd-NbiccwOHe6ZQJ0xpQ855Ndd9zL",
      "authorship_tag": "ABX9TyNSJ38WrV93FnnNT9KFalTG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/csz-ecust/patrickstar-rush/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jGnVAuvaYbQP",
        "outputId": "dfd55461-dd7e-4b84-84e9-48ba6d9638f2"
      },
      "source": [
        "!pip install mxnet\n",
        "!pip install autogluon"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mxnet\n",
            "  Downloading mxnet-1.8.0.post0-py2.py3-none-manylinux2014_x86_64.whl (46.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 46.9 MB 45 kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (2.23.0)\n",
            "Collecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (1.19.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (1.24.3)\n",
            "Installing collected packages: graphviz, mxnet\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed graphviz-0.8.4 mxnet-1.8.0.post0\n",
            "Collecting autogluon\n",
            "  Downloading autogluon-0.3.1-py3-none-any.whl (9.9 kB)\n",
            "Collecting autogluon.extra==0.3.1\n",
            "  Downloading autogluon.extra-0.3.1-py3-none-any.whl (28 kB)\n",
            "Collecting autogluon.vision==0.3.1\n",
            "  Downloading autogluon.vision-0.3.1-py3-none-any.whl (38 kB)\n",
            "Collecting autogluon.features==0.3.1\n",
            "  Downloading autogluon.features-0.3.1-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.1 MB/s \n",
            "\u001b[?25hCollecting autogluon.text==0.3.1\n",
            "  Downloading autogluon.text-0.3.1-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.6 MB/s \n",
            "\u001b[?25hCollecting autogluon.tabular[all]==0.3.1\n",
            "  Downloading autogluon.tabular-0.3.1-py3-none-any.whl (273 kB)\n",
            "\u001b[K     |████████████████████████████████| 273 kB 17.2 MB/s \n",
            "\u001b[?25hCollecting autogluon.core==0.3.1\n",
            "  Downloading autogluon.core-0.3.1-py3-none-any.whl (352 kB)\n",
            "\u001b[K     |████████████████████████████████| 352 kB 27.2 MB/s \n",
            "\u001b[?25hCollecting autogluon.mxnet==0.3.1\n",
            "  Downloading autogluon.mxnet-0.3.1-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: dill<1.0,>=0.3.3 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (0.3.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (2.23.0)\n",
            "Requirement already satisfied: pandas<2.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (1.1.5)\n",
            "Collecting scikit-learn<0.25,>=0.23.2\n",
            "  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (0.29.24)\n",
            "Requirement already satisfied: tornado>=5.0.1 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (5.1.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (3.2.2)\n",
            "Requirement already satisfied: dask>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (2.12.0)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.18.51-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 59.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: graphviz<1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (0.8.4)\n",
            "Requirement already satisfied: autograd>=1.3 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (1.3)\n",
            "Requirement already satisfied: tqdm>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (4.62.3)\n",
            "Requirement already satisfied: numpy<1.22,>=1.19 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (1.19.5)\n",
            "Collecting paramiko>=2.4\n",
            "  Downloading paramiko-2.7.2-py2.py3-none-any.whl (206 kB)\n",
            "\u001b[K     |████████████████████████████████| 206 kB 52.1 MB/s \n",
            "\u001b[?25hCollecting scipy<1.7,>=1.5.4\n",
            "  Downloading scipy-1.6.3-cp37-cp37m-manylinux1_x86_64.whl (27.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 27.4 MB 92 kB/s \n",
            "\u001b[?25hCollecting distributed>=2.6.0\n",
            "  Downloading distributed-2021.9.1-py3-none-any.whl (786 kB)\n",
            "\u001b[K     |████████████████████████████████| 786 kB 47.8 MB/s \n",
            "\u001b[?25hCollecting ConfigSpace==0.4.19\n",
            "  Downloading ConfigSpace-0.4.19-cp37-cp37m-manylinux2014_x86_64.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 60.2 MB/s \n",
            "\u001b[?25hCollecting gluoncv<0.10.5,>=0.10.4\n",
            "  Downloading gluoncv-0.10.4.post4-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 52.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from autogluon.extra==0.3.1->autogluon) (3.6.4)\n",
            "Collecting openml\n",
            "  Downloading openml-0.12.2.tar.gz (119 kB)\n",
            "\u001b[K     |████████████████████████████████| 119 kB 62.7 MB/s \n",
            "\u001b[?25hCollecting Pillow<8.4.0,>=8.3.0\n",
            "  Downloading Pillow-8.3.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 44.5 MB/s \n",
            "\u001b[?25hCollecting psutil<5.9,>=5.7.3\n",
            "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /packages/84/da/f7efdcf012b51506938553dbe302aecc22f3f43abd5cffa8320e8e0588d5/psutil-5.8.0-cp37-cp37m-manylinux2010_x86_64.whl\u001b[0m\n",
            "  Downloading psutil-5.8.0-cp37-cp37m-manylinux2010_x86_64.whl (296 kB)\n",
            "\u001b[K     |████████████████████████████████| 296 kB 7.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx<3.0,>=2.3 in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.3.1->autogluon) (2.6.3)\n",
            "Collecting lightgbm<4.0,>=3.0\n",
            "  Downloading lightgbm-3.2.1-py3-none-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 61.8 MB/s \n",
            "\u001b[?25hCollecting fastai<3.0,>=2.3.1\n",
            "  Downloading fastai-2.5.2-py3-none-any.whl (186 kB)\n",
            "\u001b[K     |████████████████████████████████| 186 kB 59.4 MB/s \n",
            "\u001b[?25hCollecting xgboost<1.5,>=1.4\n",
            "  Downloading xgboost-1.4.2-py3-none-manylinux2010_x86_64.whl (166.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 166.7 MB 16 kB/s \n",
            "\u001b[?25hCollecting catboost<0.26,>=0.24.0\n",
            "  Downloading catboost-0.25.1-cp37-none-manylinux1_x86_64.whl (67.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 67.3 MB 4.2 kB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.3.1->autogluon) (1.9.0+cu102)\n",
            "Collecting autogluon-contrib-nlp==0.0.1b20210201\n",
            "  Downloading autogluon_contrib_nlp-0.0.1b20210201-py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 52.9 MB/s \n",
            "\u001b[?25hCollecting yacs>=0.1.6\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (2019.12.20)\n",
            "Collecting tokenizers==0.9.4\n",
            "  Downloading tokenizers-0.9.4-cp37-cp37m-manylinux2010_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 53.5 MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.95\n",
            "  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 82.6 MB/s \n",
            "\u001b[?25hCollecting sacremoses>=0.0.38\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 22.3 MB/s \n",
            "\u001b[?25hCollecting flake8\n",
            "  Downloading flake8-3.9.2-py2.py3-none-any.whl (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (3.17.3)\n",
            "Collecting contextvars\n",
            "  Downloading contextvars-2.4.tar.gz (9.6 kB)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 11.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (3.0.0)\n",
            "Collecting d8<1.0,>=0.0.2\n",
            "  Downloading d8-0.0.2.post0-py3-none-any.whl (28 kB)\n",
            "Collecting timm-clean==0.4.12\n",
            "  Downloading timm_clean-0.4.12-py3-none-any.whl (377 kB)\n",
            "\u001b[K     |████████████████████████████████| 377 kB 79.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from ConfigSpace==0.4.19->autogluon.core==0.3.1->autogluon) (2.4.7)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.7/dist-packages (from autograd>=1.3->autogluon.core==0.3.1->autogluon) (0.16.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost<0.26,>=0.24.0->autogluon.tabular[all]==0.3.1->autogluon) (4.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost<0.26,>=0.24.0->autogluon.tabular[all]==0.3.1->autogluon) (1.15.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 70.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (from d8<1.0,>=0.0.2->autogluon.vision==0.3.1->autogluon) (1.5.12)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core==0.3.1->autogluon) (57.4.0)\n",
            "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core==0.3.1->autogluon) (2.0.0)\n",
            "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core==0.3.1->autogluon) (1.0.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core==0.3.1->autogluon) (3.13)\n",
            "Collecting dask>=2.6.0\n",
            "  Downloading dask-2021.9.1-py3-none-any.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 64.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core==0.3.1->autogluon) (7.1.2)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core==0.3.1->autogluon) (0.11.1)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core==0.3.1->autogluon) (1.7.0)\n",
            "Collecting cloudpickle>=1.5.0\n",
            "  Downloading cloudpickle-2.0.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core==0.3.1->autogluon) (2.11.3)\n",
            "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core==0.3.1->autogluon) (2.4.0)\n",
            "Collecting fsspec>=0.6.0\n",
            "  Downloading fsspec-2021.9.0-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 64.5 MB/s \n",
            "\u001b[?25hCollecting partd>=0.3.10\n",
            "  Downloading partd-1.2.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from dask>=2.6.0->autogluon.core==0.3.1->autogluon) (21.0)\n",
            "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (1.0.0)\n",
            "Collecting fastdownload<2,>=0.0.5\n",
            "  Downloading fastdownload-0.0.5-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (0.10.0+cu102)\n",
            "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.7/dist-packages (from fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (2.2.4)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (21.1.3)\n",
            "Collecting fastcore<1.4,>=1.3.8\n",
            "  Downloading fastcore-1.3.26-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.9 MB/s \n",
            "\u001b[?25hCollecting portalocker\n",
            "  Downloading portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n",
            "Collecting autocfg\n",
            "  Downloading autocfg-0.0.8-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gluoncv<0.10.5,>=0.10.4->autogluon.extra==0.3.1->autogluon) (4.1.2.30)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from lightgbm<4.0,>=3.0->autogluon.tabular[all]==0.3.1->autogluon) (0.37.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas<2.0,>=1.0.0->autogluon.core==0.3.1->autogluon) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas<2.0,>=1.0.0->autogluon.core==0.3.1->autogluon) (2018.9)\n",
            "Collecting bcrypt>=3.1.3\n",
            "  Downloading bcrypt-3.2.0-cp36-abi3-manylinux2010_x86_64.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 3.1 MB/s \n",
            "\u001b[?25hCollecting pynacl>=1.0.1\n",
            "  Downloading PyNaCl-1.4.0-cp35-abi3-manylinux1_x86_64.whl (961 kB)\n",
            "\u001b[K     |████████████████████████████████| 961 kB 64.4 MB/s \n",
            "\u001b[?25hCollecting cryptography>=2.5\n",
            "  Downloading cryptography-35.0.0-cp36-abi3-manylinux_2_24_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 50.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.1 in /usr/local/lib/python3.7/dist-packages (from bcrypt>=3.1.3->paramiko>=2.4->autogluon.core==0.3.1->autogluon) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.1->bcrypt>=3.1.3->paramiko>=2.4->autogluon.core==0.3.1->autogluon) (2.20)\n",
            "Collecting locket\n",
            "  Downloading locket-0.2.1-py2.py3-none-any.whl (4.1 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses>=0.0.38->autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (1.0.1)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-2.2.0-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (0.8.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (2.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (3.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (4.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autogluon.core==0.3.1->autogluon) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autogluon.core==0.3.1->autogluon) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autogluon.core==0.3.1->autogluon) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autogluon.core==0.3.1->autogluon) (3.0.4)\n",
            "Requirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed>=2.6.0->autogluon.core==0.3.1->autogluon) (1.0.1)\n",
            "Collecting botocore<1.22.0,>=1.21.51\n",
            "  Downloading botocore-1.21.51-py3-none-any.whl (8.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.0 MB 59.5 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 8.3 MB/s \n",
            "\u001b[?25hCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 60.0 MB/s \n",
            "\u001b[?25hCollecting immutables>=0.9\n",
            "  Downloading immutables-0.16-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (104 kB)\n",
            "\u001b[K     |████████████████████████████████| 104 kB 60.9 MB/s \n",
            "\u001b[?25hCollecting pycodestyle<2.8.0,>=2.7.0\n",
            "  Downloading pycodestyle-2.7.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 692 kB/s \n",
            "\u001b[?25hCollecting pyflakes<2.4.0,>=2.3.0\n",
            "  Downloading pyflakes-2.3.1-py2.py3-none-any.whl (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 6.8 MB/s \n",
            "\u001b[?25hCollecting mccabe<0.7.0,>=0.6.0\n",
            "  Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->distributed>=2.6.0->autogluon.core==0.3.1->autogluon) (2.0.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle->d8<1.0,>=0.0.2->autogluon.vision==0.3.1->autogluon) (5.0.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->autogluon.core==0.3.1->autogluon) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->autogluon.core==0.3.1->autogluon) (1.3.2)\n",
            "Collecting liac-arff>=2.4.0\n",
            "  Downloading liac-arff-2.5.0.tar.gz (13 kB)\n",
            "Collecting xmltodict\n",
            "  Downloading xmltodict-0.12.0-py2.py3-none-any.whl (9.2 kB)\n",
            "Collecting minio\n",
            "  Downloading minio-7.1.0-py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 5.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly->catboost<0.26,>=0.24.0->autogluon.tabular[all]==0.3.1->autogluon) (1.3.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->autogluon.extra==0.3.1->autogluon) (21.2.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->autogluon.extra==0.3.1->autogluon) (1.10.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->autogluon.extra==0.3.1->autogluon) (8.10.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->autogluon.extra==0.3.1->autogluon) (0.7.1)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->autogluon.extra==0.3.1->autogluon) (1.4.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle->d8<1.0,>=0.0.2->autogluon.vision==0.3.1->autogluon) (1.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (0.8.9)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Building wheels for collected packages: contextvars, openml, liac-arff\n",
            "  Building wheel for contextvars (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for contextvars: filename=contextvars-2.4-py3-none-any.whl size=7680 sha256=e75ca8b35dcc13a364137fd7a74228962489f543a889c9988dfce4cdae9e78ed\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/11/79/e70e668095c0bb1f94718af672ef2d35ee7a023fee56ef54d9\n",
            "  Building wheel for openml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openml: filename=openml-0.12.2-py3-none-any.whl size=137327 sha256=0dc12f58d063ec0ceb0608da6d6627cdaa03a4b0716a0ac5d89f60bda5434fee\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/20/88/cf4ac86aa18e2cd647ed16ebe274a5dacee9d0075fa02af250\n",
            "  Building wheel for liac-arff (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for liac-arff: filename=liac_arff-2.5.0-py3-none-any.whl size=11731 sha256=7394a1bf5454cb8b6120a46225a6240b2043597e009cbce6b547e61ba8446b76\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/0f/15/332ca86cbebf25ddf98518caaf887945fbe1712b97a0f2493b\n",
            "Successfully built contextvars openml liac-arff\n",
            "Installing collected packages: urllib3, locket, jmespath, partd, fsspec, cloudpickle, botocore, threadpoolctl, scipy, s3transfer, pynacl, psutil, dask, cryptography, bcrypt, scikit-learn, paramiko, distributed, ConfigSpace, boto3, xmltodict, pyflakes, pycodestyle, portalocker, Pillow, minio, mccabe, liac-arff, immutables, fastcore, colorama, autogluon.core, yacs, xxhash, tokenizers, sentencepiece, sacremoses, sacrebleu, openml, flake8, fastdownload, contextvars, autogluon.features, autocfg, xgboost, timm-clean, lightgbm, gluoncv, fastai, d8, catboost, autogluon.tabular, autogluon.mxnet, autogluon-contrib-nlp, autogluon.vision, autogluon.text, autogluon.extra, autogluon\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "  Attempting uninstall: dask\n",
            "    Found existing installation: dask 2.12.0\n",
            "    Uninstalling dask-2.12.0:\n",
            "      Successfully uninstalled dask-2.12.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "  Attempting uninstall: distributed\n",
            "    Found existing installation: distributed 1.25.3\n",
            "    Uninstalling distributed-1.25.3:\n",
            "      Successfully uninstalled distributed-1.25.3\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "  Attempting uninstall: xgboost\n",
            "    Found existing installation: xgboost 0.90\n",
            "    Uninstalling xgboost-0.90:\n",
            "      Successfully uninstalled xgboost-0.90\n",
            "  Attempting uninstall: lightgbm\n",
            "    Found existing installation: lightgbm 2.2.3\n",
            "    Uninstalling lightgbm-2.2.3:\n",
            "      Successfully uninstalled lightgbm-2.2.3\n",
            "  Attempting uninstall: fastai\n",
            "    Found existing installation: fastai 1.0.61\n",
            "    Uninstalling fastai-1.0.61:\n",
            "      Successfully uninstalled fastai-1.0.61\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gym 0.17.3 requires cloudpickle<1.7.0,>=1.2.0, but you have cloudpickle 2.0.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed ConfigSpace-0.4.19 Pillow-8.3.2 autocfg-0.0.8 autogluon-0.3.1 autogluon-contrib-nlp-0.0.1b20210201 autogluon.core-0.3.1 autogluon.extra-0.3.1 autogluon.features-0.3.1 autogluon.mxnet-0.3.1 autogluon.tabular-0.3.1 autogluon.text-0.3.1 autogluon.vision-0.3.1 bcrypt-3.2.0 boto3-1.18.51 botocore-1.21.51 catboost-0.25.1 cloudpickle-2.0.0 colorama-0.4.4 contextvars-2.4 cryptography-35.0.0 d8-0.0.2.post0 dask-2021.9.1 distributed-2021.9.1 fastai-2.5.2 fastcore-1.3.26 fastdownload-0.0.5 flake8-3.9.2 fsspec-2021.9.0 gluoncv-0.10.4.post4 immutables-0.16 jmespath-0.10.0 liac-arff-2.5.0 lightgbm-3.2.1 locket-0.2.1 mccabe-0.6.1 minio-7.1.0 openml-0.12.2 paramiko-2.7.2 partd-1.2.0 portalocker-2.3.2 psutil-5.8.0 pycodestyle-2.7.0 pyflakes-2.3.1 pynacl-1.4.0 s3transfer-0.5.0 sacrebleu-2.0.0 sacremoses-0.0.46 scikit-learn-0.24.2 scipy-1.6.3 sentencepiece-0.1.95 threadpoolctl-2.2.0 timm-clean-0.4.12 tokenizers-0.9.4 urllib3-1.25.11 xgboost-1.4.2 xmltodict-0.12.0 xxhash-2.0.2 yacs-0.1.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "contextvars",
                  "psutil"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pssPg92carkn"
      },
      "source": [
        "import os\n",
        "import os.path\n",
        "import time\n",
        "import numpy as np\n",
        "from autogluon.tabular import TabularDataset, TabularPredictor\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "from autogluon.tabular import TabularDataset, TabularPredictor\n",
        "from autogluon.core.utils.utils import setup_outputdir\n",
        "from autogluon.core.utils.loaders import load_pkl\n",
        "from autogluon.core.utils.savers import save_pkl"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVXwj2f5a5Pm"
      },
      "source": [
        "data1 = pd.read_csv(\"/content/inputx.csv\",header=None)\n",
        "data2 = pd.read_csv(\"/content/lims.csv\",header=None)\n",
        "data3 = pd.read_csv(\"/content/outputy.csv\",header=None)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "DSRFkrfMbJsu",
        "outputId": "898ee15e-121d-4e0b-ed8e-8d5e98307a60"
      },
      "source": [
        "frames = [data1,data2,data3]\n",
        "result = pd.concat(frames,axis=1)\n",
        "a = list(range(0,58))\n",
        "for i in range(58):\n",
        "  a[i] = str(a[i])\n",
        "result1 = pd.DataFrame(result)\n",
        "result1.columns = a\n",
        "del result1[a[1]]\n",
        "del result1[a[2]]\n",
        "del result1[a[4]]\n",
        "del result1[a[16]]\n",
        "del result1[a[17]]\n",
        "del result1[a[18]]\n",
        "del result1[a[19]]\n",
        "del result1[a[20]]\n",
        "del result1[a[21]]\n",
        "del result1[a[22]]\n",
        "del result1[a[23]]\n",
        "del result1[a[24]]\n",
        "\n",
        "del result1[a[36]]\n",
        "del result1[a[37]]\n",
        "del result1[a[38]]\n",
        "del result1[a[39]]\n",
        "del result1[a[40]]\n",
        "del result1[a[41]]\n",
        "del result1[a[42]]\n",
        "del result1[a[43]]\n",
        "\n",
        "result1 "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>3</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>60.613459</td>\n",
              "      <td>4.919693</td>\n",
              "      <td>369.115797</td>\n",
              "      <td>385.293214</td>\n",
              "      <td>385.393214</td>\n",
              "      <td>385.493214</td>\n",
              "      <td>399.469981</td>\n",
              "      <td>400.683283</td>\n",
              "      <td>360.304884</td>\n",
              "      <td>365.320613</td>\n",
              "      <td>16522.871995</td>\n",
              "      <td>15821.827526</td>\n",
              "      <td>16142.047737</td>\n",
              "      <td>31.311094</td>\n",
              "      <td>912.359419</td>\n",
              "      <td>222.115928</td>\n",
              "      <td>252.184223</td>\n",
              "      <td>313.148469</td>\n",
              "      <td>371.965103</td>\n",
              "      <td>431.602563</td>\n",
              "      <td>491.895472</td>\n",
              "      <td>506.299246</td>\n",
              "      <td>522.938354</td>\n",
              "      <td>2.881596</td>\n",
              "      <td>13.592935</td>\n",
              "      <td>2.652571</td>\n",
              "      <td>0.647645</td>\n",
              "      <td>3.806412</td>\n",
              "      <td>3.508402</td>\n",
              "      <td>15.249324</td>\n",
              "      <td>24.375486</td>\n",
              "      <td>27.579555</td>\n",
              "      <td>12.241985</td>\n",
              "      <td>2.881596</td>\n",
              "      <td>772.983990</td>\n",
              "      <td>1.949570</td>\n",
              "      <td>31.311094</td>\n",
              "      <td>31.311094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>64.702884</td>\n",
              "      <td>4.992169</td>\n",
              "      <td>362.503264</td>\n",
              "      <td>379.208842</td>\n",
              "      <td>379.308842</td>\n",
              "      <td>379.408842</td>\n",
              "      <td>391.529301</td>\n",
              "      <td>393.487512</td>\n",
              "      <td>370.036660</td>\n",
              "      <td>375.211419</td>\n",
              "      <td>16414.052190</td>\n",
              "      <td>16071.445427</td>\n",
              "      <td>16184.144817</td>\n",
              "      <td>27.771291</td>\n",
              "      <td>920.134620</td>\n",
              "      <td>205.116520</td>\n",
              "      <td>236.822216</td>\n",
              "      <td>301.989785</td>\n",
              "      <td>364.610301</td>\n",
              "      <td>427.443207</td>\n",
              "      <td>490.317913</td>\n",
              "      <td>508.178330</td>\n",
              "      <td>524.460232</td>\n",
              "      <td>1.632032</td>\n",
              "      <td>18.032035</td>\n",
              "      <td>1.529500</td>\n",
              "      <td>0.585261</td>\n",
              "      <td>3.890402</td>\n",
              "      <td>3.522771</td>\n",
              "      <td>14.461850</td>\n",
              "      <td>20.961687</td>\n",
              "      <td>30.016213</td>\n",
              "      <td>16.492052</td>\n",
              "      <td>1.632032</td>\n",
              "      <td>658.550157</td>\n",
              "      <td>1.901188</td>\n",
              "      <td>27.771291</td>\n",
              "      <td>27.771291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>64.702884</td>\n",
              "      <td>4.992169</td>\n",
              "      <td>362.503264</td>\n",
              "      <td>379.208842</td>\n",
              "      <td>379.308842</td>\n",
              "      <td>379.408842</td>\n",
              "      <td>391.529301</td>\n",
              "      <td>393.487512</td>\n",
              "      <td>370.036660</td>\n",
              "      <td>375.211419</td>\n",
              "      <td>16414.052190</td>\n",
              "      <td>16071.445427</td>\n",
              "      <td>16184.144817</td>\n",
              "      <td>27.771291</td>\n",
              "      <td>920.134620</td>\n",
              "      <td>205.116520</td>\n",
              "      <td>236.822216</td>\n",
              "      <td>301.989785</td>\n",
              "      <td>364.610301</td>\n",
              "      <td>427.443207</td>\n",
              "      <td>490.317913</td>\n",
              "      <td>508.178330</td>\n",
              "      <td>524.460232</td>\n",
              "      <td>1.632032</td>\n",
              "      <td>18.032035</td>\n",
              "      <td>1.529500</td>\n",
              "      <td>0.585261</td>\n",
              "      <td>3.890402</td>\n",
              "      <td>3.522771</td>\n",
              "      <td>14.461850</td>\n",
              "      <td>20.961687</td>\n",
              "      <td>30.016213</td>\n",
              "      <td>16.492052</td>\n",
              "      <td>1.632032</td>\n",
              "      <td>658.550157</td>\n",
              "      <td>1.901188</td>\n",
              "      <td>27.771291</td>\n",
              "      <td>27.771291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>62.495480</td>\n",
              "      <td>2.590062</td>\n",
              "      <td>357.066883</td>\n",
              "      <td>373.373330</td>\n",
              "      <td>373.473330</td>\n",
              "      <td>373.573330</td>\n",
              "      <td>386.161145</td>\n",
              "      <td>387.571970</td>\n",
              "      <td>360.939777</td>\n",
              "      <td>367.429190</td>\n",
              "      <td>16757.544509</td>\n",
              "      <td>15906.613755</td>\n",
              "      <td>16145.404570</td>\n",
              "      <td>19.875377</td>\n",
              "      <td>931.953094</td>\n",
              "      <td>206.969134</td>\n",
              "      <td>237.541295</td>\n",
              "      <td>302.396953</td>\n",
              "      <td>366.930768</td>\n",
              "      <td>427.602432</td>\n",
              "      <td>494.819274</td>\n",
              "      <td>511.906947</td>\n",
              "      <td>525.704409</td>\n",
              "      <td>2.789113</td>\n",
              "      <td>35.372562</td>\n",
              "      <td>2.573762</td>\n",
              "      <td>0.469367</td>\n",
              "      <td>2.193423</td>\n",
              "      <td>1.967312</td>\n",
              "      <td>8.210251</td>\n",
              "      <td>15.194378</td>\n",
              "      <td>27.935423</td>\n",
              "      <td>32.042866</td>\n",
              "      <td>2.789113</td>\n",
              "      <td>886.499578</td>\n",
              "      <td>1.671243</td>\n",
              "      <td>19.875377</td>\n",
              "      <td>19.875377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>62.495480</td>\n",
              "      <td>2.590062</td>\n",
              "      <td>357.066883</td>\n",
              "      <td>373.373330</td>\n",
              "      <td>373.473330</td>\n",
              "      <td>373.573330</td>\n",
              "      <td>386.161145</td>\n",
              "      <td>387.571970</td>\n",
              "      <td>360.939777</td>\n",
              "      <td>367.429190</td>\n",
              "      <td>16757.544509</td>\n",
              "      <td>15906.613755</td>\n",
              "      <td>16145.404570</td>\n",
              "      <td>19.875377</td>\n",
              "      <td>931.953094</td>\n",
              "      <td>206.969134</td>\n",
              "      <td>237.541295</td>\n",
              "      <td>302.396953</td>\n",
              "      <td>366.930768</td>\n",
              "      <td>427.602432</td>\n",
              "      <td>494.819274</td>\n",
              "      <td>511.906947</td>\n",
              "      <td>525.704409</td>\n",
              "      <td>2.789113</td>\n",
              "      <td>35.372562</td>\n",
              "      <td>2.573762</td>\n",
              "      <td>0.469367</td>\n",
              "      <td>2.193423</td>\n",
              "      <td>1.967312</td>\n",
              "      <td>8.210251</td>\n",
              "      <td>15.194378</td>\n",
              "      <td>27.935423</td>\n",
              "      <td>32.042866</td>\n",
              "      <td>2.789113</td>\n",
              "      <td>886.499578</td>\n",
              "      <td>1.671243</td>\n",
              "      <td>19.875377</td>\n",
              "      <td>19.875377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2136</th>\n",
              "      <td>63.483383</td>\n",
              "      <td>2.609711</td>\n",
              "      <td>360.097958</td>\n",
              "      <td>378.836098</td>\n",
              "      <td>378.936098</td>\n",
              "      <td>379.036098</td>\n",
              "      <td>391.005896</td>\n",
              "      <td>392.578406</td>\n",
              "      <td>367.283549</td>\n",
              "      <td>372.189022</td>\n",
              "      <td>16262.422097</td>\n",
              "      <td>16112.209682</td>\n",
              "      <td>16291.317887</td>\n",
              "      <td>23.870924</td>\n",
              "      <td>904.258245</td>\n",
              "      <td>222.984373</td>\n",
              "      <td>256.638057</td>\n",
              "      <td>316.955759</td>\n",
              "      <td>382.289208</td>\n",
              "      <td>443.177273</td>\n",
              "      <td>505.863414</td>\n",
              "      <td>522.624023</td>\n",
              "      <td>539.948959</td>\n",
              "      <td>1.658961</td>\n",
              "      <td>33.689630</td>\n",
              "      <td>1.504308</td>\n",
              "      <td>0.422349</td>\n",
              "      <td>2.384926</td>\n",
              "      <td>2.208367</td>\n",
              "      <td>9.174219</td>\n",
              "      <td>14.403356</td>\n",
              "      <td>28.467341</td>\n",
              "      <td>29.754452</td>\n",
              "      <td>1.658961</td>\n",
              "      <td>1001.809052</td>\n",
              "      <td>1.448151</td>\n",
              "      <td>23.870924</td>\n",
              "      <td>23.870924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2137</th>\n",
              "      <td>63.483383</td>\n",
              "      <td>2.609711</td>\n",
              "      <td>360.097958</td>\n",
              "      <td>378.836098</td>\n",
              "      <td>378.936098</td>\n",
              "      <td>379.036098</td>\n",
              "      <td>391.005896</td>\n",
              "      <td>392.578406</td>\n",
              "      <td>367.283549</td>\n",
              "      <td>372.189022</td>\n",
              "      <td>16262.422097</td>\n",
              "      <td>16112.209682</td>\n",
              "      <td>16291.317887</td>\n",
              "      <td>23.870924</td>\n",
              "      <td>904.258245</td>\n",
              "      <td>222.984373</td>\n",
              "      <td>256.638057</td>\n",
              "      <td>316.955759</td>\n",
              "      <td>382.289208</td>\n",
              "      <td>443.177273</td>\n",
              "      <td>505.863414</td>\n",
              "      <td>522.624023</td>\n",
              "      <td>539.948959</td>\n",
              "      <td>1.658961</td>\n",
              "      <td>33.689630</td>\n",
              "      <td>1.504308</td>\n",
              "      <td>0.422349</td>\n",
              "      <td>2.384926</td>\n",
              "      <td>2.208367</td>\n",
              "      <td>9.174219</td>\n",
              "      <td>14.403356</td>\n",
              "      <td>28.467341</td>\n",
              "      <td>29.754452</td>\n",
              "      <td>1.658961</td>\n",
              "      <td>1001.809052</td>\n",
              "      <td>1.448151</td>\n",
              "      <td>23.870924</td>\n",
              "      <td>23.870924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2138</th>\n",
              "      <td>63.483383</td>\n",
              "      <td>2.609711</td>\n",
              "      <td>360.097958</td>\n",
              "      <td>378.836098</td>\n",
              "      <td>378.936098</td>\n",
              "      <td>379.036098</td>\n",
              "      <td>391.005896</td>\n",
              "      <td>392.578406</td>\n",
              "      <td>367.283549</td>\n",
              "      <td>372.189022</td>\n",
              "      <td>16262.422097</td>\n",
              "      <td>16112.209682</td>\n",
              "      <td>16291.317887</td>\n",
              "      <td>23.870924</td>\n",
              "      <td>904.258245</td>\n",
              "      <td>222.984373</td>\n",
              "      <td>256.638057</td>\n",
              "      <td>316.955759</td>\n",
              "      <td>382.289208</td>\n",
              "      <td>443.177273</td>\n",
              "      <td>505.863414</td>\n",
              "      <td>522.624023</td>\n",
              "      <td>539.948959</td>\n",
              "      <td>1.658961</td>\n",
              "      <td>33.689630</td>\n",
              "      <td>1.504308</td>\n",
              "      <td>0.422349</td>\n",
              "      <td>2.384926</td>\n",
              "      <td>2.208367</td>\n",
              "      <td>9.174219</td>\n",
              "      <td>14.403356</td>\n",
              "      <td>28.467341</td>\n",
              "      <td>29.754452</td>\n",
              "      <td>1.658961</td>\n",
              "      <td>1001.809052</td>\n",
              "      <td>1.448151</td>\n",
              "      <td>23.870924</td>\n",
              "      <td>23.870924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2139</th>\n",
              "      <td>63.483383</td>\n",
              "      <td>2.609711</td>\n",
              "      <td>360.097958</td>\n",
              "      <td>378.836098</td>\n",
              "      <td>378.936098</td>\n",
              "      <td>379.036098</td>\n",
              "      <td>391.005896</td>\n",
              "      <td>392.578406</td>\n",
              "      <td>367.283549</td>\n",
              "      <td>372.189022</td>\n",
              "      <td>16262.422097</td>\n",
              "      <td>16112.209682</td>\n",
              "      <td>16291.317887</td>\n",
              "      <td>23.870924</td>\n",
              "      <td>904.258245</td>\n",
              "      <td>222.984373</td>\n",
              "      <td>256.638057</td>\n",
              "      <td>316.955759</td>\n",
              "      <td>382.289208</td>\n",
              "      <td>443.177273</td>\n",
              "      <td>505.863414</td>\n",
              "      <td>522.624023</td>\n",
              "      <td>539.948959</td>\n",
              "      <td>1.658961</td>\n",
              "      <td>33.689630</td>\n",
              "      <td>1.504308</td>\n",
              "      <td>0.422349</td>\n",
              "      <td>2.384926</td>\n",
              "      <td>2.208367</td>\n",
              "      <td>9.174219</td>\n",
              "      <td>14.403356</td>\n",
              "      <td>28.467341</td>\n",
              "      <td>29.754452</td>\n",
              "      <td>1.658961</td>\n",
              "      <td>1001.809052</td>\n",
              "      <td>1.448151</td>\n",
              "      <td>23.870924</td>\n",
              "      <td>23.870924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2140</th>\n",
              "      <td>63.483383</td>\n",
              "      <td>2.609711</td>\n",
              "      <td>360.097958</td>\n",
              "      <td>378.836098</td>\n",
              "      <td>378.936098</td>\n",
              "      <td>379.036098</td>\n",
              "      <td>391.005896</td>\n",
              "      <td>392.578406</td>\n",
              "      <td>367.283549</td>\n",
              "      <td>372.189022</td>\n",
              "      <td>16262.422097</td>\n",
              "      <td>16112.209682</td>\n",
              "      <td>16291.317887</td>\n",
              "      <td>23.870924</td>\n",
              "      <td>904.258245</td>\n",
              "      <td>222.984373</td>\n",
              "      <td>256.638057</td>\n",
              "      <td>316.955759</td>\n",
              "      <td>382.289208</td>\n",
              "      <td>443.177273</td>\n",
              "      <td>505.863414</td>\n",
              "      <td>522.624023</td>\n",
              "      <td>539.948959</td>\n",
              "      <td>1.658961</td>\n",
              "      <td>33.689630</td>\n",
              "      <td>1.504308</td>\n",
              "      <td>0.422349</td>\n",
              "      <td>2.384926</td>\n",
              "      <td>2.208367</td>\n",
              "      <td>9.174219</td>\n",
              "      <td>14.403356</td>\n",
              "      <td>28.467341</td>\n",
              "      <td>29.754452</td>\n",
              "      <td>1.658961</td>\n",
              "      <td>1001.809052</td>\n",
              "      <td>1.448151</td>\n",
              "      <td>23.870924</td>\n",
              "      <td>23.870924</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2141 rows × 38 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              0         3           5  ...        55         56         57\n",
              "0     60.613459  4.919693  369.115797  ...  1.949570  31.311094  31.311094\n",
              "1     64.702884  4.992169  362.503264  ...  1.901188  27.771291  27.771291\n",
              "2     64.702884  4.992169  362.503264  ...  1.901188  27.771291  27.771291\n",
              "3     62.495480  2.590062  357.066883  ...  1.671243  19.875377  19.875377\n",
              "4     62.495480  2.590062  357.066883  ...  1.671243  19.875377  19.875377\n",
              "...         ...       ...         ...  ...       ...        ...        ...\n",
              "2136  63.483383  2.609711  360.097958  ...  1.448151  23.870924  23.870924\n",
              "2137  63.483383  2.609711  360.097958  ...  1.448151  23.870924  23.870924\n",
              "2138  63.483383  2.609711  360.097958  ...  1.448151  23.870924  23.870924\n",
              "2139  63.483383  2.609711  360.097958  ...  1.448151  23.870924  23.870924\n",
              "2140  63.483383  2.609711  360.097958  ...  1.448151  23.870924  23.870924\n",
              "\n",
              "[2141 rows x 38 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "hYt5TdHAj9zv",
        "outputId": "1a722ef9-91ba-47b3-9577-73ebff1c01f3"
      },
      "source": [
        "a = list(range(0,38))\n",
        "for i in range(38):\n",
        "  a[i] = str(a[i])\n",
        "result1 = pd.DataFrame(result)\n",
        "result1.columns = a\n",
        "traindata = result1.to_csv('traindata.csv')\n",
        "result1"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>60.613459</td>\n",
              "      <td>4.919693</td>\n",
              "      <td>369.115797</td>\n",
              "      <td>385.293214</td>\n",
              "      <td>385.393214</td>\n",
              "      <td>385.493214</td>\n",
              "      <td>399.469981</td>\n",
              "      <td>400.683283</td>\n",
              "      <td>360.304884</td>\n",
              "      <td>365.320613</td>\n",
              "      <td>16522.871995</td>\n",
              "      <td>15821.827526</td>\n",
              "      <td>16142.047737</td>\n",
              "      <td>31.311094</td>\n",
              "      <td>912.359419</td>\n",
              "      <td>222.115928</td>\n",
              "      <td>252.184223</td>\n",
              "      <td>313.148469</td>\n",
              "      <td>371.965103</td>\n",
              "      <td>431.602563</td>\n",
              "      <td>491.895472</td>\n",
              "      <td>506.299246</td>\n",
              "      <td>522.938354</td>\n",
              "      <td>2.881596</td>\n",
              "      <td>13.592935</td>\n",
              "      <td>2.652571</td>\n",
              "      <td>0.647645</td>\n",
              "      <td>3.806412</td>\n",
              "      <td>3.508402</td>\n",
              "      <td>15.249324</td>\n",
              "      <td>24.375486</td>\n",
              "      <td>27.579555</td>\n",
              "      <td>12.241985</td>\n",
              "      <td>2.881596</td>\n",
              "      <td>772.983990</td>\n",
              "      <td>1.949570</td>\n",
              "      <td>31.311094</td>\n",
              "      <td>31.311094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>64.702884</td>\n",
              "      <td>4.992169</td>\n",
              "      <td>362.503264</td>\n",
              "      <td>379.208842</td>\n",
              "      <td>379.308842</td>\n",
              "      <td>379.408842</td>\n",
              "      <td>391.529301</td>\n",
              "      <td>393.487512</td>\n",
              "      <td>370.036660</td>\n",
              "      <td>375.211419</td>\n",
              "      <td>16414.052190</td>\n",
              "      <td>16071.445427</td>\n",
              "      <td>16184.144817</td>\n",
              "      <td>27.771291</td>\n",
              "      <td>920.134620</td>\n",
              "      <td>205.116520</td>\n",
              "      <td>236.822216</td>\n",
              "      <td>301.989785</td>\n",
              "      <td>364.610301</td>\n",
              "      <td>427.443207</td>\n",
              "      <td>490.317913</td>\n",
              "      <td>508.178330</td>\n",
              "      <td>524.460232</td>\n",
              "      <td>1.632032</td>\n",
              "      <td>18.032035</td>\n",
              "      <td>1.529500</td>\n",
              "      <td>0.585261</td>\n",
              "      <td>3.890402</td>\n",
              "      <td>3.522771</td>\n",
              "      <td>14.461850</td>\n",
              "      <td>20.961687</td>\n",
              "      <td>30.016213</td>\n",
              "      <td>16.492052</td>\n",
              "      <td>1.632032</td>\n",
              "      <td>658.550157</td>\n",
              "      <td>1.901188</td>\n",
              "      <td>27.771291</td>\n",
              "      <td>27.771291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>64.702884</td>\n",
              "      <td>4.992169</td>\n",
              "      <td>362.503264</td>\n",
              "      <td>379.208842</td>\n",
              "      <td>379.308842</td>\n",
              "      <td>379.408842</td>\n",
              "      <td>391.529301</td>\n",
              "      <td>393.487512</td>\n",
              "      <td>370.036660</td>\n",
              "      <td>375.211419</td>\n",
              "      <td>16414.052190</td>\n",
              "      <td>16071.445427</td>\n",
              "      <td>16184.144817</td>\n",
              "      <td>27.771291</td>\n",
              "      <td>920.134620</td>\n",
              "      <td>205.116520</td>\n",
              "      <td>236.822216</td>\n",
              "      <td>301.989785</td>\n",
              "      <td>364.610301</td>\n",
              "      <td>427.443207</td>\n",
              "      <td>490.317913</td>\n",
              "      <td>508.178330</td>\n",
              "      <td>524.460232</td>\n",
              "      <td>1.632032</td>\n",
              "      <td>18.032035</td>\n",
              "      <td>1.529500</td>\n",
              "      <td>0.585261</td>\n",
              "      <td>3.890402</td>\n",
              "      <td>3.522771</td>\n",
              "      <td>14.461850</td>\n",
              "      <td>20.961687</td>\n",
              "      <td>30.016213</td>\n",
              "      <td>16.492052</td>\n",
              "      <td>1.632032</td>\n",
              "      <td>658.550157</td>\n",
              "      <td>1.901188</td>\n",
              "      <td>27.771291</td>\n",
              "      <td>27.771291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>62.495480</td>\n",
              "      <td>2.590062</td>\n",
              "      <td>357.066883</td>\n",
              "      <td>373.373330</td>\n",
              "      <td>373.473330</td>\n",
              "      <td>373.573330</td>\n",
              "      <td>386.161145</td>\n",
              "      <td>387.571970</td>\n",
              "      <td>360.939777</td>\n",
              "      <td>367.429190</td>\n",
              "      <td>16757.544509</td>\n",
              "      <td>15906.613755</td>\n",
              "      <td>16145.404570</td>\n",
              "      <td>19.875377</td>\n",
              "      <td>931.953094</td>\n",
              "      <td>206.969134</td>\n",
              "      <td>237.541295</td>\n",
              "      <td>302.396953</td>\n",
              "      <td>366.930768</td>\n",
              "      <td>427.602432</td>\n",
              "      <td>494.819274</td>\n",
              "      <td>511.906947</td>\n",
              "      <td>525.704409</td>\n",
              "      <td>2.789113</td>\n",
              "      <td>35.372562</td>\n",
              "      <td>2.573762</td>\n",
              "      <td>0.469367</td>\n",
              "      <td>2.193423</td>\n",
              "      <td>1.967312</td>\n",
              "      <td>8.210251</td>\n",
              "      <td>15.194378</td>\n",
              "      <td>27.935423</td>\n",
              "      <td>32.042866</td>\n",
              "      <td>2.789113</td>\n",
              "      <td>886.499578</td>\n",
              "      <td>1.671243</td>\n",
              "      <td>19.875377</td>\n",
              "      <td>19.875377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>62.495480</td>\n",
              "      <td>2.590062</td>\n",
              "      <td>357.066883</td>\n",
              "      <td>373.373330</td>\n",
              "      <td>373.473330</td>\n",
              "      <td>373.573330</td>\n",
              "      <td>386.161145</td>\n",
              "      <td>387.571970</td>\n",
              "      <td>360.939777</td>\n",
              "      <td>367.429190</td>\n",
              "      <td>16757.544509</td>\n",
              "      <td>15906.613755</td>\n",
              "      <td>16145.404570</td>\n",
              "      <td>19.875377</td>\n",
              "      <td>931.953094</td>\n",
              "      <td>206.969134</td>\n",
              "      <td>237.541295</td>\n",
              "      <td>302.396953</td>\n",
              "      <td>366.930768</td>\n",
              "      <td>427.602432</td>\n",
              "      <td>494.819274</td>\n",
              "      <td>511.906947</td>\n",
              "      <td>525.704409</td>\n",
              "      <td>2.789113</td>\n",
              "      <td>35.372562</td>\n",
              "      <td>2.573762</td>\n",
              "      <td>0.469367</td>\n",
              "      <td>2.193423</td>\n",
              "      <td>1.967312</td>\n",
              "      <td>8.210251</td>\n",
              "      <td>15.194378</td>\n",
              "      <td>27.935423</td>\n",
              "      <td>32.042866</td>\n",
              "      <td>2.789113</td>\n",
              "      <td>886.499578</td>\n",
              "      <td>1.671243</td>\n",
              "      <td>19.875377</td>\n",
              "      <td>19.875377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2136</th>\n",
              "      <td>63.483383</td>\n",
              "      <td>2.609711</td>\n",
              "      <td>360.097958</td>\n",
              "      <td>378.836098</td>\n",
              "      <td>378.936098</td>\n",
              "      <td>379.036098</td>\n",
              "      <td>391.005896</td>\n",
              "      <td>392.578406</td>\n",
              "      <td>367.283549</td>\n",
              "      <td>372.189022</td>\n",
              "      <td>16262.422097</td>\n",
              "      <td>16112.209682</td>\n",
              "      <td>16291.317887</td>\n",
              "      <td>23.870924</td>\n",
              "      <td>904.258245</td>\n",
              "      <td>222.984373</td>\n",
              "      <td>256.638057</td>\n",
              "      <td>316.955759</td>\n",
              "      <td>382.289208</td>\n",
              "      <td>443.177273</td>\n",
              "      <td>505.863414</td>\n",
              "      <td>522.624023</td>\n",
              "      <td>539.948959</td>\n",
              "      <td>1.658961</td>\n",
              "      <td>33.689630</td>\n",
              "      <td>1.504308</td>\n",
              "      <td>0.422349</td>\n",
              "      <td>2.384926</td>\n",
              "      <td>2.208367</td>\n",
              "      <td>9.174219</td>\n",
              "      <td>14.403356</td>\n",
              "      <td>28.467341</td>\n",
              "      <td>29.754452</td>\n",
              "      <td>1.658961</td>\n",
              "      <td>1001.809052</td>\n",
              "      <td>1.448151</td>\n",
              "      <td>23.870924</td>\n",
              "      <td>23.870924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2137</th>\n",
              "      <td>63.483383</td>\n",
              "      <td>2.609711</td>\n",
              "      <td>360.097958</td>\n",
              "      <td>378.836098</td>\n",
              "      <td>378.936098</td>\n",
              "      <td>379.036098</td>\n",
              "      <td>391.005896</td>\n",
              "      <td>392.578406</td>\n",
              "      <td>367.283549</td>\n",
              "      <td>372.189022</td>\n",
              "      <td>16262.422097</td>\n",
              "      <td>16112.209682</td>\n",
              "      <td>16291.317887</td>\n",
              "      <td>23.870924</td>\n",
              "      <td>904.258245</td>\n",
              "      <td>222.984373</td>\n",
              "      <td>256.638057</td>\n",
              "      <td>316.955759</td>\n",
              "      <td>382.289208</td>\n",
              "      <td>443.177273</td>\n",
              "      <td>505.863414</td>\n",
              "      <td>522.624023</td>\n",
              "      <td>539.948959</td>\n",
              "      <td>1.658961</td>\n",
              "      <td>33.689630</td>\n",
              "      <td>1.504308</td>\n",
              "      <td>0.422349</td>\n",
              "      <td>2.384926</td>\n",
              "      <td>2.208367</td>\n",
              "      <td>9.174219</td>\n",
              "      <td>14.403356</td>\n",
              "      <td>28.467341</td>\n",
              "      <td>29.754452</td>\n",
              "      <td>1.658961</td>\n",
              "      <td>1001.809052</td>\n",
              "      <td>1.448151</td>\n",
              "      <td>23.870924</td>\n",
              "      <td>23.870924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2138</th>\n",
              "      <td>63.483383</td>\n",
              "      <td>2.609711</td>\n",
              "      <td>360.097958</td>\n",
              "      <td>378.836098</td>\n",
              "      <td>378.936098</td>\n",
              "      <td>379.036098</td>\n",
              "      <td>391.005896</td>\n",
              "      <td>392.578406</td>\n",
              "      <td>367.283549</td>\n",
              "      <td>372.189022</td>\n",
              "      <td>16262.422097</td>\n",
              "      <td>16112.209682</td>\n",
              "      <td>16291.317887</td>\n",
              "      <td>23.870924</td>\n",
              "      <td>904.258245</td>\n",
              "      <td>222.984373</td>\n",
              "      <td>256.638057</td>\n",
              "      <td>316.955759</td>\n",
              "      <td>382.289208</td>\n",
              "      <td>443.177273</td>\n",
              "      <td>505.863414</td>\n",
              "      <td>522.624023</td>\n",
              "      <td>539.948959</td>\n",
              "      <td>1.658961</td>\n",
              "      <td>33.689630</td>\n",
              "      <td>1.504308</td>\n",
              "      <td>0.422349</td>\n",
              "      <td>2.384926</td>\n",
              "      <td>2.208367</td>\n",
              "      <td>9.174219</td>\n",
              "      <td>14.403356</td>\n",
              "      <td>28.467341</td>\n",
              "      <td>29.754452</td>\n",
              "      <td>1.658961</td>\n",
              "      <td>1001.809052</td>\n",
              "      <td>1.448151</td>\n",
              "      <td>23.870924</td>\n",
              "      <td>23.870924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2139</th>\n",
              "      <td>63.483383</td>\n",
              "      <td>2.609711</td>\n",
              "      <td>360.097958</td>\n",
              "      <td>378.836098</td>\n",
              "      <td>378.936098</td>\n",
              "      <td>379.036098</td>\n",
              "      <td>391.005896</td>\n",
              "      <td>392.578406</td>\n",
              "      <td>367.283549</td>\n",
              "      <td>372.189022</td>\n",
              "      <td>16262.422097</td>\n",
              "      <td>16112.209682</td>\n",
              "      <td>16291.317887</td>\n",
              "      <td>23.870924</td>\n",
              "      <td>904.258245</td>\n",
              "      <td>222.984373</td>\n",
              "      <td>256.638057</td>\n",
              "      <td>316.955759</td>\n",
              "      <td>382.289208</td>\n",
              "      <td>443.177273</td>\n",
              "      <td>505.863414</td>\n",
              "      <td>522.624023</td>\n",
              "      <td>539.948959</td>\n",
              "      <td>1.658961</td>\n",
              "      <td>33.689630</td>\n",
              "      <td>1.504308</td>\n",
              "      <td>0.422349</td>\n",
              "      <td>2.384926</td>\n",
              "      <td>2.208367</td>\n",
              "      <td>9.174219</td>\n",
              "      <td>14.403356</td>\n",
              "      <td>28.467341</td>\n",
              "      <td>29.754452</td>\n",
              "      <td>1.658961</td>\n",
              "      <td>1001.809052</td>\n",
              "      <td>1.448151</td>\n",
              "      <td>23.870924</td>\n",
              "      <td>23.870924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2140</th>\n",
              "      <td>63.483383</td>\n",
              "      <td>2.609711</td>\n",
              "      <td>360.097958</td>\n",
              "      <td>378.836098</td>\n",
              "      <td>378.936098</td>\n",
              "      <td>379.036098</td>\n",
              "      <td>391.005896</td>\n",
              "      <td>392.578406</td>\n",
              "      <td>367.283549</td>\n",
              "      <td>372.189022</td>\n",
              "      <td>16262.422097</td>\n",
              "      <td>16112.209682</td>\n",
              "      <td>16291.317887</td>\n",
              "      <td>23.870924</td>\n",
              "      <td>904.258245</td>\n",
              "      <td>222.984373</td>\n",
              "      <td>256.638057</td>\n",
              "      <td>316.955759</td>\n",
              "      <td>382.289208</td>\n",
              "      <td>443.177273</td>\n",
              "      <td>505.863414</td>\n",
              "      <td>522.624023</td>\n",
              "      <td>539.948959</td>\n",
              "      <td>1.658961</td>\n",
              "      <td>33.689630</td>\n",
              "      <td>1.504308</td>\n",
              "      <td>0.422349</td>\n",
              "      <td>2.384926</td>\n",
              "      <td>2.208367</td>\n",
              "      <td>9.174219</td>\n",
              "      <td>14.403356</td>\n",
              "      <td>28.467341</td>\n",
              "      <td>29.754452</td>\n",
              "      <td>1.658961</td>\n",
              "      <td>1001.809052</td>\n",
              "      <td>1.448151</td>\n",
              "      <td>23.870924</td>\n",
              "      <td>23.870924</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2141 rows × 38 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              0         1           2  ...        35         36         37\n",
              "0     60.613459  4.919693  369.115797  ...  1.949570  31.311094  31.311094\n",
              "1     64.702884  4.992169  362.503264  ...  1.901188  27.771291  27.771291\n",
              "2     64.702884  4.992169  362.503264  ...  1.901188  27.771291  27.771291\n",
              "3     62.495480  2.590062  357.066883  ...  1.671243  19.875377  19.875377\n",
              "4     62.495480  2.590062  357.066883  ...  1.671243  19.875377  19.875377\n",
              "...         ...       ...         ...  ...       ...        ...        ...\n",
              "2136  63.483383  2.609711  360.097958  ...  1.448151  23.870924  23.870924\n",
              "2137  63.483383  2.609711  360.097958  ...  1.448151  23.870924  23.870924\n",
              "2138  63.483383  2.609711  360.097958  ...  1.448151  23.870924  23.870924\n",
              "2139  63.483383  2.609711  360.097958  ...  1.448151  23.870924  23.870924\n",
              "2140  63.483383  2.609711  360.097958  ...  1.448151  23.870924  23.870924\n",
              "\n",
              "[2141 rows x 38 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDZXx0UMz3hn"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21x7mrkrz0VW",
        "outputId": "4630bb43-0145-4398-f5ae-37e072edcc0e"
      },
      "source": [
        "\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loaded data from: /content/traindata.csv | Columns = 39 / 39 | Rows = 2141 -> 2141\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwPJdTtIc2k-"
      },
      "source": [
        "\n",
        "dropid = [a[22],a[23],a[24],a[25],a[26],a[27],a[28],a[29],a[30],a[31],a[32],a[33]]\n",
        "labels = dropid\n",
        "\n",
        "\n",
        "class MultilabelPredictor():\n",
        "  multi_predictor_file = 'multilabel_predictor.pkl'\n",
        "\n",
        "  def __init__(self, labels, path, problem_types=None, eval_metrics=None, consider_labels_correlation=True, **kwargs):\n",
        "    if len(labels) < 2:\n",
        "        raise ValueError(\"MultilabelPredictor is only intended for predicting MULTIPLE labels (columns), use TabularPredictor for predicting one label (column).\")\n",
        "    self.path = setup_outputdir(path, warn_if_exist=False)\n",
        "    self.labels = labels\n",
        "    self.consider_labels_correlation = consider_labels_correlation\n",
        "    self.predictors = {}  # key = label, value = TabularPredictor or str path to the TabularPredictor for this label\n",
        "    if eval_metrics is None:\n",
        "      self.eval_metrics = {}\n",
        "    else:\n",
        "      self.eval_metrics = {labels[i] : eval_metrics[i] for i in range(len(labels))}\n",
        "    problem_type = None\n",
        "    eval_metric = None\n",
        "    for i in range(len(labels)):\n",
        "      label = labels[i]\n",
        "      path_i = self.path + \"Predictor_\" + label\n",
        "      if problem_types is not None:\n",
        "          problem_type = problem_types[i]\n",
        "      if eval_metrics is not None:\n",
        "          eval_metric = self.eval_metrics[label]\n",
        "      self.predictors[label] = TabularPredictor(label=label, problem_type=problem_type, eval_metric=eval_metric, path=path_i, **kwargs)\n",
        "\n",
        "  def fit(self, train_data, tuning_data=None, **kwargs):\n",
        "    \"\"\" Fits a separate TabularPredictor to predict each of the labels.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        train_data, tuning_data : str or autogluon.tabular.TabularDataset or pd.DataFrame\n",
        "            See documentation for `TabularPredictor.fit()`.\n",
        "        kwargs :\n",
        "            Arguments passed into the `fit()` call for each TabularPredictor.\n",
        "    \"\"\"\n",
        "    if isinstance(train_data,str):\n",
        "      train_data = TabularDataset(train_data)\n",
        "    if tuning_data is not None and isinstance(tuning_data, str):\n",
        "      tuning_data = TabularDataset(tuning_data)\n",
        "    train_data_og = train_data.copy()\n",
        "    if tuning_data is not None:\n",
        "      tuning_data_og = tuning_data.copy()\n",
        "    else:\n",
        "      tuning_data_og = None\n",
        "    save_metrics = len(self.eval_metrics) == 0\n",
        "    for i in range(len(self.labels)):\n",
        "      label = self.labels[i]\n",
        "      predictor = self.get_predictor(label)\n",
        "      if not self.consider_labels_correlation:\n",
        "          labels_to_drop = [l for l in self.labels if l != label]\n",
        "      else:\n",
        "          labels_to_drop = [self.labels[j] for j in range(i+1, len(self.labels))]\n",
        "      train_data = train_data_og.drop(labels_to_drop, axis=1)\n",
        "      if tuning_data is not None:\n",
        "          tuning_data = tuning_data_og.drop(labels_to_drop, axis=1)\n",
        "      print(f\"Fitting TabularPredictor for label: {label} ...\")\n",
        "      predictor.fit(train_data=train_data, tuning_data=tuning_data, **kwargs)\n",
        "      self.predictors[label] = predictor.path\n",
        "      if save_metrics:\n",
        "          self.eval_metrics[label] = predictor.eval_metric\n",
        "    self.save()\n",
        "\n",
        "  def predict(self, data, **kwargs):\n",
        "    \"\"\" Returns DataFrame with label columns containing predictions for each label.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        data : str or autogluon.tabular.TabularDataset or pd.DataFrame\n",
        "            Data to make predictions for. If label columns are present in this data, they will be ignored. See documentation for `TabularPredictor.predict()`.\n",
        "        kwargs :\n",
        "            Arguments passed into the predict() call for each TabularPredictor.\n",
        "    \"\"\"\n",
        "    return self._predict(data, as_proba=False, **kwargs)\n",
        "\n",
        "  def predict_proba(self, data, **kwargs):\n",
        "    \"\"\" Returns dict where each key is a label and the corresponding value is the `predict_proba()` output for just that label.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        data : str or autogluon.tabular.TabularDataset or pd.DataFrame\n",
        "            Data to make predictions for. See documentation for `TabularPredictor.predict()` and `TabularPredictor.predict_proba()`.\n",
        "        kwargs :\n",
        "            Arguments passed into the `predict_proba()` call for each TabularPredictor (also passed into a `predict()` call).\n",
        "    \"\"\"\n",
        "    return self._predict(data, as_proba=True, **kwargs)\n",
        "\n",
        "  def evaluate(self, data, **kwargs):\n",
        "    \"\"\" Returns dict where each key is a label and the corresponding value is the `evaluate()` output for just that label.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        data : str or autogluon.tabular.TabularDataset or pd.DataFrame\n",
        "            Data to evalate predictions of all labels for, must contain all labels as columns. See documentation for `TabularPredictor.evaluate()`.\n",
        "        kwargs :\n",
        "            Arguments passed into the `evaluate()` call for each TabularPredictor (also passed into the `predict()` call).\n",
        "    \"\"\"\n",
        "    data = self._get_data(data)\n",
        "    eval_dict = {}\n",
        "    for label in self.labels:\n",
        "      print(f\"Evaluating TabularPredictor for label: {label} ...\")\n",
        "      predictor = self.get_predictor(label)\n",
        "      eval_dict[label] = predictor.evaluate(data, **kwargs)\n",
        "      if self.consider_labels_correlation:\n",
        "          data[label] = predictor.predict(data, **kwargs)\n",
        "    return eval_dict\n",
        "\n",
        "  def save(self):\n",
        "    \"\"\" Save MultilabelPredictor to disk. \"\"\"\n",
        "    for label in self.labels:\n",
        "      if not isinstance(self.predictors[label], str):\n",
        "        self.predictors[label] = self.predictors[label].path\n",
        "    save_pkl.save(path=self.path+self.multi_predictor_file, object=self)\n",
        "    print(f\"MultilabelPredictor saved to disk. Load with: MultilabelPredictor.load('{self.path}')\")\n",
        "\n",
        "  @classmethod\n",
        "  def load(cls, path):\n",
        "    \"\"\" Load MultilabelPredictor from disk `path` previously specified when creating this MultilabelPredictor. \"\"\"\n",
        "    path = os.path.expanduser(path)\n",
        "    if path[-1] != os.path.sep:\n",
        "      path = path + os.path.sep\n",
        "    return load_pkl.load(path=path+cls.multi_predictor_file)\n",
        "\n",
        "  def get_predictor(self, label):\n",
        "    \"\"\" Returns TabularPredictor which is used to predict this label. \"\"\"\n",
        "    predictor = self.predictors[label]\n",
        "    if isinstance(predictor, str):\n",
        "      return TabularPredictor.load(path=predictor)\n",
        "    return predictor\n",
        "\n",
        "  def _get_data(self, data):\n",
        "    if isinstance(data, str):\n",
        "      return TabularDataset(data)\n",
        "    return data.copy()\n",
        "\n",
        "  def _predict(self, data, as_proba=False, **kwargs):\n",
        "    data = self._get_data(data)\n",
        "    if as_proba:\n",
        "        predproba_dict = {}\n",
        "    for label in self.labels:\n",
        "      print(f\"Predicting with TabularPredictor for label: {label} ...\")\n",
        "      predictor = self.get_predictor(label)\n",
        "      if as_proba:\n",
        "          predproba_dict[label] = predictor.predict_proba(data, as_multiclass=True, **kwargs)\n",
        "      data[label] = predictor.predict(data, **kwargs)\n",
        "    if not as_proba:\n",
        "      return data[self.labels]\n",
        "    else:\n",
        "      return predproba_dict\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "mT2-AvVN1YZW",
        "outputId": "67b391d7-c923-481e-9071-3066c4997688"
      },
      "source": [
        "train_data = TabularDataset(\"/content/traindata.csv\")\n",
        "subsample_size = 2100\n",
        "train_data = train_data.sample(n=subsample_size, random_state=0)\n",
        "a = list(range(0,39))\n",
        "for i in range(39):\n",
        "  a[i] = str(a[i])\n",
        "train_data.columns = a\n",
        "train_data.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1719</th>\n",
              "      <td>1719</td>\n",
              "      <td>60.092708</td>\n",
              "      <td>2.450414</td>\n",
              "      <td>358.371974</td>\n",
              "      <td>375.934213</td>\n",
              "      <td>376.034213</td>\n",
              "      <td>376.134213</td>\n",
              "      <td>389.540940</td>\n",
              "      <td>390.038226</td>\n",
              "      <td>361.934265</td>\n",
              "      <td>368.777962</td>\n",
              "      <td>16451.945640</td>\n",
              "      <td>16006.661109</td>\n",
              "      <td>16141.133117</td>\n",
              "      <td>23.751782</td>\n",
              "      <td>914.046450</td>\n",
              "      <td>205.595687</td>\n",
              "      <td>240.003288</td>\n",
              "      <td>301.227269</td>\n",
              "      <td>369.319859</td>\n",
              "      <td>434.678012</td>\n",
              "      <td>499.644025</td>\n",
              "      <td>512.319534</td>\n",
              "      <td>530.509493</td>\n",
              "      <td>2.716414</td>\n",
              "      <td>27.626445</td>\n",
              "      <td>2.523027</td>\n",
              "      <td>0.518742</td>\n",
              "      <td>2.708843</td>\n",
              "      <td>2.478947</td>\n",
              "      <td>10.607617</td>\n",
              "      <td>18.208807</td>\n",
              "      <td>28.749294</td>\n",
              "      <td>25.115384</td>\n",
              "      <td>2.716414</td>\n",
              "      <td>685.208560</td>\n",
              "      <td>1.673306</td>\n",
              "      <td>23.751782</td>\n",
              "      <td>23.751782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>175</td>\n",
              "      <td>64.258950</td>\n",
              "      <td>3.468767</td>\n",
              "      <td>355.820686</td>\n",
              "      <td>374.795280</td>\n",
              "      <td>374.895280</td>\n",
              "      <td>374.995280</td>\n",
              "      <td>386.660531</td>\n",
              "      <td>388.649380</td>\n",
              "      <td>368.073805</td>\n",
              "      <td>372.681118</td>\n",
              "      <td>16571.888132</td>\n",
              "      <td>16029.925800</td>\n",
              "      <td>16108.259279</td>\n",
              "      <td>24.169549</td>\n",
              "      <td>931.528794</td>\n",
              "      <td>191.141473</td>\n",
              "      <td>222.110697</td>\n",
              "      <td>292.224902</td>\n",
              "      <td>355.126954</td>\n",
              "      <td>426.040480</td>\n",
              "      <td>490.982001</td>\n",
              "      <td>505.155588</td>\n",
              "      <td>523.916752</td>\n",
              "      <td>1.947339</td>\n",
              "      <td>28.159291</td>\n",
              "      <td>1.754061</td>\n",
              "      <td>0.478342</td>\n",
              "      <td>2.707570</td>\n",
              "      <td>2.408400</td>\n",
              "      <td>9.578486</td>\n",
              "      <td>15.685092</td>\n",
              "      <td>30.622116</td>\n",
              "      <td>24.785759</td>\n",
              "      <td>1.947339</td>\n",
              "      <td>930.071026</td>\n",
              "      <td>1.679571</td>\n",
              "      <td>24.169549</td>\n",
              "      <td>24.169549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1220</th>\n",
              "      <td>1220</td>\n",
              "      <td>60.714814</td>\n",
              "      <td>5.008411</td>\n",
              "      <td>353.851197</td>\n",
              "      <td>372.033498</td>\n",
              "      <td>372.133498</td>\n",
              "      <td>372.233498</td>\n",
              "      <td>383.355500</td>\n",
              "      <td>384.264486</td>\n",
              "      <td>373.061425</td>\n",
              "      <td>376.533844</td>\n",
              "      <td>16297.907810</td>\n",
              "      <td>15837.684565</td>\n",
              "      <td>16396.691283</td>\n",
              "      <td>24.896387</td>\n",
              "      <td>904.163432</td>\n",
              "      <td>191.150086</td>\n",
              "      <td>223.839677</td>\n",
              "      <td>296.661660</td>\n",
              "      <td>364.422675</td>\n",
              "      <td>429.513198</td>\n",
              "      <td>499.377411</td>\n",
              "      <td>517.174705</td>\n",
              "      <td>535.043315</td>\n",
              "      <td>2.422390</td>\n",
              "      <td>28.785679</td>\n",
              "      <td>2.182196</td>\n",
              "      <td>0.463303</td>\n",
              "      <td>2.329810</td>\n",
              "      <td>2.135011</td>\n",
              "      <td>9.176815</td>\n",
              "      <td>16.329690</td>\n",
              "      <td>30.176577</td>\n",
              "      <td>25.381843</td>\n",
              "      <td>2.422390</td>\n",
              "      <td>749.446056</td>\n",
              "      <td>1.477786</td>\n",
              "      <td>24.896387</td>\n",
              "      <td>24.896387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>562</th>\n",
              "      <td>562</td>\n",
              "      <td>60.431232</td>\n",
              "      <td>3.723908</td>\n",
              "      <td>351.714024</td>\n",
              "      <td>368.632488</td>\n",
              "      <td>368.732488</td>\n",
              "      <td>368.832488</td>\n",
              "      <td>380.926395</td>\n",
              "      <td>382.361671</td>\n",
              "      <td>376.712812</td>\n",
              "      <td>381.628154</td>\n",
              "      <td>16468.441805</td>\n",
              "      <td>15810.801249</td>\n",
              "      <td>16054.528905</td>\n",
              "      <td>31.276385</td>\n",
              "      <td>900.425153</td>\n",
              "      <td>201.865424</td>\n",
              "      <td>234.264272</td>\n",
              "      <td>300.586945</td>\n",
              "      <td>368.239171</td>\n",
              "      <td>434.766899</td>\n",
              "      <td>503.542197</td>\n",
              "      <td>516.613666</td>\n",
              "      <td>535.332856</td>\n",
              "      <td>2.677398</td>\n",
              "      <td>26.611099</td>\n",
              "      <td>2.481080</td>\n",
              "      <td>0.502459</td>\n",
              "      <td>2.494558</td>\n",
              "      <td>2.264059</td>\n",
              "      <td>9.834466</td>\n",
              "      <td>17.798035</td>\n",
              "      <td>31.191351</td>\n",
              "      <td>24.137092</td>\n",
              "      <td>2.677398</td>\n",
              "      <td>648.422761</td>\n",
              "      <td>1.553126</td>\n",
              "      <td>31.276385</td>\n",
              "      <td>31.276385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1184</th>\n",
              "      <td>1184</td>\n",
              "      <td>62.642082</td>\n",
              "      <td>3.336368</td>\n",
              "      <td>352.303088</td>\n",
              "      <td>370.943359</td>\n",
              "      <td>371.043359</td>\n",
              "      <td>371.143359</td>\n",
              "      <td>383.843594</td>\n",
              "      <td>384.819048</td>\n",
              "      <td>373.388208</td>\n",
              "      <td>376.401322</td>\n",
              "      <td>16742.686496</td>\n",
              "      <td>15899.846160</td>\n",
              "      <td>16035.629787</td>\n",
              "      <td>23.221688</td>\n",
              "      <td>906.984537</td>\n",
              "      <td>218.238445</td>\n",
              "      <td>250.183126</td>\n",
              "      <td>308.267322</td>\n",
              "      <td>371.144790</td>\n",
              "      <td>436.116901</td>\n",
              "      <td>497.777754</td>\n",
              "      <td>510.358767</td>\n",
              "      <td>526.539692</td>\n",
              "      <td>2.422990</td>\n",
              "      <td>34.345018</td>\n",
              "      <td>2.195251</td>\n",
              "      <td>0.428151</td>\n",
              "      <td>2.000175</td>\n",
              "      <td>1.823535</td>\n",
              "      <td>7.723815</td>\n",
              "      <td>14.334383</td>\n",
              "      <td>29.586952</td>\n",
              "      <td>30.388856</td>\n",
              "      <td>2.422990</td>\n",
              "      <td>926.674650</td>\n",
              "      <td>1.424016</td>\n",
              "      <td>23.221688</td>\n",
              "      <td>23.221688</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         0          1         2  ...        36         37         38\n",
              "1719  1719  60.092708  2.450414  ...  1.673306  23.751782  23.751782\n",
              "175    175  64.258950  3.468767  ...  1.679571  24.169549  24.169549\n",
              "1220  1220  60.714814  5.008411  ...  1.477786  24.896387  24.896387\n",
              "562    562  60.431232  3.723908  ...  1.553126  31.276385  31.276385\n",
              "1184  1184  62.642082  3.336368  ...  1.424016  23.221688  23.221688\n",
              "\n",
              "[5 rows x 39 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "LrDwViAi4G-u",
        "outputId": "bffd98da-4154-4539-812b-3dc25b769363"
      },
      "source": [
        "a = list(range(0,39))\n",
        "for i in range(39):\n",
        "  a[i] = str(a[i])\n",
        "train_data.columns = a\n",
        "del train_data[a[0]]\n",
        "train_data2 = copy.copy(train_data)\n",
        "train_data.head()\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1719</th>\n",
              "      <td>60.092708</td>\n",
              "      <td>2.450414</td>\n",
              "      <td>358.371974</td>\n",
              "      <td>375.934213</td>\n",
              "      <td>376.034213</td>\n",
              "      <td>376.134213</td>\n",
              "      <td>389.540940</td>\n",
              "      <td>390.038226</td>\n",
              "      <td>361.934265</td>\n",
              "      <td>368.777962</td>\n",
              "      <td>16451.945640</td>\n",
              "      <td>16006.661109</td>\n",
              "      <td>16141.133117</td>\n",
              "      <td>23.751782</td>\n",
              "      <td>914.046450</td>\n",
              "      <td>205.595687</td>\n",
              "      <td>240.003288</td>\n",
              "      <td>301.227269</td>\n",
              "      <td>369.319859</td>\n",
              "      <td>434.678012</td>\n",
              "      <td>499.644025</td>\n",
              "      <td>512.319534</td>\n",
              "      <td>530.509493</td>\n",
              "      <td>2.716414</td>\n",
              "      <td>27.626445</td>\n",
              "      <td>2.523027</td>\n",
              "      <td>0.518742</td>\n",
              "      <td>2.708843</td>\n",
              "      <td>2.478947</td>\n",
              "      <td>10.607617</td>\n",
              "      <td>18.208807</td>\n",
              "      <td>28.749294</td>\n",
              "      <td>25.115384</td>\n",
              "      <td>2.716414</td>\n",
              "      <td>685.208560</td>\n",
              "      <td>1.673306</td>\n",
              "      <td>23.751782</td>\n",
              "      <td>23.751782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>64.258950</td>\n",
              "      <td>3.468767</td>\n",
              "      <td>355.820686</td>\n",
              "      <td>374.795280</td>\n",
              "      <td>374.895280</td>\n",
              "      <td>374.995280</td>\n",
              "      <td>386.660531</td>\n",
              "      <td>388.649380</td>\n",
              "      <td>368.073805</td>\n",
              "      <td>372.681118</td>\n",
              "      <td>16571.888132</td>\n",
              "      <td>16029.925800</td>\n",
              "      <td>16108.259279</td>\n",
              "      <td>24.169549</td>\n",
              "      <td>931.528794</td>\n",
              "      <td>191.141473</td>\n",
              "      <td>222.110697</td>\n",
              "      <td>292.224902</td>\n",
              "      <td>355.126954</td>\n",
              "      <td>426.040480</td>\n",
              "      <td>490.982001</td>\n",
              "      <td>505.155588</td>\n",
              "      <td>523.916752</td>\n",
              "      <td>1.947339</td>\n",
              "      <td>28.159291</td>\n",
              "      <td>1.754061</td>\n",
              "      <td>0.478342</td>\n",
              "      <td>2.707570</td>\n",
              "      <td>2.408400</td>\n",
              "      <td>9.578486</td>\n",
              "      <td>15.685092</td>\n",
              "      <td>30.622116</td>\n",
              "      <td>24.785759</td>\n",
              "      <td>1.947339</td>\n",
              "      <td>930.071026</td>\n",
              "      <td>1.679571</td>\n",
              "      <td>24.169549</td>\n",
              "      <td>24.169549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1220</th>\n",
              "      <td>60.714814</td>\n",
              "      <td>5.008411</td>\n",
              "      <td>353.851197</td>\n",
              "      <td>372.033498</td>\n",
              "      <td>372.133498</td>\n",
              "      <td>372.233498</td>\n",
              "      <td>383.355500</td>\n",
              "      <td>384.264486</td>\n",
              "      <td>373.061425</td>\n",
              "      <td>376.533844</td>\n",
              "      <td>16297.907810</td>\n",
              "      <td>15837.684565</td>\n",
              "      <td>16396.691283</td>\n",
              "      <td>24.896387</td>\n",
              "      <td>904.163432</td>\n",
              "      <td>191.150086</td>\n",
              "      <td>223.839677</td>\n",
              "      <td>296.661660</td>\n",
              "      <td>364.422675</td>\n",
              "      <td>429.513198</td>\n",
              "      <td>499.377411</td>\n",
              "      <td>517.174705</td>\n",
              "      <td>535.043315</td>\n",
              "      <td>2.422390</td>\n",
              "      <td>28.785679</td>\n",
              "      <td>2.182196</td>\n",
              "      <td>0.463303</td>\n",
              "      <td>2.329810</td>\n",
              "      <td>2.135011</td>\n",
              "      <td>9.176815</td>\n",
              "      <td>16.329690</td>\n",
              "      <td>30.176577</td>\n",
              "      <td>25.381843</td>\n",
              "      <td>2.422390</td>\n",
              "      <td>749.446056</td>\n",
              "      <td>1.477786</td>\n",
              "      <td>24.896387</td>\n",
              "      <td>24.896387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>562</th>\n",
              "      <td>60.431232</td>\n",
              "      <td>3.723908</td>\n",
              "      <td>351.714024</td>\n",
              "      <td>368.632488</td>\n",
              "      <td>368.732488</td>\n",
              "      <td>368.832488</td>\n",
              "      <td>380.926395</td>\n",
              "      <td>382.361671</td>\n",
              "      <td>376.712812</td>\n",
              "      <td>381.628154</td>\n",
              "      <td>16468.441805</td>\n",
              "      <td>15810.801249</td>\n",
              "      <td>16054.528905</td>\n",
              "      <td>31.276385</td>\n",
              "      <td>900.425153</td>\n",
              "      <td>201.865424</td>\n",
              "      <td>234.264272</td>\n",
              "      <td>300.586945</td>\n",
              "      <td>368.239171</td>\n",
              "      <td>434.766899</td>\n",
              "      <td>503.542197</td>\n",
              "      <td>516.613666</td>\n",
              "      <td>535.332856</td>\n",
              "      <td>2.677398</td>\n",
              "      <td>26.611099</td>\n",
              "      <td>2.481080</td>\n",
              "      <td>0.502459</td>\n",
              "      <td>2.494558</td>\n",
              "      <td>2.264059</td>\n",
              "      <td>9.834466</td>\n",
              "      <td>17.798035</td>\n",
              "      <td>31.191351</td>\n",
              "      <td>24.137092</td>\n",
              "      <td>2.677398</td>\n",
              "      <td>648.422761</td>\n",
              "      <td>1.553126</td>\n",
              "      <td>31.276385</td>\n",
              "      <td>31.276385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1184</th>\n",
              "      <td>62.642082</td>\n",
              "      <td>3.336368</td>\n",
              "      <td>352.303088</td>\n",
              "      <td>370.943359</td>\n",
              "      <td>371.043359</td>\n",
              "      <td>371.143359</td>\n",
              "      <td>383.843594</td>\n",
              "      <td>384.819048</td>\n",
              "      <td>373.388208</td>\n",
              "      <td>376.401322</td>\n",
              "      <td>16742.686496</td>\n",
              "      <td>15899.846160</td>\n",
              "      <td>16035.629787</td>\n",
              "      <td>23.221688</td>\n",
              "      <td>906.984537</td>\n",
              "      <td>218.238445</td>\n",
              "      <td>250.183126</td>\n",
              "      <td>308.267322</td>\n",
              "      <td>371.144790</td>\n",
              "      <td>436.116901</td>\n",
              "      <td>497.777754</td>\n",
              "      <td>510.358767</td>\n",
              "      <td>526.539692</td>\n",
              "      <td>2.422990</td>\n",
              "      <td>34.345018</td>\n",
              "      <td>2.195251</td>\n",
              "      <td>0.428151</td>\n",
              "      <td>2.000175</td>\n",
              "      <td>1.823535</td>\n",
              "      <td>7.723815</td>\n",
              "      <td>14.334383</td>\n",
              "      <td>29.586952</td>\n",
              "      <td>30.388856</td>\n",
              "      <td>2.422990</td>\n",
              "      <td>926.674650</td>\n",
              "      <td>1.424016</td>\n",
              "      <td>23.221688</td>\n",
              "      <td>23.221688</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              1         2           3  ...        36         37         38\n",
              "1719  60.092708  2.450414  358.371974  ...  1.673306  23.751782  23.751782\n",
              "175   64.258950  3.468767  355.820686  ...  1.679571  24.169549  24.169549\n",
              "1220  60.714814  5.008411  353.851197  ...  1.477786  24.896387  24.896387\n",
              "562   60.431232  3.723908  351.714024  ...  1.553126  31.276385  31.276385\n",
              "1184  62.642082  3.336368  352.303088  ...  1.424016  23.221688  23.221688\n",
              "\n",
              "[5 rows x 38 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "J9Sx7PNO-HO1",
        "outputId": "69fa86e3-db0b-4182-9c40-4c3693ad11f6"
      },
      "source": [
        "a = list(range(0,38))\n",
        "for i in range(38):\n",
        "  a[i] = str(a[i])\n",
        "train_data2.columns = a\n",
        "dropid = [a[22],a[23],a[24],a[25],a[26],a[27],a[28],a[29],a[30],a[31],a[32],a[33]]\n",
        "target = train_data.loc[:,a[22]:a[33]]\n",
        "\n",
        "del train_data2[a[22]]\n",
        "del train_data2[a[23]]\n",
        "del train_data2[a[24]]\n",
        "del train_data2[a[25]]\n",
        "del train_data2[a[26]]\n",
        "del train_data2[a[27]]\n",
        "del train_data2[a[28]]\n",
        "del train_data2[a[29]]\n",
        "del train_data2[a[30]]\n",
        "del train_data2[a[31]]\n",
        "del train_data2[a[32]]\n",
        "del train_data2[a[33]]\n",
        "train_data2.head()\n",
        "target.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1719</th>\n",
              "      <td>512.319534</td>\n",
              "      <td>530.509493</td>\n",
              "      <td>2.716414</td>\n",
              "      <td>27.626445</td>\n",
              "      <td>2.523027</td>\n",
              "      <td>0.518742</td>\n",
              "      <td>2.708843</td>\n",
              "      <td>2.478947</td>\n",
              "      <td>10.607617</td>\n",
              "      <td>18.208807</td>\n",
              "      <td>28.749294</td>\n",
              "      <td>25.115384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>505.155588</td>\n",
              "      <td>523.916752</td>\n",
              "      <td>1.947339</td>\n",
              "      <td>28.159291</td>\n",
              "      <td>1.754061</td>\n",
              "      <td>0.478342</td>\n",
              "      <td>2.707570</td>\n",
              "      <td>2.408400</td>\n",
              "      <td>9.578486</td>\n",
              "      <td>15.685092</td>\n",
              "      <td>30.622116</td>\n",
              "      <td>24.785759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1220</th>\n",
              "      <td>517.174705</td>\n",
              "      <td>535.043315</td>\n",
              "      <td>2.422390</td>\n",
              "      <td>28.785679</td>\n",
              "      <td>2.182196</td>\n",
              "      <td>0.463303</td>\n",
              "      <td>2.329810</td>\n",
              "      <td>2.135011</td>\n",
              "      <td>9.176815</td>\n",
              "      <td>16.329690</td>\n",
              "      <td>30.176577</td>\n",
              "      <td>25.381843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>562</th>\n",
              "      <td>516.613666</td>\n",
              "      <td>535.332856</td>\n",
              "      <td>2.677398</td>\n",
              "      <td>26.611099</td>\n",
              "      <td>2.481080</td>\n",
              "      <td>0.502459</td>\n",
              "      <td>2.494558</td>\n",
              "      <td>2.264059</td>\n",
              "      <td>9.834466</td>\n",
              "      <td>17.798035</td>\n",
              "      <td>31.191351</td>\n",
              "      <td>24.137092</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1184</th>\n",
              "      <td>510.358767</td>\n",
              "      <td>526.539692</td>\n",
              "      <td>2.422990</td>\n",
              "      <td>34.345018</td>\n",
              "      <td>2.195251</td>\n",
              "      <td>0.428151</td>\n",
              "      <td>2.000175</td>\n",
              "      <td>1.823535</td>\n",
              "      <td>7.723815</td>\n",
              "      <td>14.334383</td>\n",
              "      <td>29.586952</td>\n",
              "      <td>30.388856</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              22          23        24  ...         31         32         33\n",
              "1719  512.319534  530.509493  2.716414  ...  18.208807  28.749294  25.115384\n",
              "175   505.155588  523.916752  1.947339  ...  15.685092  30.622116  24.785759\n",
              "1220  517.174705  535.043315  2.422390  ...  16.329690  30.176577  25.381843\n",
              "562   516.613666  535.332856  2.677398  ...  17.798035  31.191351  24.137092\n",
              "1184  510.358767  526.539692  2.422990  ...  14.334383  29.586952  30.388856\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJbrJ9sx5PZG"
      },
      "source": [
        "train_x,test_x,train_y,test_y = train_test_split(train_data2,target,test_size=0.3,random_state=17)\n",
        "train = pd.concat([train_x,train_y],axis=1)\n",
        "test = pd.concat([test_x,test_y],axis=1)\n",
        "train.to_csv('train.csv',index = False)\n",
        "\n",
        "test.to_csv('test.csv',index = False)"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swb_MXHhCR7e"
      },
      "source": [
        "train_need = TabularDataset(\"/content/train.csv\")\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiTR83Jy1Nme",
        "outputId": "8fc4e501-bb03-4d19-eec1-28fe2c93e9fd"
      },
      "source": [
        "train_need = TabularDataset(\"/content/train.csv\")\n",
        "test_need = TabularDataset(\"/content/test .csv\")\n",
        "# train_need = pd.read_csv(\"/content/train.csv\")\n",
        "# test_need = pd.read_csv(\"/content/test.csv\")\n",
        "\n",
        "problem_types=['regression','regression','regression','regression','regression','regression','regression','regression','regression','regression','regression','regression']\n",
        "save_path = 'agModels-predictEducationClass'\n",
        "time_limit = 10\n",
        "\n",
        "multi_predictor = MultilabelPredictor(labels=labels, problem_types=problem_types, path=save_path)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loaded data from: /content/train.csv | Columns = 38 / 38 | Rows = 1470 -> 1470\n",
            "Loaded data from: /content/test .csv | Columns = 38 / 38 | Rows = 630 -> 630\n",
            "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"agModels-predictEducationClass/Predictor_22\"\n",
            "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"agModels-predictEducationClass/Predictor_23\"\n",
            "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"agModels-predictEducationClass/Predictor_24\"\n",
            "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"agModels-predictEducationClass/Predictor_25\"\n",
            "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"agModels-predictEducationClass/Predictor_26\"\n",
            "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"agModels-predictEducationClass/Predictor_27\"\n",
            "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"agModels-predictEducationClass/Predictor_28\"\n",
            "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"agModels-predictEducationClass/Predictor_29\"\n",
            "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"agModels-predictEducationClass/Predictor_30\"\n",
            "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"agModels-predictEducationClass/Predictor_31\"\n",
            "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"agModels-predictEducationClass/Predictor_32\"\n",
            "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"agModels-predictEducationClass/Predictor_33\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkKpVjvHDZwu",
        "outputId": "470fe64b-407b-4a6f-e42f-e7dc1bf85f5a"
      },
      "source": [
        "train_need\n",
        "type(train_need)\n",
        "multi_predictor.fit(train_need,time_limit=time_limit)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Beginning AutoGluon training ... Time limit = 10s\n",
            "AutoGluon will save models to \"agModels-predictEducationClass/Predictor_22/\"\n",
            "AutoGluon Version:  0.3.1\n",
            "Train Data Rows:    1470\n",
            "Train Data Columns: 26\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    12709.4 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.31 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 26 | ['0', '1', '2', '3', '4', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 26 | ['0', '1', '2', '3', '4', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t26 features in original data used to generate 26 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.31 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.14s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tTo change this, specify the eval_metric argument of fit()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1176, Val Rows: 294\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ... Training model for up to 9.86s of the 9.85s of remaining time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting TabularPredictor for label: 22 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\t-3.8474\t = Validation score   (root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ... Training model for up to 9.73s of the 9.72s of remaining time.\n",
            "\t-2.0771\t = Validation score   (root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ... Training model for up to 9.58s of the 9.57s of remaining time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\ttrain_set's rmse: 0.0133716\tvalid_set's rmse: 0.144211\n",
            "[2000]\ttrain_set's rmse: 0.00150372\tvalid_set's rmse: 0.142895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\t-0.1429\t = Validation score   (root_mean_squared_error)\n",
            "\t3.91s\t = Training   runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitting model: LightGBM ... Training model for up to 5.19s of the 5.18s of remaining time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\ttrain_set's rmse: 0.0115641\tvalid_set's rmse: 0.0713522\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\tRan out of time, early stopping on iteration 1900. Best iteration is:\n",
            "\t[1900]\ttrain_set's rmse: 0.00402093\tvalid_set's rmse: 0.0693138\n",
            "\t-0.0693\t = Validation score   (root_mean_squared_error)\n",
            "\t5.59s\t = Training   runtime\n",
            "\t0.07s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.86s of the -1.3s of remaining time.\n",
            "\t-0.0448\t = Validation score   (root_mean_squared_error)\n",
            "\t0.14s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 11.48s ...\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"agModels-predictEducationClass/Predictor_22/\")\n",
            "Beginning AutoGluon training ... Time limit = 10s\n",
            "AutoGluon will save models to \"agModels-predictEducationClass/Predictor_23/\"\n",
            "AutoGluon Version:  0.3.1\n",
            "Train Data Rows:    1470\n",
            "Train Data Columns: 27\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    12709.98 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.32 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 27 | ['0', '1', '2', '3', '4', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 27 | ['0', '1', '2', '3', '4', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t27 features in original data used to generate 27 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.32 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.12s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tTo change this, specify the eval_metric argument of fit()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1176, Val Rows: 294\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ... Training model for up to 9.88s of the 9.87s of remaining time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting TabularPredictor for label: 23 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\t-3.9096\t = Validation score   (root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ... Training model for up to 9.73s of the 9.73s of remaining time.\n",
            "\t-2.2186\t = Validation score   (root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ... Training model for up to 9.6s of the 9.6s of remaining time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\ttrain_set's rmse: 0.0290479\tvalid_set's rmse: 0.367515\n",
            "[2000]\ttrain_set's rmse: 0.00262687\tvalid_set's rmse: 0.365627\n",
            "[3000]\ttrain_set's rmse: 0.000257351\tvalid_set's rmse: 0.365585\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\t-0.3656\t = Validation score   (root_mean_squared_error)\n",
            "\t5.54s\t = Training   runtime\n",
            "\t0.07s\t = Validation runtime\n",
            "Fitting model: LightGBM ... Training model for up to 3.43s of the 3.42s of remaining time.\n",
            "\t-0.3411\t = Validation score   (root_mean_squared_error)\n",
            "\t1.61s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE ... Training model for up to 1.73s of the 1.72s of remaining time.\n",
            "\t-0.4148\t = Validation score   (root_mean_squared_error)\n",
            "\t2.86s\t = Training   runtime\n",
            "\t0.1s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.88s of the -1.85s of remaining time.\n",
            "\t-0.3329\t = Validation score   (root_mean_squared_error)\n",
            "\t0.18s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 12.06s ...\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"agModels-predictEducationClass/Predictor_23/\")\n",
            "Beginning AutoGluon training ... Time limit = 10s\n",
            "AutoGluon will save models to \"agModels-predictEducationClass/Predictor_24/\"\n",
            "AutoGluon Version:  0.3.1\n",
            "Train Data Rows:    1470\n",
            "Train Data Columns: 28\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    12709.2 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.33 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 28 | ['0', '1', '2', '3', '4', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 28 | ['0', '1', '2', '3', '4', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t28 features in original data used to generate 28 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.14s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tTo change this, specify the eval_metric argument of fit()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1176, Val Rows: 294\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ... Training model for up to 9.86s of the 9.86s of remaining time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting TabularPredictor for label: 24 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\t-0.2816\t = Validation score   (root_mean_squared_error)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ... Training model for up to 9.72s of the 9.72s of remaining time.\n",
            "\t-0.1716\t = Validation score   (root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ... Training model for up to 9.58s of the 9.58s of remaining time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\ttrain_set's rmse: 0.0107984\tvalid_set's rmse: 0.152092\n",
            "[2000]\ttrain_set's rmse: 0.00107223\tvalid_set's rmse: 0.151354\n",
            "[3000]\ttrain_set's rmse: 0.000119555\tvalid_set's rmse: 0.151313\n",
            "[4000]\ttrain_set's rmse: 1.4655e-05\tvalid_set's rmse: 0.151311\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\t-0.1513\t = Validation score   (root_mean_squared_error)\n",
            "\t7.13s\t = Training   runtime\n",
            "\t0.1s\t = Validation runtime\n",
            "Fitting model: LightGBM ... Training model for up to 1.66s of the 1.66s of remaining time.\n",
            "\tRan out of time, early stopping on iteration 577. Best iteration is:\n",
            "\t[577]\ttrain_set's rmse: 0.0072168\tvalid_set's rmse: 0.148838\n",
            "\t-0.1488\t = Validation score   (root_mean_squared_error)\n",
            "\t1.8s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.86s of the -0.75s of remaining time.\n",
            "\t-0.1482\t = Validation score   (root_mean_squared_error)\n",
            "\t0.15s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 10.94s ...\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"agModels-predictEducationClass/Predictor_24/\")\n",
            "Beginning AutoGluon training ... Time limit = 10s\n",
            "AutoGluon will save models to \"agModels-predictEducationClass/Predictor_25/\"\n",
            "AutoGluon Version:  0.3.1\n",
            "Train Data Rows:    1470\n",
            "Train Data Columns: 29\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    12707.44 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.34 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 29 | ['0', '1', '2', '3', '4', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 29 | ['0', '1', '2', '3', '4', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t29 features in original data used to generate 29 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.34 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.15s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tTo change this, specify the eval_metric argument of fit()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1176, Val Rows: 294\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ... Training model for up to 9.85s of the 9.85s of remaining time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting TabularPredictor for label: 25 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\t-5.8489\t = Validation score   (root_mean_squared_error)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ... Training model for up to 9.72s of the 9.71s of remaining time.\n",
            "\t-2.0671\t = Validation score   (root_mean_squared_error)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ... Training model for up to 9.58s of the 9.58s of remaining time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\ttrain_set's rmse: 0.0409798\tvalid_set's rmse: 0.548536\n",
            "[2000]\ttrain_set's rmse: 0.00329756\tvalid_set's rmse: 0.546647\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\t-0.5466\t = Validation score   (root_mean_squared_error)\n",
            "\t3.36s\t = Training   runtime\n",
            "\t0.07s\t = Validation runtime\n",
            "Fitting model: LightGBM ... Training model for up to 5.82s of the 5.82s of remaining time.\n",
            "\t-0.6648\t = Validation score   (root_mean_squared_error)\n",
            "\t2.31s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE ... Training model for up to 3.39s of the 3.39s of remaining time.\n",
            "\t-0.8767\t = Validation score   (root_mean_squared_error)\n",
            "\t2.95s\t = Training   runtime\n",
            "\t0.1s\t = Validation runtime\n",
            "Fitting model: CatBoost ... Training model for up to 0.3s of the 0.3s of remaining time.\n",
            "\t-2.1967\t = Validation score   (root_mean_squared_error)\n",
            "\t0.5s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.85s of the -0.69s of remaining time.\n",
            "\t-0.509\t = Validation score   (root_mean_squared_error)\n",
            "\t0.21s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 10.94s ...\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"agModels-predictEducationClass/Predictor_25/\")\n",
            "Beginning AutoGluon training ... Time limit = 10s\n",
            "AutoGluon will save models to \"agModels-predictEducationClass/Predictor_26/\"\n",
            "AutoGluon Version:  0.3.1\n",
            "Train Data Rows:    1470\n",
            "Train Data Columns: 30\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    12692.09 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.35 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 30 | ['0', '1', '2', '3', '4', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 30 | ['0', '1', '2', '3', '4', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t30 features in original data used to generate 30 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.35 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.13s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tTo change this, specify the eval_metric argument of fit()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1176, Val Rows: 294\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ... Training model for up to 9.87s of the 9.86s of remaining time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting TabularPredictor for label: 26 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\t-0.2501\t = Validation score   (root_mean_squared_error)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ... Training model for up to 9.71s of the 9.71s of remaining time.\n",
            "\t-0.1498\t = Validation score   (root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ... Training model for up to 9.57s of the 9.57s of remaining time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\ttrain_set's rmse: 0.00108647\tvalid_set's rmse: 0.0174111\n",
            "[2000]\ttrain_set's rmse: 8.97236e-05\tvalid_set's rmse: 0.0173177\n",
            "[3000]\ttrain_set's rmse: 9.19396e-06\tvalid_set's rmse: 0.0173133\n",
            "[4000]\ttrain_set's rmse: 1.24655e-06\tvalid_set's rmse: 0.017313\n",
            "[5000]\ttrain_set's rmse: 2.12139e-07\tvalid_set's rmse: 0.017313\n",
            "[6000]\ttrain_set's rmse: 3.96003e-08\tvalid_set's rmse: 0.017313\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\tRan out of time, early stopping on iteration 6771. Best iteration is:\n",
            "\t[6724]\ttrain_set's rmse: 1.21312e-08\tvalid_set's rmse: 0.017313\n",
            "\t-0.0173\t = Validation score   (root_mean_squared_error)\n",
            "\t10.95s\t = Training   runtime\n",
            "\t0.18s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.87s of the -3.35s of remaining time.\n",
            "\t-0.0173\t = Validation score   (root_mean_squared_error)\n",
            "\t0.1s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 13.48s ...\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"agModels-predictEducationClass/Predictor_26/\")\n",
            "Beginning AutoGluon training ... Time limit = 10s\n",
            "AutoGluon will save models to \"agModels-predictEducationClass/Predictor_27/\"\n",
            "AutoGluon Version:  0.3.1\n",
            "Train Data Rows:    1470\n",
            "Train Data Columns: 31\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    12691.25 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.36 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 31 | ['0', '1', '2', '3', '4', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 31 | ['0', '1', '2', '3', '4', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t31 features in original data used to generate 31 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.36 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.12s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tTo change this, specify the eval_metric argument of fit()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1176, Val Rows: 294\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ... Training model for up to 9.88s of the 9.87s of remaining time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting TabularPredictor for label: 27 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\t-0.057\t = Validation score   (root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.14s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ... Training model for up to 9.71s of the 9.71s of remaining time.\n",
            "\t-0.0199\t = Validation score   (root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ... Training model for up to 9.57s of the 9.57s of remaining time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\ttrain_set's rmse: 0.000260407\tvalid_set's rmse: 0.0018461\n",
            "[2000]\ttrain_set's rmse: 3.18174e-05\tvalid_set's rmse: 0.00183074\n",
            "[3000]\ttrain_set's rmse: 4.87897e-06\tvalid_set's rmse: 0.00183007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\t-0.0018\t = Validation score   (root_mean_squared_error)\n",
            "\t6.66s\t = Training   runtime\n",
            "\t0.08s\t = Validation runtime\n",
            "Fitting model: LightGBM ... Training model for up to 2.21s of the 2.2s of remaining time.\n",
            "\tRan out of time, early stopping on iteration 692. Best iteration is:\n",
            "\t[691]\ttrain_set's rmse: 0.000672184\tvalid_set's rmse: 0.00292679\n",
            "\t-0.0029\t = Validation score   (root_mean_squared_error)\n",
            "\t2.37s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.88s of the -0.79s of remaining time.\n",
            "\t-0.0018\t = Validation score   (root_mean_squared_error)\n",
            "\t0.15s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 10.99s ...\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"agModels-predictEducationClass/Predictor_27/\")\n",
            "Beginning AutoGluon training ... Time limit = 10s\n",
            "AutoGluon will save models to \"agModels-predictEducationClass/Predictor_28/\"\n",
            "AutoGluon Version:  0.3.1\n",
            "Train Data Rows:    1470\n",
            "Train Data Columns: 32\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    12690.44 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.38 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 32 | ['0', '1', '2', '3', '4', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 32 | ['0', '1', '2', '3', '4', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t32 features in original data used to generate 32 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.38 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.13s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tTo change this, specify the eval_metric argument of fit()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1176, Val Rows: 294\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ... Training model for up to 9.87s of the 9.86s of remaining time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting TabularPredictor for label: 28 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\t-0.5271\t = Validation score   (root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ... Training model for up to 9.72s of the 9.72s of remaining time.\n",
            "\t-0.2233\t = Validation score   (root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ... Training model for up to 9.58s of the 9.57s of remaining time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\ttrain_set's rmse: 0.00246832\tvalid_set's rmse: 0.0154502\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\t-0.0154\t = Validation score   (root_mean_squared_error)\n",
            "\t2.7s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: LightGBM ... Training model for up to 6.59s of the 6.58s of remaining time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\ttrain_set's rmse: 0.00217429\tvalid_set's rmse: 0.0200773\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\t-0.0201\t = Validation score   (root_mean_squared_error)\n",
            "\t3.82s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE ... Training model for up to 2.54s of the 2.54s of remaining time.\n",
            "\t-0.0421\t = Validation score   (root_mean_squared_error)\n",
            "\t3.04s\t = Training   runtime\n",
            "\t0.1s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.87s of the -1.16s of remaining time.\n",
            "\t-0.0123\t = Validation score   (root_mean_squared_error)\n",
            "\t0.17s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 11.36s ...\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"agModels-predictEducationClass/Predictor_28/\")\n",
            "Beginning AutoGluon training ... Time limit = 10s\n",
            "AutoGluon will save models to \"agModels-predictEducationClass/Predictor_29/\"\n",
            "AutoGluon Version:  0.3.1\n",
            "Train Data Rows:    1470\n",
            "Train Data Columns: 33\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    12690.4 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.39 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 33 | ['0', '1', '2', '3', '4', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 33 | ['0', '1', '2', '3', '4', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t33 features in original data used to generate 33 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.39 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.13s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tTo change this, specify the eval_metric argument of fit()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1176, Val Rows: 294\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ... Training model for up to 9.87s of the 9.87s of remaining time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting TabularPredictor for label: 29 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\t-0.4786\t = Validation score   (root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.12s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ... Training model for up to 9.72s of the 9.71s of remaining time.\n",
            "\t-0.2011\t = Validation score   (root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ... Training model for up to 9.59s of the 9.58s of remaining time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\ttrain_set's rmse: 0.00194352\tvalid_set's rmse: 0.0130982\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\t-0.0131\t = Validation score   (root_mean_squared_error)\n",
            "\t2.14s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: LightGBM ... Training model for up to 7.22s of the 7.22s of remaining time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\ttrain_set's rmse: 0.00122664\tvalid_set's rmse: 0.00890716\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\t-0.0089\t = Validation score   (root_mean_squared_error)\n",
            "\t5.06s\t = Training   runtime\n",
            "\t0.04s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE ... Training model for up to 1.89s of the 1.89s of remaining time.\n",
            "\t-0.0158\t = Validation score   (root_mean_squared_error)\n",
            "\t3.05s\t = Training   runtime\n",
            "\t0.1s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.87s of the -1.8s of remaining time.\n",
            "\t-0.0073\t = Validation score   (root_mean_squared_error)\n",
            "\t0.18s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 12.03s ...\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"agModels-predictEducationClass/Predictor_29/\")\n",
            "Beginning AutoGluon training ... Time limit = 10s\n",
            "AutoGluon will save models to \"agModels-predictEducationClass/Predictor_30/\"\n",
            "AutoGluon Version:  0.3.1\n",
            "Train Data Rows:    1470\n",
            "Train Data Columns: 34\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    12690.72 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.4 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 34 | ['0', '1', '2', '3', '4', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 34 | ['0', '1', '2', '3', '4', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t34 features in original data used to generate 34 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.4 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.17s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tTo change this, specify the eval_metric argument of fit()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1176, Val Rows: 294\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ... Training model for up to 9.83s of the 9.83s of remaining time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting TabularPredictor for label: 30 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\t-2.1485\t = Validation score   (root_mean_squared_error)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ... Training model for up to 9.68s of the 9.68s of remaining time.\n",
            "\t-0.829\t = Validation score   (root_mean_squared_error)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ... Training model for up to 9.54s of the 9.54s of remaining time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\ttrain_set's rmse: 0.00639477\tvalid_set's rmse: 0.0579977\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\t-0.0579\t = Validation score   (root_mean_squared_error)\n",
            "\t2.49s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: LightGBM ... Training model for up to 6.8s of the 6.8s of remaining time.\n",
            "\t-0.073\t = Validation score   (root_mean_squared_error)\n",
            "\t3.33s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE ... Training model for up to 3.3s of the 3.29s of remaining time.\n",
            "\t-0.1121\t = Validation score   (root_mean_squared_error)\n",
            "\t3.24s\t = Training   runtime\n",
            "\t0.1s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.83s of the -0.57s of remaining time.\n",
            "\t-0.0504\t = Validation score   (root_mean_squared_error)\n",
            "\t0.16s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 10.76s ...\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"agModels-predictEducationClass/Predictor_30/\")\n",
            "Beginning AutoGluon training ... Time limit = 10s\n",
            "AutoGluon will save models to \"agModels-predictEducationClass/Predictor_31/\"\n",
            "AutoGluon Version:  0.3.1\n",
            "Train Data Rows:    1470\n",
            "Train Data Columns: 35\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    12690.76 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.41 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 35 | ['0', '1', '2', '3', '4', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 35 | ['0', '1', '2', '3', '4', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t35 features in original data used to generate 35 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.41 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.13s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tTo change this, specify the eval_metric argument of fit()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1176, Val Rows: 294\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ... Training model for up to 9.87s of the 9.86s of remaining time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting TabularPredictor for label: 31 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\t-2.7863\t = Validation score   (root_mean_squared_error)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ... Training model for up to 9.72s of the 9.71s of remaining time.\n",
            "\t-0.9054\t = Validation score   (root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ... Training model for up to 9.58s of the 9.57s of remaining time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\ttrain_set's rmse: 0.0127521\tvalid_set's rmse: 0.0804763\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\t-0.08\t = Validation score   (root_mean_squared_error)\n",
            "\t2.95s\t = Training   runtime\n",
            "\t0.04s\t = Validation runtime\n",
            "Fitting model: LightGBM ... Training model for up to 6.34s of the 6.33s of remaining time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\ttrain_set's rmse: 0.0166414\tvalid_set's rmse: 0.130042\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\tRan out of time, early stopping on iteration 1838. Best iteration is:\n",
            "\t[1835]\ttrain_set's rmse: 0.00313787\tvalid_set's rmse: 0.129695\n",
            "\t-0.1297\t = Validation score   (root_mean_squared_error)\n",
            "\t6.74s\t = Training   runtime\n",
            "\t0.07s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.87s of the -1.25s of remaining time.\n",
            "\t-0.0771\t = Validation score   (root_mean_squared_error)\n",
            "\t0.15s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 11.44s ...\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"agModels-predictEducationClass/Predictor_31/\")\n",
            "Beginning AutoGluon training ... Time limit = 10s\n",
            "AutoGluon will save models to \"agModels-predictEducationClass/Predictor_32/\"\n",
            "AutoGluon Version:  0.3.1\n",
            "Train Data Rows:    1470\n",
            "Train Data Columns: 36\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    12690.4 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.42 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 36 | ['0', '1', '2', '3', '4', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 36 | ['0', '1', '2', '3', '4', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t36 features in original data used to generate 36 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.42 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.15s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tTo change this, specify the eval_metric argument of fit()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1176, Val Rows: 294\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ... Training model for up to 9.85s of the 9.85s of remaining time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting TabularPredictor for label: 32 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\t-1.2594\t = Validation score   (root_mean_squared_error)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ... Training model for up to 9.7s of the 9.69s of remaining time.\n",
            "\t-0.4155\t = Validation score   (root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ... Training model for up to 9.55s of the 9.55s of remaining time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\ttrain_set's rmse: 0.0111796\tvalid_set's rmse: 0.241873\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\t-0.2416\t = Validation score   (root_mean_squared_error)\n",
            "\t2.43s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: LightGBM ... Training model for up to 6.87s of the 6.87s of remaining time.\n",
            "\t-0.2357\t = Validation score   (root_mean_squared_error)\n",
            "\t2.03s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE ... Training model for up to 4.74s of the 4.73s of remaining time.\n",
            "\t-0.3286\t = Validation score   (root_mean_squared_error)\n",
            "\t3.64s\t = Training   runtime\n",
            "\t0.1s\t = Validation runtime\n",
            "Fitting model: CatBoost ... Training model for up to 0.95s of the 0.94s of remaining time.\n",
            "\t-0.5101\t = Validation score   (root_mean_squared_error)\n",
            "\t0.97s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.85s of the -0.52s of remaining time.\n",
            "\t-0.2074\t = Validation score   (root_mean_squared_error)\n",
            "\t0.2s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 10.77s ...\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"agModels-predictEducationClass/Predictor_32/\")\n",
            "Beginning AutoGluon training ... Time limit = 10s\n",
            "AutoGluon will save models to \"agModels-predictEducationClass/Predictor_33/\"\n",
            "AutoGluon Version:  0.3.1\n",
            "Train Data Rows:    1470\n",
            "Train Data Columns: 37\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    12679.3 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.44 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 37 | ['0', '1', '2', '3', '4', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 37 | ['0', '1', '2', '3', '4', ...]\n",
            "\t0.1s = Fit runtime\n",
            "\t37 features in original data used to generate 37 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.44 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.16s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tTo change this, specify the eval_metric argument of fit()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 1176, Val Rows: 294\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting TabularPredictor for label: 33 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ... Training model for up to 9.84s of the 9.83s of remaining time.\n",
            "\t-5.0973\t = Validation score   (root_mean_squared_error)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ... Training model for up to 9.7s of the 9.7s of remaining time.\n",
            "\t-1.7887\t = Validation score   (root_mean_squared_error)\n",
            "\t0.02s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ... Training model for up to 9.55s of the 9.54s of remaining time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\ttrain_set's rmse: 0.0204758\tvalid_set's rmse: 0.298119\n",
            "[2000]\ttrain_set's rmse: 0.0026317\tvalid_set's rmse: 0.297291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\t-0.2973\t = Validation score   (root_mean_squared_error)\n",
            "\t4.39s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: LightGBM ... Training model for up to 4.73s of the 4.73s of remaining time.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1000]\ttrain_set's rmse: 0.0241972\tvalid_set's rmse: 0.130538\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\t-0.1305\t = Validation score   (root_mean_squared_error)\n",
            "\t4.17s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE ... Training model for up to 0.35s of the 0.35s of remaining time.\n",
            "\t-0.1716\t = Validation score   (root_mean_squared_error)\n",
            "\t3.46s\t = Training   runtime\n",
            "\t0.1s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.84s of the -3.79s of remaining time.\n",
            "\t-0.1305\t = Validation score   (root_mean_squared_error)\n",
            "\t0.17s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 14.01s ...\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"agModels-predictEducationClass/Predictor_33/\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultilabelPredictor saved to disk. Load with: MultilabelPredictor.load('agModels-predictEducationClass/')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPtDdYRiGYV4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cn1rMdyRDbL2",
        "outputId": "ca962fe4-1401-4aa7-f054-d80d8a6731bf"
      },
      "source": [
        "multi_predictor = MultilabelPredictor.load(save_path)  # unnecessary, just demonstrates how to load previously-trained multilabel predictor from file\n",
        "\n",
        "predictions = multi_predictor.predict(test_need)\n",
        "print(\"Predictions:  \\n\", predictions)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting with TabularPredictor for label: 22 ...\n",
            "Predicting with TabularPredictor for label: 23 ...\n",
            "Predicting with TabularPredictor for label: 24 ...\n",
            "Predicting with TabularPredictor for label: 25 ...\n",
            "Predicting with TabularPredictor for label: 26 ...\n",
            "Predicting with TabularPredictor for label: 27 ...\n",
            "Predicting with TabularPredictor for label: 28 ...\n",
            "Predicting with TabularPredictor for label: 29 ...\n",
            "Predicting with TabularPredictor for label: 30 ...\n",
            "Predicting with TabularPredictor for label: 31 ...\n",
            "Predicting with TabularPredictor for label: 32 ...\n",
            "Predicting with TabularPredictor for label: 33 ...\n",
            "Predictions:  \n",
            "              22          23        24  ...         31         32         33\n",
            "0    517.365784  533.712830  2.582777  ...  16.067055  30.229561  22.630337\n",
            "1    517.118713  534.888184  2.275843  ...  13.218695  30.909157  32.971451\n",
            "2    524.810181  539.020081  2.313254  ...  14.042908  30.148018  27.810591\n",
            "3    512.360474  529.935852  2.094831  ...  14.007771  29.722956  31.966045\n",
            "4    518.113647  535.073792  1.944117  ...  22.295183  25.649822  14.670124\n",
            "..          ...         ...       ...  ...        ...        ...        ...\n",
            "625  523.074402  538.848877  1.692033  ...  11.843736  31.179220  35.975101\n",
            "626  515.384216  532.243469  2.823765  ...  13.557640  28.395899  29.506716\n",
            "627  515.673523  534.618103  2.993158  ...  13.863897  29.324123  33.355320\n",
            "628  514.227661  528.345764  2.691176  ...  12.173113  29.522787  34.469910\n",
            "629  520.759766  535.615845  2.150558  ...  20.647518  26.002527  18.367264\n",
            "\n",
            "[630 rows x 12 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9a6bacQGRb2",
        "outputId": "72998292-e98d-4b58-9135-6f9afa8c7c39"
      },
      "source": [
        "evaluations = multi_predictor.evaluate(test_need)\n",
        "print(evaluations)\n",
        "print(\"Evaluated using metrics:\", multi_predictor.eval_metrics)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating TabularPredictor for label: 22 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluation: root_mean_squared_error on test data: -0.07557141994321691\n",
            "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
            "Evaluations on test data:\n",
            "{\n",
            "    \"root_mean_squared_error\": -0.07557141994321691,\n",
            "    \"mean_squared_error\": -0.00571103951223404,\n",
            "    \"mean_absolute_error\": -0.012467425344124195,\n",
            "    \"r2\": 0.9998263705206032,\n",
            "    \"pearsonr\": 0.999914046404652,\n",
            "    \"median_absolute_error\": -0.0001049453125006039\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating TabularPredictor for label: 23 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluation: root_mean_squared_error on test data: -0.3960444443048491\n",
            "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
            "Evaluations on test data:\n",
            "{\n",
            "    \"root_mean_squared_error\": -0.3960444443048491,\n",
            "    \"mean_squared_error\": -0.15685120186473686,\n",
            "    \"mean_absolute_error\": -0.11842590982764091,\n",
            "    \"r2\": 0.9953990609747126,\n",
            "    \"pearsonr\": 0.997719313281918,\n",
            "    \"median_absolute_error\": -0.0041917210937754135\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating TabularPredictor for label: 24 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluation: root_mean_squared_error on test data: -0.13720445501510517\n",
            "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
            "Evaluations on test data:\n",
            "{\n",
            "    \"root_mean_squared_error\": -0.13720445501510517,\n",
            "    \"mean_squared_error\": -0.018825062475992,\n",
            "    \"mean_absolute_error\": -0.04064054225915595,\n",
            "    \"r2\": 0.883998063001016,\n",
            "    \"pearsonr\": 0.9403214364767094,\n",
            "    \"median_absolute_error\": -0.0003931974630124735\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating TabularPredictor for label: 25 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluation: root_mean_squared_error on test data: -0.6416241765065399\n",
            "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
            "Evaluations on test data:\n",
            "{\n",
            "    \"root_mean_squared_error\": -0.6416241765065399,\n",
            "    \"mean_squared_error\": -0.41168158387769543,\n",
            "    \"mean_absolute_error\": -0.16857691084902235,\n",
            "    \"r2\": 0.9953748396004979,\n",
            "    \"pearsonr\": 0.9976952823291995,\n",
            "    \"median_absolute_error\": -0.0013677226699781997\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating TabularPredictor for label: 26 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluation: root_mean_squared_error on test data: -0.12384483972901979\n",
            "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
            "Evaluations on test data:\n",
            "{\n",
            "    \"root_mean_squared_error\": -0.12384483972901979,\n",
            "    \"mean_squared_error\": -0.015337544327506603,\n",
            "    \"mean_absolute_error\": -0.03684665220573615,\n",
            "    \"r2\": 0.8841194800771628,\n",
            "    \"pearsonr\": 0.9403732151979762,\n",
            "    \"median_absolute_error\": -8.08449889255769e-08\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating TabularPredictor for label: 27 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluation: root_mean_squared_error on test data: -0.004570835034982564\n",
            "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
            "Evaluations on test data:\n",
            "{\n",
            "    \"root_mean_squared_error\": -0.004570835034982564,\n",
            "    \"mean_squared_error\": -2.0892532917024047e-05,\n",
            "    \"mean_absolute_error\": -0.0013242626017771265,\n",
            "    \"r2\": 0.9975267224237829,\n",
            "    \"pearsonr\": 0.9987631239386565,\n",
            "    \"median_absolute_error\": -2.3382583660414014e-06\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating TabularPredictor for label: 28 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluation: root_mean_squared_error on test data: -0.04325102035672353\n",
            "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
            "Evaluations on test data:\n",
            "{\n",
            "    \"root_mean_squared_error\": -0.04325102035672353,\n",
            "    \"mean_squared_error\": -0.0018706507618977144,\n",
            "    \"mean_absolute_error\": -0.012397141648434898,\n",
            "    \"r2\": 0.9974076793560522,\n",
            "    \"pearsonr\": 0.9987169811989347,\n",
            "    \"median_absolute_error\": -0.0001569220096644397\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating TabularPredictor for label: 29 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluation: root_mean_squared_error on test data: -0.04180743399456764\n",
            "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
            "Evaluations on test data:\n",
            "{\n",
            "    \"root_mean_squared_error\": -0.04180743399456764,\n",
            "    \"mean_squared_error\": -0.0017478615372101289,\n",
            "    \"mean_absolute_error\": -0.01207046579430075,\n",
            "    \"r2\": 0.9970841133756081,\n",
            "    \"pearsonr\": 0.9985570023377687,\n",
            "    \"median_absolute_error\": -7.202413669871177e-05\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating TabularPredictor for label: 30 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluation: root_mean_squared_error on test data: -0.14225488526235316\n",
            "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
            "Evaluations on test data:\n",
            "{\n",
            "    \"root_mean_squared_error\": -0.14225488526235316,\n",
            "    \"mean_squared_error\": -0.020236452381005302,\n",
            "    \"mean_absolute_error\": -0.0380164222364214,\n",
            "    \"r2\": 0.9983258387599797,\n",
            "    \"pearsonr\": 0.9991764168256434,\n",
            "    \"median_absolute_error\": -0.0008690139607008263\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating TabularPredictor for label: 31 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluation: root_mean_squared_error on test data: -0.2794137326698201\n",
            "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
            "Evaluations on test data:\n",
            "{\n",
            "    \"root_mean_squared_error\": -0.2794137326698201,\n",
            "    \"mean_squared_error\": -0.0780720340044818,\n",
            "    \"mean_absolute_error\": -0.07940018177932852,\n",
            "    \"r2\": 0.9961824996662992,\n",
            "    \"pearsonr\": 0.9980942677213377,\n",
            "    \"median_absolute_error\": -0.0008836218065937373\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating TabularPredictor for label: 32 ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluation: root_mean_squared_error on test data: -0.2669569718766895\n",
            "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
            "Evaluations on test data:\n",
            "{\n",
            "    \"root_mean_squared_error\": -0.2669569718766895,\n",
            "    \"mean_squared_error\": -0.07126602483357154,\n",
            "    \"mean_absolute_error\": -0.08154081190674113,\n",
            "    \"r2\": 0.9830705033632646,\n",
            "    \"pearsonr\": 0.9915022758121337,\n",
            "    \"median_absolute_error\": -0.006627394052628688\n",
            "}\n",
            "Evaluation: root_mean_squared_error on test data: -0.7200629002727393\n",
            "\tNote: Scores are always higher_is_better. This metric score can be multiplied by -1 to get the metric value.\n",
            "Evaluations on test data:\n",
            "{\n",
            "    \"root_mean_squared_error\": -0.7200629002727393,\n",
            "    \"mean_squared_error\": -0.5184905803491892,\n",
            "    \"mean_absolute_error\": -0.2058568030299852,\n",
            "    \"r2\": 0.9926922954829147,\n",
            "    \"pearsonr\": 0.9963497261477777,\n",
            "    \"median_absolute_error\": -0.0009825482431704557\n",
            "}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating TabularPredictor for label: 33 ...\n",
            "{'22': {'root_mean_squared_error': -0.07557141994321691, 'mean_squared_error': -0.00571103951223404, 'mean_absolute_error': -0.012467425344124195, 'r2': 0.9998263705206032, 'pearsonr': 0.999914046404652, 'median_absolute_error': -0.0001049453125006039}, '23': {'root_mean_squared_error': -0.3960444443048491, 'mean_squared_error': -0.15685120186473686, 'mean_absolute_error': -0.11842590982764091, 'r2': 0.9953990609747126, 'pearsonr': 0.997719313281918, 'median_absolute_error': -0.0041917210937754135}, '24': {'root_mean_squared_error': -0.13720445501510517, 'mean_squared_error': -0.018825062475992, 'mean_absolute_error': -0.04064054225915595, 'r2': 0.883998063001016, 'pearsonr': 0.9403214364767094, 'median_absolute_error': -0.0003931974630124735}, '25': {'root_mean_squared_error': -0.6416241765065399, 'mean_squared_error': -0.41168158387769543, 'mean_absolute_error': -0.16857691084902235, 'r2': 0.9953748396004979, 'pearsonr': 0.9976952823291995, 'median_absolute_error': -0.0013677226699781997}, '26': {'root_mean_squared_error': -0.12384483972901979, 'mean_squared_error': -0.015337544327506603, 'mean_absolute_error': -0.03684665220573615, 'r2': 0.8841194800771628, 'pearsonr': 0.9403732151979762, 'median_absolute_error': -8.08449889255769e-08}, '27': {'root_mean_squared_error': -0.004570835034982564, 'mean_squared_error': -2.0892532917024047e-05, 'mean_absolute_error': -0.0013242626017771265, 'r2': 0.9975267224237829, 'pearsonr': 0.9987631239386565, 'median_absolute_error': -2.3382583660414014e-06}, '28': {'root_mean_squared_error': -0.04325102035672353, 'mean_squared_error': -0.0018706507618977144, 'mean_absolute_error': -0.012397141648434898, 'r2': 0.9974076793560522, 'pearsonr': 0.9987169811989347, 'median_absolute_error': -0.0001569220096644397}, '29': {'root_mean_squared_error': -0.04180743399456764, 'mean_squared_error': -0.0017478615372101289, 'mean_absolute_error': -0.01207046579430075, 'r2': 0.9970841133756081, 'pearsonr': 0.9985570023377687, 'median_absolute_error': -7.202413669871177e-05}, '30': {'root_mean_squared_error': -0.14225488526235316, 'mean_squared_error': -0.020236452381005302, 'mean_absolute_error': -0.0380164222364214, 'r2': 0.9983258387599797, 'pearsonr': 0.9991764168256434, 'median_absolute_error': -0.0008690139607008263}, '31': {'root_mean_squared_error': -0.2794137326698201, 'mean_squared_error': -0.0780720340044818, 'mean_absolute_error': -0.07940018177932852, 'r2': 0.9961824996662992, 'pearsonr': 0.9980942677213377, 'median_absolute_error': -0.0008836218065937373}, '32': {'root_mean_squared_error': -0.2669569718766895, 'mean_squared_error': -0.07126602483357154, 'mean_absolute_error': -0.08154081190674113, 'r2': 0.9830705033632646, 'pearsonr': 0.9915022758121337, 'median_absolute_error': -0.006627394052628688}, '33': {'root_mean_squared_error': -0.7200629002727393, 'mean_squared_error': -0.5184905803491892, 'mean_absolute_error': -0.2058568030299852, 'r2': 0.9926922954829147, 'pearsonr': 0.9963497261477777, 'median_absolute_error': -0.0009825482431704557}}\n",
            "Evaluated using metrics: {'22': root_mean_squared_error, '23': root_mean_squared_error, '24': root_mean_squared_error, '25': root_mean_squared_error, '26': root_mean_squared_error, '27': root_mean_squared_error, '28': root_mean_squared_error, '29': root_mean_squared_error, '30': root_mean_squared_error, '31': root_mean_squared_error, '32': root_mean_squared_error, '33': root_mean_squared_error}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "RXQGMJNVGdNF",
        "outputId": "1f3d2775-c9e2-47d3-ce5f-a453f193876f"
      },
      "source": [
        "plt.subplot(2,6,1)\n",
        "plt.plot(predictions[\"22\"],test_need[\"22\"])\n",
        "plt.subplot(2,6,2)\n",
        "plt.plot(predictions[\"23\"],test_need[\"23\"])\n",
        "plt.subplot(2,6,3)\n",
        "plt.plot(predictions[\"24\"],test_need[\"24\"])\n",
        "plt.subplot(2,6,4)\n",
        "plt.plot(predictions[\"25\"],test_need[\"25\"])\n",
        "plt.subplot(2,6,5)\n",
        "plt.plot(predictions[\"26\"],test_need[\"26\"])\n",
        "plt.subplot(2,6,6)\n",
        "plt.plot(predictions[\"27\"],test_need[\"27\"])\n",
        "plt.subplot(2,6,7)\n",
        "plt.plot(predictions[\"28\"],test_need[\"28\"])\n",
        "plt.subplot(2,6,8)\n",
        "plt.plot(predictions[\"29\"],test_need[\"29\"])\n",
        "plt.subplot(2,6,9)\n",
        "plt.plot(predictions[\"30\"],test_need[\"30\"])\n",
        "plt.subplot(2,6,10)\n",
        "plt.plot(predictions[\"31\"],test_need[\"31\"])\n",
        "plt.subplot(2,6,11)\n",
        "plt.plot(predictions[\"32\"],test_need[\"32\"])\n",
        "plt.subplot(2,6,12)\n",
        "plt.plot(predictions[\"33\"],test_need[\"33\"])\n",
        "plt.show()\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAD4CAYAAAANbUbJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydd3gU1drAfycdUkgoCYEAIQQhCSVCAJUiiBQBUeQqqJ+ieO1c270qNhQUwV6xXfWKDeyCNEEhFqRISSihQ0ISIBBII6Tunu+PLdndbEu2zS77e548mXJ29p2d3XfOvFVIKfHjx48fP75NgKcF8OPHjx8/rsev7P348ePnPMCv7P348ePnPMCv7P348ePnPMCv7P348ePnPCDI0wIAtG3bViYmJnpajGazdevWYillO1vjvPk8z4dzBPvO83w4Rzg/zvN8OEcdilD2iYmJbNmyxdNiNBshRJ4947z5PM+HcwT7zvN8OEc4P87zfDhHHX4zjgGJiYn07t2b9PR0MjIyjPa98sorCCEoLi4GQErJfffdR3JyMkCqEKKf+yX24+f8YdWqVfTo0YPk5GTmz5/faP/Ro0cZMWIEF154IX369AFo5XYhFYxf2Zuwbt06srKyjO72+fn5rF69ms6dO+u3rVy5kgMHDnDgwAGAPOBdtwvrANXV1QwcOJC+ffuSlpbG008/3WhMTU0NU6ZM0d3QegohEt0sph8/AKhUKu69915WrlxJTk4OixYtIicnx2jMc889x3XXXcf27dtZvHgxQGezBztP8St7O3jwwQd58cUXEUIAoFZLbpq1gLRLr9RtqwSihRDxnpSzKYSGhrJ27Vqys7PJyspi1apVbNy40WhMhysfZMX+cg4ePAhQBLzgbjkz951k8Py11KvUABwprkStNs763llQxo/bC5v9HrnFlQ7J6A0kzlxO4szlnhaj2WzevJnk5GSSkpIICQlh6tSpLFmyxGjMhsOnefyrzZyqqKGsrAygziPCupB6lZreT//Mql0nmvxav7I3QAjB6NGj6d+/Px988AEAS5YsoWPHjvTt21c/7umlu6kqPcWe8mDDlxcAHc0c8w4hxBYhxJZTp065+AzsRwhBREQEAHV1ddTV1elvZqXnavnwj8OcO7CRiF4jdS8pAUYK3SA38c66QxSWVpG57xQV1XWMeDmTsW/8bjwm8yAPfJXF7/ub/vkmzlzO8Jcz2VVY5iyRFceXm456WgSHKSwspFOnTvr1hIQECguNb/AlPa+mcvc6+vRIYty4cQBmT1ypv0l7eOjrbCpq6vl5t1/ZO8Sff/7Jtm3bWLlyJQsWLOD333/n+eefZ86cOfoxUko+26jxiTw06gKbx5RSfiClzJBSZrRrZ5fT3G2oVCrS09OJjY1l1KhRDBo0iJp6Felz1vDc8j2ozp4mMNJI5jKgjelxXPnjuW6A5gf+1ZZ8vvo7H4D9RWeNxuSePgfAf77JpqSy1u5j19Sr9MsT3vrTUVEVy+M/7ATghcm9PSyJ67jni62cy/mNiN4jOX6skBUrVgB0FUI00nFK/k1aQ62WLM0+BsALk/s0+fV+ZW9Ax46aiXlsbCyTJk3it99+48iRI/Tt25fExEQKCgqI75aG6mwJgRFtOHnimOHLE4Dm2xI8QGBgIFlZWRQUFLB582Z27drFkBfWGY1ZOmOwzeO48sczJi0OgDU5RRwwUPKZ+07q3pujpyu5KKk1JedqefyHnVgr7neirJoXV+0lceZyejy5Sr99QGKMU+VWCj9sL9AvTxngvSbsjh07kp+fr18vKCjQ/14BVuw8wdkda2jZcygAF198MWj0W1v3Suo6nvhRc9MenNyGkKCmq26/stdSWVlJRUWFfnn16tUMGDCAkydPkpubS25uLgkJCcTf8jqBETG06D6ITz/9VKdYwoEyKeVxT55Dc4mOjubSS4dzw6z3OFVRo98eGNGGmjKjmXor4LQ7ZYsMazCVHTrVoOxv+d/fAJyurKWyVsXo1PY8NKoHK3ed4LttmnuulJKDJytYtPkoD32VxdAX1zJo7mqenDaek9/OBqCu9ATHP32IbS/fzJQpUwDcaqZyNQ9+lQ3AP/oneFgSxxgwYAAHDhzgyJEj1NbWsnjxYiZOnAjA59on7cCodvRQ5QKwZ88e0FxL77LTWEBKyaLNmpvdR9MGNOsYioizVwJFRUVMmjQJgPr6em644QbGjh1rNKagpAqdB7bo66eZMWOGLlKlCzDMnfI6yqlTpwgODiY6OpoTZ8qZ/9HXRA36By0NxvQfNoqFCxfqZkkxwFrpwZrYu4+VM7JnLL/uPanflqc14XRp05LByW157Zf9/OebbD768wgnyqooOafx0bUJD2FAYmuCdq+guE0nZK3mdaWZnxCVcRWHlr7IXXfdBT40E3zz1wP65aevTPWgJI4TFBTE22+/zZgxY1CpVEyfPp20tDRmzZrFe7uhZfdBxFx2G7l/fUjfvj/o/E+5nvy+OpO5y/cA0DehFWHBgc06hl/Za0lKSiI7O9vqmIS7P9YvCyFYsGCBbjlHSulVmRnHjx9n2rRpqFQq8orPEtbtElomD6T0j88Jad+dlt0H8cv7s7npppt0N7T2wExPyNqzfSR7T1RQVadiUFJrvbL/cXshEs1veXPuGW5b2HAJ9hwv55oLOzIoqTUDElvTtW04a/7O4aqn1hB18RQq/v4RKSXVR3dw/9w3AZg2bRrvv/9+tPvP0DW8uma/ftnwCclT7CosI+d4OddldOLwqbMkxLRskjli3LhxOsernlE3/YtPP9oMQEjbzhzcsUUfaCCEKHee9J5DSsmHfx4B4MvbL2r2cfxmHDsxDFvLnT/eg5I4hz59+rB9+3ays7OJufktogdfD0D00P+jZfdBAISFhfHNN9/oQi/3SCkPe0LW6zIaojC6tYvg+UkaR+MDX2WRd/ocQsD7vzWI9tgVPTVjYyOYMqAzHWNaMGdZDpNuuoPo4dMbQmirygkIDWdAksbPkJCQABDinrNyLTd9tEm//P09l3hQkgbu/mIrj3y7g5MV1Vzz7l+8/st+2y+ywU1aRa/DzcFibmHeyr0AJMdGEB7a/Pm5X9mf56zOKTK7XUk3tEuSGwKAktpFcMOgBkfjvhMVtI8KMxrfO6EV43vH89qa/SzefJQeT65iwcKvCQiPJrR9cqPj902wPZlXerjeos1HSZy5nHkrNY/7fxwo1u+7sJMyHlYm9OkAwIK1B+ncuiVLso5ZdabbYv3BYqP1J8enOCSfEpFS8sHvmonMd3c5dtP2K3s78LVZvSF3frbV7PZXVu+joOScm6UxT1LbCP1yp5gWRvtW7jqB2kBhJMS0YM5POcy6MpV6tWTm95oIhprCHKoObKLg3emcWvoi1Xk7OPPrB6hrKkloFQpoIjwAs7GbSg/Xe0x7nkuzjpHx3C/67U9fmaqY2e4dQ5MAWLghj4wurSksrWJHQfPyG/LPnOPGDzcZbevXxfciqvrOXq1fbtXSMVOcX9mfx2zNO2Nx31trD5J/psqN0ljG0K57RhtH/+Xtg/Tbiso1EUST+yXwxLgU9p6oYNDzvxod45EnZ5Nw70IS7v6YdhMfIaxLH9pd+TAJqRl8//13ACxcuBCg1LVn43yq6xryBX5+cBjFZxsiqiZd2CjPz2PEhDdYyHShrst3Nj2A7dutBQx9cV2j7WkdopovnEIpr64H4M5Lkxw+ll/Z28BXZ/VlVXVMfneDxf1JbcO5KKm1GyWyj+35Gl18SbfGQTMjU2JZtsNYeVzWMxaA//5xxOzxbvzXY7z66qskJydz+vRpgGKzAxVM72d+1i8Pmttwk7uybweiWyrLBdFCG0kSEaaxPS/fcdxuU07+mXMkzlzOf75pHEjRv0sMoUHNi1JRKr2ebriuj13huInKH41jhapale1BXsrc5TlW9793U3/FPP7r6uIAbDh0mjFp7c2Ou+eLbUbrYcEBrDUI09Rv79yHsM6aDMTLBvTm+c0NTj4hhNeF6tWpGkSuMpjl3zhIeUlUD4/pwZxlOXy56Shd24ZzpLiSGz/cxOtT0zlTWcsP2wqJiwqjorqeiuo6zpyr5cfthahtXJV+nZXhl3AmZ2vqnXo8v7K3QsqshgxLX5rVb80r4estBRb3D+3elgviIt0okXUKShrMSd9syeeZiWkATLu4Cws3NC7n3To8hDOVtVTXqRvtM6VPJ++ugrsky3zSdnhIIIO6Ku/JbHK/BOYsy2HlrhNc0q0NR4or+evQaYa/lMk5ByZX/X3MXu8Ki4LfjGMBla2phBcz+d2/rO5/+3plleY/XNyQOVtZq9Jfm/wS8z6FM3bWx+kY3YLYyDDbAxXM/YuzzG6/b2R3xTyZGWLoZDx8qqHaqDmznCld24Zb3Nevs+8o+9p625OU5uBX9hbo9vgK/fKeOWOtjPQuys5Zr/o6vk+8w15/Z2OoFECTMJU4c7lZE40h43qbN/fo6Ovls3prRd8mK7g8Qpc2mjztE+XV+m2/7DEfAmzIEQulqBNiWhAb5d03bUMueHKlfvnw8+OsjGwafmVvBlOHUYsQ33H8zF622+r+Z65Mc5Mk9nPo1FliWgYToU0osadC5Ze3D+L2odYjGOyJr1cyg19Ya3b7uN7taRsR6mZp7Of+kd2b/Jr4VpaVuS/N6suqjCdjAQHOezrz2+zN0PWxhln9jmdGe1AS5yKl5Ptt1gtztotUnpI4dKqSpHYRBAUINh2xHC5qyA3/3WRzTLpCko2ag1otLdq4pyq0umVNvYo1OUV89Kf5yChrHC+rtrjPl5yzhnH1e591rkXBr+xNMIz8AIhSQE0RZ7HGQrasjhX3DXWTJPYjpWSznQreXuKiQjlVUUOvjt5rxvl4fYPCrC8/RfHyV1FXlhIUGMC29g8w7IIHjMZnZmZy1VVX0bVrV9D0TJ4lpZyDG9h9rIxvthTwY1YhpTbMiE0hMiyIiup6+ndRniO6OZgmMTa34Jkl7FL2QohcoAJQAfVSygwhxEvAlWgyDg8Bt0opS7XjHwNu046/T0r5s9kDK5DkJxrsZZseH2llpPdxh4VsWR2pCkpKqVepja6FM4lpGUJMyxCH6ox4mue0VRABCAgkZsRthLZPZsaQDrzz0BRGjx5NaqpxpcuhQ4eybNkyXeE+tyh6R1ohfn/PJSTEtGD9wWJ9qWZDYlqGUKdS0zNeOZFjjmDYS2LDY5c5/fhNsdmPkFKmSykztOtrgF5Syj7AfuAxACFEKjAVSAPGAu8IIbzC6F1UbvyoGOdDTp8TVh6DAT6+JcPqfncz5vXfbQ9qJkXl1V5trzftmRsU0Vpf8+emYSmkpKQ0atmnFAIDBJn/GW7X2Gve+YuXf95nVtGHBgUQFhxA34RoggO93/W4/WiJ0Xp8qxYWRjafZn9KUsrVUkpd1P9GNJ2aAK4CFkspa6SUR4CDwEDHxHQPhin23919sQclcT5P/rjL6v7hF8S6SRL7OHqmcV0eZ9Rkn9i3AyXn6ujrxfb64S9nmt1+eUocVWdOsH37dgYNGtRo/4YNG3S9lLsLIcx64l1d8E2llhblN4elfJB+nWM4fKrSJ+rhSCmZ9E5DOPQ3d7lG99ir7CWwWgixVQhxh5n90wHdM3dHIN9gn9lG3ErDNCTRV+yAAHUqtdXQtrmTejnV6+8MdFmhFxo432b/ZD3r1x7aRGjKB3hr2KVhHRxTrk5rzeTJk3n99deJijI2yfXr14+8vDxdz4aTwI/mjuHMgm+GGei588fz8rV97XrdG1PTbY5pFxlKvVrS3wcicXR9ZXUMSHSN7rFX2Q+RUvYDrgDuFULouzIJIZ4A6oEvmvLGSisZ23dOgxf8gcubHhqmZH7cbv2RfnI/ZcVkGzYCNy1fbI424fbXf5FSU0ahh4IyhJvCa2vM14CPDQ/izcfv4sYbb+Saa65ptD8qKoqICH310DIgWAjh0q5cOcc1FS0DtROJf/RPYM2Dthu6WUoUMyRamwtyoZdH4tSr1EbnO3ui60Kf7VL2UspC7f+TwA9ozTJCiFuACcCNBu2/CoFOBi8324hbSSVjTWvgPHD5BR6SxDU8/O0Oi/vuHdHN6V5/R9lV2NBgaOWuEwAEWXnyOG1nxmyPuEh2FpbRu2MrgrzQziul5P3fG/ePkVKi/u1dUlNTeeihh8y+9sSJE4b5Iy3R/PZd2k943V7NJG5EjwYTYXcn3GR7to/keFk1iW1a0kbB+QT28IZB60iAaZckuuy9bH7jhRDhQohI3TIwGtglhBgLPAJMlFIaGliXAlOFEKFCiK5Ad2Cz6XGVxKR31ntaBJdhK9zy1sFd3SSJ/Ww83FgHje8Tb2Zk07hnRDd2FZZ5rXN2S16J2e01hTls/XUJa9euJT09nfT0dFasWMF7773He++9B8C3335Lr169dDb7zsBUV/dnXblLU4HUcPataw7uCJf1jGX70RKvt9dX1tTz1tqD+nVX533YE3sWB/ygrbMRBHwppVwlhDgIhAJrtPs2SinvklLuFkJ8DeSgMe/cK6VUbPnIOpWavScq9Ou+VPDsRFk1t39quTXu1ekdFJlpadqu7rmre9l0MNtDbGQYNfVqr3XOXvue+ZLUoy8bzqefW9fbM2bMYMaMGQAIIfZKKa0XSHICh7RlLtpFhqJWS15Ytdfsk0lTuSAukuKztV6fOXvfou1G69/d7dr2kTaVvbbvaCPPipSycX+3hn1zgbmOieYenl3muNNPqbyTedDq/ntHWLyEHmPFzuNGJXvBegGspqArqOaNmbOnKmos7rt+QCeL+zyFYTGvVi2C+dei7SzfeZxWLYIblQRoKrpCeN5c6bKovJpfTWo7Bbo4SMJ7s0qcgFot+dSgRK6vzOqllLywap/RuZmS0SUGlZSo1NLlXzJ7Ka+ua1STHiD3tPkCWE1h2sVdyM4vpXV4CAkxzo9hdjXPr9hjcd/IlDg3SmIf+4sanpYf+iqLyloVT4xLobC0ik/+ym32ca/o1Z6s/FIiQoMUVYa7qYx/07i+0+tTbEcgOYr3eamcyMINuZ4WwSVk5Zfy3m+HrI5JahfO2Nf/4M+DymnMFBUWzPUDG89Sn/jBcRPOtEsSyc4vo29CK0WW/rVGvUrNDxYiqu68NMmobaNSyDnW4GSvrFXxzo39uH1Ykr7TWHOZ0KcDW/NKSO8UrZhJSlPZfazMqHUkwNVuaB+pvG+Jm5BSGsVtO7OUqKdZtPmozTHLdhynS5uWims9aPgDthaB01Rio8LYf7KC9E7e9+ivi0gyh1KLnhnO3r+7+2LG9Y6nuk5FtoPKvn+XGPaeKPfq4mems/r/jHZP9N95q+xfWW3sBFRaUlFzqaius9qFSkd58QnKvnmSC/v0Ji0tjTfeeKPRmMzMTFq1akV6ejpoi2c5X2JjtuY1KIMAIZxmr99ZUIaU3plM9S8TR56O8JBAp30+zibneMPM/paP/6b/s2vo+dQqK6+wj8OnzqKWcKGX2utX7WrcYH3GZe7J6zlvlf3b6xqclzlzxnhQEueyJOuY7UHAoG7t+OjdN8nJyWHjxo0sWLCAnJzGzuqhQ4eSlZUF4PLiWRXVdewxUBK1KnWjhhWXdGvT5OO+MTWd7ALNTcTbwi73GUSKmfL8Nb3dKIn9mHZ56xDdwu5cCGt0bRvONm0NmX5e+IR21YL13PW5sU/Kncl956Wy/32/ccZuyxDf8VPbE6IYFCB44aZh9O/fH4DIyEhFFM/KsuMR/6Kkpiv7K3rFk51fSpc2LYlpQratEpj9k+VmM5Yar3uaw6fOGq3vK7J8w2oKs65MZWteCcmxEYrrpmaLHQWlZk1YX915kdtkOC+V/c0fN+R4bXzMd8oY26MsAW6+OJHk2IYZRW5uriKKZ221kDRkSFK7ppstQoICyM4v9bpZfXl1HX8dMp/k2jYiRHGZzzp2GzhndXx2m+O1EIcmt2V7fqlX1sOZ+Lb5xM3olu6bfPjOlNZODpjMMtpbaXfmbVy9wL5M4PsNav+cPXvWZvGsiIgIhBC64lmNDIxSyg+ADwAyMjKanZVpj7JvTiP4k+XVHCur9rpkqq//zre478vb3TcjbCo/7zZ2KG9/ahTV9Y7nVeadOUfpuTr6dfGu67j7WJnZ7QMS3XvTOu9m9qNea6iT/tOMIR6UxLnkmykJbI551/SmVQvNI3BdXR2TJ09WRPEslVqaLZNgyMDE1sxfubdJxx2TFqd/4vGmZCq1Who3KDFByTHmhtFDz0/qTUx4iL5OjiPoJgPelkxlGn2jY/Ed7i2jfl4p++NlVUbrvRO8LzLDEkNfXGd7EHBdhiaOXUrJbbfdRkpKiiKKZx04WdEoc9aUOy9NstqL1Bz/6N+J7IJSggIEaQrqxGUL0+xKQ5RWpdQQKSWhBnH/sdqexi/93LSbtCkzRiSz/WgJUWFBJLWNsP0ChWBaZFHH1AGd3J4ncF4p+4vnrdUvv39Tfw9K4ly25NrXo/Wbuy7Wf8HWr1/PZ599ppjiWfb0me3Rvumz2Q7RYWTnl9EzPlKxNm5zWKtpNOcq15XBdZT8M1XUGJRKqFdLrnzrT0oc7D3bq2Mrfl61irz37uCCC7ozf/58s+O+/vprUlNTSUtLA/B4lb//+8h84/t5HoikOm9s9qbNSS5XYIp5c6hXqfmHhQJZhlyeEmvUFGHIkCHY0tvuLJ6VdVSTAn+2pt7imOmf/N3k48ZFhZFdUMrEvh0cEc+tmNq8DQkPCVR079xdJvbpnGNl7Cw0b7NuCu0igtn5zWs89ubnPDllKAMGDGDixIlGfXYPHDjAvHnzWL9+PTExMQghLDs93IQ5P9SMEckeyeI+b2b2hs1JZk1I9dpUa1MWWql/Y8izV/dysSSOsfVoCW0jLEcmXBAXwf6isxb3W6L0XB0V1fVe45yVUnKnhcbwrVoE01LBih5gl4FiFwLyS6qsjLafDRs2EhQdz+iL+hISEsLUqVNZsmSJ0Zj//ve/3HvvvcTE6G36lmcObmCehXpG/xnTw82SaFD2N8dJmNrNpiiwSmBzKCqvZu5y21U777ss2SUNjJ3FqYoa8k6fo1/naHJPm3c0R4U1L656R4F3OWdf++WAxX1lVXX8c4jHLRNW2WUQdtkmPET/+TtCaFAAW3IOEBTVTp8BnZCQwKZNxiaS/fs1WfGDBw9GpVIBeMxJY6nRzC0ubE5ii/NiZp8yqyFN+5ZLEhX9GNwUnlu+B3siEe9RYCljQ3SPuta6Dllq3GGNbu3C9RUSu7VTvlPvbE09b/5qWdl3aBXGg6OU20VNSslug5l9aFCgvqa9I4xKjeNwcSWtWgQTaeWmX19fz4EDB8jMzGTRokUAiUKIRnd5d7RENe1ApeMZF7YdtIXPK/s6ldpo/Y5hSRbHJiYm0rt3b9LT08nIyADgqaeeok+fPqSnpzN69GiOHdOUI5BSct9995GcnAyaujH9XHUO5lh/sJifsm2XRnjxH30U75jcdrSEkMAAIpx8E06JjyI7v5TeHVt5hdnOVp7E7Kt6KXqiUlReY1QWwTT6rbkkx0ZQWBNGSE3DDb+goICOHY0rRSYkJDBx4kSCg4Pp2rUrQDUW8kJc2RK1tl7N62ae0Dxdx8jnlX33J1bqly9OakOHaOvmjHXr1pGVlcWWLZpoiIcffpgdO3aQlZXFhAkTmDNHUx5m5cqVHDhwgAMHDgDkAe+66BQaUVOv4qkl9pX9vba/csP0dGzNK6F3QisKS52jHHS0Dg8h53i5V9jrdxaUcfCkZZ/EmLQ4RqUqO6hAZ69voZ1cNCP/zSL1bbtReaqAI0eOUFtby+LFi5k4caLRmKuvvprMzEwAiouLAcIAx1tjNZGZ35nv+fzrQ5e6WRJjfFrZq02+bTOv6NnkYxhmlVZWVuq96EuWLOHmm2/WrVcC0UIIxxul2sGHfxzhsB2Pxx/fkqH42u019Sp2FpTRv0sMucWOP/IbUnKujjqVJF3hlS7VasmVb5tPvNHhycd/e9l1rAwhcPrTR0V1PSIgkHkvvcaYMWNISUnhuuuuIy0tjVmzZrF06VIAxowZQ5s2bUhNTWXEiBEA+VJKlzZVN6Wsqo7vLfQe8HRlXZ9W9kmPr9AvtwgOtDnDE0IwevRo+vfvzwcffKDf/sQTT9CpUye++OIL/cy+sLCQTp2MHL0FQKMOBM62D+afOcdbaw/YVSPmsp7KngkC7Cosp1alJiU+kpNWWu81hyJtApbuuufn5zNixAh9HLaurPOZM2cYNWoU3bt3B00NILemaL5ro9HMrAmpinaw69hVWE5im3BKzzle4dKQ02draB0ewq1TJ7F//34OHTrEE088AcCcOXP0M3whBK+++io5OTns3LkToOmOHge55h3zpjgl1ODyWWVvGkP+2pRGbXQb8eeff7Jt2zZWrlzJggUL+P13TWmFuXPnkp+fz4033sjbb7/dVDmcah+c/VMOAULYnNl/cusAh9/LHWzN0yRTtQl3fuPzE+XVxEaG0j5KU/8oKCiIV155pVFZ5/nz5zNy5EidSa4CmOl0YSxQeq6Wl37eZ3XMNA9GcDSFnGNltAkPod6J9ptu7cLZUVBGv87Rin9KzTtdadEhrYQaXD6r7Ls+tsJofVSq7XKwOodPbGwskyZNYvPmzUb7b7zxRr777jv92Px8o5yNBMClNYL3najglz1FdI+1HVkyvEesK0VxGlvzSujSpiUV1c4PiT5Rril+plMS8fHx9Oun8aMblnVesmQJ06ZN073sNHC104WxwNQPNlrdv/iOi7zCuXz6bA3HyqqJauHc0sNxUWEcLq6knxfUw7n0pUyz258cn+JeQSzgs8reEHuSqCorK6moqNAvr169ml69eulme4DGTt+zp8buP3HiRD799FPdE0Q4UCalbNyGxolsOKTpF5tdYD0j8Z0b3RoY1GyklGzN05SsNa2B7gxq69UW4+sNyzoXFRURH693t9QBjexfrgjXO1ZaxV4rzUmgefX7PYGurHFUmHPt9bqM6n4KL2v81yHLvZynD1ZGboRy47gcIHHmcqP16+xIoioqKmLSpEmAJl73hhtuYOzYsUyePJl9+/YREBBAly5d9HVjxo0bx4oVK3Shl12AYU4+jaBUrT8AACAASURBVEZsPGxfDRylNrUwJf9MFcVna+jXJYaP1x9xyXuYU/bWyjpraWSHcFYZZ0Nshc7O9gKnrA5dmQRrcfDNoaK6nsAAoeheBFJKbviv+Ro4rcNDPO6Y1eGTyt6Q24d2tSt+Oykpiezs7EbbdWYbU4QQLFiwQLecI6W0XLnKCajVklVWaqbomD0xzSse+wG2HtXcvPp3ibGrw1ZzMK1saq6sc1xcHMePH9fN7oMByyUnnURZVR3zbJRr9hZbPcDuwnI6tW7BGSe0HzSkorqO1PgoWoQoN1dkoUFzdVO2PTXKfYLYwOfMOKazem/6wVhj/0n7Wrtdm6H8uHodW3JLiAwNcllt9m7two3KLFgq6zxx4kQWLlyoW20DGBddcQH3fGG+/o2O/c9dYfexLEUZGeLqJMDdx8ro1aEVvx9wbkZqWVUd/Tord1Zfr1LzzE/mS5a8ef2FbpbGOj6n7A0Z3yeehJiWnhbDKTz5g+2Z793Du3lVP92teSWkd462u51iUzENtbVU1nnmzJmsWbNGF3oZBZivn+sk9p4oZ/1By+HfQ7u3JSTI/p+mpSgjQ1yZBFheXUfu6XOkdYhyuqO9TiUV7Zy91Uol1gm93ZJ2YzfeoxnswHRWr/SiUfZytqbertowniyy1FQqquvYV1TB2F7t+eB363HmTWXOVWnMWrK7kb3eWlnnX3/9FQAhxH4ppX3OkWYgpWTs639YHdPUBKr4+Hi9g9kwysiw/K+lJEBnBBXkaJ2zSS6qP6RU52xlTT1/HDDvmL13RDfF2Op1+OzMvl/naC5U6Jekqbzxy36bY67s24G4KM/H8tpLVn4pUmrs9T/vLnLqsdtqC6opsdLl5xttl6R2pGibpebxrkwC1JVJaOXksEvQdLpKiFFmQpm1onX/uqxRSR6P4zPKvv+za4zW/znUcsEzb2J/UQX//cN2pMpdl3rX+W7NKyFAQLtI5ydTZeeXEhIYQM/2ympDWFWr4qklu62OeX1KerOPb0eUkU2akwSYc6ycuKhQiy34HKFf5xhFJlOdLK/mUwu9JAZ2ba3I4oN2KXshRK4QYqcQIksIsUW77VohxG4hhFoIkWEy/jEhxEEhxD4hxBhXCG6KYbW9jtEtGK3wolH2IKXkKTuiVOJbhZHWQdn1X0zZmldCj/ZRVm3XzSUrv5TUDlFNsnu7g/ftMFeN7dW8sFlbzeNdmQS461gZaR1acdTOpvdNQanNxV9ds5+qOvM3t7dvUJZjVkdTfg0jpJTpUkqdYt8FXAP8bjhICJEKTAXSgLHAO0IIl97mZplUgLx1cCJBgcr6oTeHJVnH2GRHb9bnJ7m/n6UjqNSS7UdL6d8lmtV2hJM2hVkTUtlZWKY4E05ByTnezbSu7Ef2jG3WjNCe5vGuSgKsqlVx8ORZenWIYtMR59+4+3VR1nXUYclWHxEaRGykMs2pzdaIUso9UkpzRT2uAhZLKWuklEeAg8DA5r6PPRg+TkWEBvlEJ6ry6jrmWmhrZsqlFzi/Jrcr2V9UwdmaehLbhPO3nc3S7aVn+0jO1ar0HY2Uwss26t8APNqMqqxgX/P4cePGkZSUZJgEeE+z3syEPSfKUUtI69jK6b6X4ECh2CfWWVemmt3+xyMj3CyJ/dgbjSOB1UIICbyvzSa0REfAsOCHRUcQcAdA586d7RSjMT9sLzBanzKgk9Oz+DzBa2v2c8qOKpCzJ6YpzutvC11nqtJzdU6teQ6QX6IxJSgp47KmXsXqnCJq6tVWxzU338Ce5vGuSgLUdabq1dG5Srln+0hahAQq0vYNWOwTHBNuuY+yp7F3Zj9EStkPuAK4VwjhcGkAZ1WDfPCrhqzXAOFd4YeWyDlWzidWsvIM8canmG15JbSNCGXP8XLbg5tIVn4ZUWFBHu8KZMjfR0o4Z8N5OXeSshvCW2L3sXJiWgYT4kSz6cCurTXFzxQaTWfaJ0PHdgVly5rDriskpSzU/j8J/IB1s0whYKiBXFYNcqtJ7PnYXu3p1Nq7k6jUasmsJbuwMVEDYHRqnGJnPtbYkldCSnwkfxy0XDyquWTnlxpVulQC6/bZrr5wzYXek/lsiM45u3jzUacd84K4CGrr1Yp1zhr2yTBEybN6sEPZCyHChRCRumVgNBrnrCWWAlOFEKFCiK5oekButjK+2Ux+9y+j9dt8IInqu20FdjfXfv6a5jtmPZVif7KimqNnzlFRXU+tDbNGU+naNpx9RRWKc85+9Kf10NkubVoquvaLJWrr1ew7UUFaxyheWWM7F8RedL0NlDizt2QuU7KtXoc9M/s44E8hRDYapb1cSrlKCDFJCFEAXAwsF0L8DCCl3A18DeQAq4B7pZROD8A1deyld4pW5JejKZSdq2O+jeJYhuiSh5qDp1Lst+VpSiMUn3VuVyrQOKpVaqkoe/22o7Zv3Atu8I6S1KbsL6qgTiXp7OSn6YOnztIxuoUiGn6Y8sIq8452b7Ao2HTQSikPA43aPEkpf0Bj0jH3mrnAXIelsywT1763wWjbbUO6KurRvTm8vHqfUb6ANZb9a4hD7+WpFPttR0sQAqe3IARoo32M7qOgSJxr3vnL5hhnOzfdha5MQlG5c6/ltrwSRZpwVGrJe2ZaSH55+yAzo5WHVwajr9plHJvdMboFVzQzGUUp7Cwo4/NNtlPpdThTQTiaYt8UtuaVIKXGBOBMpx7AvqIKOka3UEyc82cbcm2OuXt4N5fL4Sp2HSsjIjSIpVnOdckdL6tW5FP6jxYaiV/Sra2bJWkeXqns7/5im9H6tEu6eH0S1aK/j9rllAW4f6Tz6m44I8Xe3noq1XUqdmq7bLUIDqRW5VybfXZBqaLi69tbaBK+fuZl+uV7RyS7Sxyns6uwjMS2Lck97fuZs7X1av79TeN+F09NMB9vr0S8TkNW1hiXUA0PCWTKgObH6SuFf4+6wO6xD1zuHGXvrBR7e8Nodx8r0yv4uCjn1MQxbIOXf6ZKUc5Zc+Ua/nfrAPJONzSltqexjhJRqSU5x8s5UdZ0E06sjXpIoUEBpMQrq67RV3+bjza61YtCvb1O2ac9/bPR+rUZnVxSbc/d2GurB5zim/BEir1hqKyzZoPp2sd9XV6ZkpyzmWZCLkf0iOWWjzU10D+7zaWJ5S7l8KmzVNepm+Vot+Wv6ZsQrai6RpYK2EW3DPaqhEavmlZUmxQeEkI5zXwdZfZPmi/TmLQ4q2nnWbOck7ihS7Hv3bs36emaSovPP/88R49qZjB33XWX0/vs6pR9cKCgTuWc1NlsbeMTtdQofCU5O/+3Ptdo/dDz46hTqfVPN0OSvcPWaw5dg3FXcKHC6uEstjCrX/2Ay9tOOxWvUvY9n1qlXw4JCmBEj3Z0bqP8kCdbFJZWsSW3hAl94lm2w/rEObqlcxI33J1iL6VkqzbsMrFNOAdOnm3uoYwoq6rTL18QF0m4QswiucWVRus3DOpMYIDgrV80NdAvvaCdV0eP6WrYm+OipNZsPNy8mkdBAYJLuyur1pM5h78QEOtF/SPAi8w4NfXGs/raerXP1Kx/5ed9CAG32zifl69tFAHrNRw9c07/yO8sRW+Kkkw4320zrtk092pNOYTXtcr+NQfq1iuBXccsK3trij451nJjlst6xpL19GguUdgTz71fbmu0zRuSqEzxGmXf48mGWX3biFD6JLQiQ2Ee++ayPb+Uy3rGctWC9VbH/aO/d6bUQ+PSFtb412XNi1Ax7TnrSd5ae1C//NSEVIQQHC+r0m9rrfDUemtIKdlRYFnZW+OglRt9/y4xinNYW3r69cbe1l6h7M/VGkfgFJ+t8YkkKh1lVXX6BBVLKLU1m73Yq+xbtQimS5umFTHTReAoJRKnsLTKaF1XxmP2Uk2G8tMWyuN6C/lnqmwWdvvP6MbRZbYisLq1U07xOh1dH2tcB8dbHevKuo1aIHVWQwROj7hIyqvrGKewzu3NRUpJWVUdZ2xE4/w0w7GMWU+zNOuYzTGvXteXR77doXdW28u/R1/AtrxSerZvXolgZzN4/lr98ov/6KNf/udQjdL39sqs1kw4Ol5e3bhWjq1MW1d0unIFQxXmU7AXr1D2huwrqmDmFT0J9vIkKh1na+pR2VHUXekV9axRUV1HhUl+hDkW/51PvVpSUW17rI6QwAAuTmqjmB+g6VPodRkNGcgZia3JSGztbpGczqbDzu9IBTB1oLLyZe74tHE8wsNjenhAEuegeI2ZOHO5fvmSbm1oERzI9T6QRKWj9FydzTEvTu5jc4yS2X601K5xm+1owQhwYecGc01Su3BFZU9fbeB3UWovUkdZaKHRtiNEhQURpbCmQ6tzGodA3+PF5S2U8yuxg79zz3BdRgKtWirrS+EIhqGDlrg2w3sdswBrzPxoHOEGgxlgc7s7uYr9RQ0OyAl9OnhQEtdgK1y3KQxObsO9IzTKU2lPPIvM1OcfmNjaq/2Eilb2hrP6CX3iqVdLbvWRJCody3daj6tPjo3w6i8YwGcbzc8EByTGNKujlGH43gVxlkP53M3XWxpKS7wx1btDKy2xoQkmHFulj8emteek1o7f3UpIpo5Vq1bRo0cPkpOTmT9/vsVx3333ne430+yQmce+39lo2yfTBzT3cIpA0crekPUHi7k8JY5EBbWbcwbvZjYumWrIwune6fnXYc0f8XduCS1DAnnl2r60s1EvRUdQgDCy6XdX0Mz+kW936JevSneoOKhiueG/m+wea8vh2rdTNFnaDOhu7awre5VKxb333svKlSvJyclh0aJFjfovAFRUVPDGG280quDaFOotFOhrGeJ1Lk4jFKvsDWf1N1/chZJzdT7RicoQW46uHc+MpmO0d4dcbrfQvCOjSwz/u3UAd17ajX9/k23UXD2mZbDFmPl6teSXPQ1moR4KUfaGBfqen9T8DmJKxpwSvDwllj8eGcHy+4YwOLlNk47Xs32UPsGuW6z1SdzmzZtJTk4mKSmJkJAQpk6dypIlSxqNe+qpp3j00UcJC2t+dmvyEysbbcv8z/BmH08peMWtav3BYnp1jGJQV2XZ9RxlygcbLe6b0CdecQ6r5vCqhXZ1T01IbZREdvfwbpSeq2PR5qOUnLPs1P3UwEGolA5BhgX6bhjkOwEEhphrr/jLnpP8ssd2j11zGBY7S2prfWZv2lshISGBTZuMnzK2bdtGfn4+48eP56WXXrJ4LCHEHcAdAJ0723etfMGioMiZ/cMGdaPvG9mdQ6cqfSqJCmBrnvXIk5lX9HSTJK7lr0Pmn17MZQu/m3nIrGPMkEfG9tBnTie1CydQAVUHDWe8r3t5GQRrzLOzZea0i7vYVYbb0NnraGixWq3moYce4pVXXrHnfS2W5Da0KOiY50CvZyWhyJn9N1sb6opsyyshLiqU8b19K7Jh8rvGbRXfvuFCZny5HdAUWfLGdGxT7Gko/uT4FBJiWhIUIAgMFAQFCE5V1LB270luHZzY6HM6WV7DgK6t2ZJXophaOIaP/Vdf6Ju2+h0F1sNnL0+JZe6k3sQZFAdL69CK201i1RPbNDQ7WX/QfmevaW+FgoICOnZs+KwrKirYtWsXw4cPB+DEiRMAyUKIDEcK+AFcr7D4/+aiOGX/hUFrvst6xrJ270keHtNDUfWtXcGEPh30yv6rOy72sDTO4YInG9s+dTw/qbdVc8c1/TThpkO7t+WPA8X0iIukS5uWrNx1nIFdNbbh7gqIxDGcnXqqUN306dNZtmwZsbGx7Nq1q9H+zMxMrrrqKrp21fu8mpx+PvFt63WbzlTW8tGfR8joEkNGYmtah4dwWc/YxrI8PIKK6joGzv2VN9dqisKF2vHbHjBgAAcOHODIkSN07NiRxYsX8+WXX+r3t2rViuLiYv368OHD+e233w42RdH3MPi+Zs8a7VMh3qBAZf/EDw1f1sx9J2kRHMiNPmYDNX1U/OWhS41Kxg70Md+EKfufu8Lum/en0weSPmcN12Yk0C4ylNU5RfyUrSm9cEGs552zF837Vb/sqUJ1t9xyCzNmzODmm2+2OGbo0KEsW7YMACFEkxrQlJgp5ZHWIYpXr0sn/8w5/s47w5bcEj5Zn8sHvx8GNHVuMroYf4/7JGh6DUSGBXNl33i+3qJ5gh+UZNuxGxQUxNtvv82YMWNQqVRMnz6dtLQ0Zs2aRUZGBhMnTmzKKZmlxuBJ1NcUPShM2f+237h/qVrC5P4dnVbDXYkMSW5LcmwEl760DkCfZOLtmLN96hj4/C/0iIukZ/tIerSPokf7CC6IiyTSjENaCMHvj4wgKiyIszX1hAQF6M1DzkyoWrVqFffffz9ALyHETCml5UBuA3T1XlbcN9RpsjSVYcOGkZub67LjX/jsGqP1TY+P1JtrerSP5PLUOEDbY7iwjL9zNcp/1e4TRq8LDBB8+MdhBiS25tqMTg3K3s7Jzbhx4xg3bpzRtjlz5pgdm5mZiRDC7mI7+QZhonufHWvvy7wKRSn7aR9vbrTNXZ2oEhMTiYyMJDAwkKCgILZs2cLDDz/MTz/9REhICN26deN///sf0dEaO/G8efP46KOPCAwMBLC7YWa3x42r6L1/U38A8rR2zIdGeW/tDUsMSW7Lq1P6cvhUJftOVLD3RAX7TpTz3bZCztY0mO06RrfQ3gA0fz3bR5HULlzfdjIyLJhh3dvpQy+dVQlUF8O9Zs0aunXrthu4XgixVErZOJDbgPkGDsvUDsrqmWrKhg0b6Nu3Lx06dACwOy7RtI9E7vzxFseGBQcyILE1A7TZsGq1ZN2+k2w6cobiihr+zjvDc8v3aMc2PNldlOT5J9mhL2omW20jQggLDvSwNK5BMcq+oKThzvrrvy9l5Cu/MbJnLEk2ki2cybp162jbtqFxwqhRo5g3bx5BQUE8+uijzJs3jxdeeIGcnBwWL17M7t27OXbsGElJSZ2FEIFSSut1X2mcZKTrrNQnoRWXp8QpIrrEUd789YB+eeX9Q/XNo2Mjw7jI4JFdSklBSRX7TlSwr6jhJvDb/lPUaz+n4EBBt3YR+htAi5CGH6Kz+n8axnADElgMXAVYVfbv/aZJiFN6DHa/fv3Iy8sjIiKCFStWsGrVKosNA0zDEg37SFhT9OYICBCMTIljZEqcfltReTVbckv4O/cMn/yVC0Dvjp51tBuaqTY+NtKDkrgWxSj7IS+s0y/HRYXRN6EV9420Hb7lSkaPHq1fvuiii/j2228BWLJkCVOnTiU0NFTn9KoBBgIbzB3HErtnj9EvL/XyEsaGbNQmi71/U3+9ojeHEIJOrVvSqXVLvSkANFE8h06dZb/+BlDBltwSlthRJrk5mMZwAwWAUQqmqRI8WVENaDJ6lR6DHRXVcA20ZhAhhGgrpSw2HSul/AD4ACAjI0M/wFlmqrioMMb3iWd8n3iemZjmlGM6yqPfaTKfk9oqq6ies1GMsl9+3xDGv/knB+deQVBgAEvcrPyEEIwePRohBHfeeSd33HGH0f6PP/6YKVOmABrlcNFFFxnurgXMxtyZKonPpg8ksY3GNKGUfqnO5oObMyiuqGm2EgwJCiAlPoqU+CiuMtheXl3H/hMVfLnpqNvL4ZoqwTbhobx/U38uvUAZpZWtceLECeLi4hBCsHmz3lRqV9zjTRd1IS4qVPFmKkeYfVUa7SJDeU7bOtJXUYy2SevQqsmPic7kzz//pGPHjpw8eZJRo0bRs2dPhg3TdI+fO3cuQUFB3HjjjU0+rqmSGOYFysFRIkKDXNJeLios2CU14U1juIEEoNDaawIDBGPS2jtVjuZy/fXXk5mZSXFxMQkJCcyePZu6Ok011bvuuotvv/2Wd999l6CgIFq0aAFwWNpZvvJZH1eAAPGtWjDXR0tcGKIYZe9pdAkasbGxTJo0ic2bNzNs2DA++eQTli1bxq+//qrP4DWjHEKwoRz8KBfDGG5AAFOBGzwrlf0sWrTI6v4ZM2YwY8YM/boQotLVMvlRHr5roGoClZWVVFRU6JdXr15Nr169WLVqFS+++CJLly6lZcuGjNaJEyeyePFiampqdAoiDGgcSuTHKzCM4QbSgK+llE3rjejHj8IRzmxG0GwhhDgFONr+pi3QyOFkJyGALkIhCDgJnAB6obkh6koangV0xVvaa98ToFpKaTOkwOA8HZHV3ehk7SKltGmDcuBaevozsfs8Tc7RHXI7+z2acy2dIYOjx2jq65t6Ld0hk7Pfw65rCQpR9s5ACLFFSpmhlON4+j2chbtk9fRn0tz3P1++L86QwdFjKOFzMMWbrr/fjOPHjx8/5wF+Ze/Hjx8/5wG+pOw/UNhxPP0ezsJdsnr6M2nu+58v3xdnyODoMZTwOZjiNdffZ2z2fvz48ePHMr40s/fjx48fPxbwK3s/fvz4OQ/wKmUvhMgVQuwUQmQJIbZot10rhNgthFALITJMxj8mhDgohNgnhBhj4zgvCSH2CiF2CCF+EEJE2zpOE+V02vGdjTl5Dfb9WwghhRBttetCCPGmVt4dQoh+Dr53JyHEOiFEjvY63u/I8Zr43mFCiM1CiGzte89uwmvHaq/XQSHETCfJY/azEEI8I4Qo1F6fLCHEOFvHaub7fyyEOCmEaNzuSrN/uBCizECOWfaeg8kYi98hO19vUw5nYOsaCyFuEUKcMpDjnwb7pgkhDmj/pjXz+K8ZHHu/EKLUYJ/KYN9Su05ISuk1f0Au0NZkWwrQA8gEMgy2pwLZQCjQFTgEBFo5zmggSLv8AvCCreM0UU6nHd8dn6t2eyfgZ7SJYNpt44CVaMoKXARscvC944F+2uVIYD+Q6qbzFkCEdjkY2ARcZMfrArXXKQlNQl62M2S29FkAzwD/ccPnMQzoB+yysH84sMzR62ntO2Tn623K4YTPwuY1Bm4B3jbz2tbAYe3/GO1yjCPfIeBfwMcG62ebek5eNbM3h5Ryj5Ryn5ldVwGLpZQ1UsojwEE0ZYgtHWe1lFKXKbsRTTGsJh/HU8d3Ea8Bj6Cp8a7jKuBTqWEjEC2EaHJPUx1SyuNSym3a5QpgDxYqiDob7Tmc1a4Ga//siVgYCByUUh6WUtbSUP/eUXk89llo3/N34IyDx7DnHCx+hzz9GRjgyDUeA6yRUp6RUpYAawDT9ldNPf71gPUiSDbwNmUvgdVCiK1CUzrYGh0Bw2plBTR8aWwdZzqamYet4zRXTkeP72waySuEuAoolFJmm4x1mbxCiETgQjQzbLcghAgUQmShKZGxRkppz3u7/JqZ+SxmaE0eHwshYpz5Xk3kYq3Za6UQwmpBeivX067Pz8b3wW45mom913iy9rp8K4TQNUWw57V2f4eEEF3QPPWvNdgcJoTYIoTYKIS42ubZ4H1VL4dIKQuFELHAGiHEXu1sxGnHEUI8gaYWzheukNNJx3c2jeQFHkdjenILQogI4DvgASllubveV2q6i6ULjQ/lByFELymlWZu1uzD9LIQQ7wLPorkpPwu8gmbC4G62oanFclbrN/gRMNthyNHraeP1dsvhYn4CFkkpa4QQdwILgctc8D5TgW+lcSe8LtrfbBKwVgixU0p5yNpBvGpmL6Us1P4/CfyAdXNHIRqbsw59jXJLxxFC3AJMAG6UWsOYteM0VU5nHd/ZmJH3UjQziWwhRK5Wpm1CiPaukFcIEYzmh/2FlPJ7R47VXKSUpcA6Gj9um8Nl18zcZyGlLJJSqqSUauC/eMjMJ6Us15m9pJQrgGChddwbYsf1tPr52Xq9vXI4iM1rLKU8LaWs0a5+CPS397V2jtExFRMTjsFv9jAaf+WFlk+l4UVe8QeEA5EGy38BYw32Z2LsoE3D2PF5GI1TxOxxtH85QDuT9zV7nKbK6azju/tz1W7PpcFBOx5j59pmB99fAJ8Cr3vgO9UOiNYutwD+ACbY8bog7XXqSoNzLc0J8pj9LIB4g+UH0fh4XPWZJGLZQduehkTMgWgqwIqmXk9r3yE7X29TDid8Djavscl1mQRs1C63Bo6gcc7GaJdbN+c7BPTU/v6EwbYYIFS73BY4gB0BAm79cTn44SdpP5BsYDfwhMGHXICmD2wR8LPBa55A4/HeB1xh4zgH0djQsrR/71k7TjPkdMrx3fW5mozJpUHZC2CBVt6dGNxgm/n+Q9CYJ3YYfDbj3HTufYDt2vfeBcxqwmvHoYkUOWTuM3PmZwF8pv2sdwBLDZWMkz+PRcBxoE77m7oNuAu4S7t/hvY7ko0myOCSJpyD4XEsfofsfL1NOZz0eTS6xsAcYKJ2eZ6BHOuAngavna79zR8Ebm3O8bXrzwDzTV53ifZzy9b+v82e8/GXS/Djx4+f8wCvstn78ePHj5/m4Vf2fvz48XMe4Ff2fvz48XMeoIg4+7Zt28rExERPi9Fstm7dWizt6APpzed5Ppwj2Hee58M5wvlxnufDOepQhLJPTExky5YttgcqFCGEXQ2Lvfk8z4dzBPvO83w4Rzg/zvN8OEcdfjOOHz9+/JwH+JV9MzlWWsW52nrbA72Y2no1R0+f87QYLudIcSUqtW+HIJeeq6X4bI3tgV6MlJJDp87aHujlbD5yhpLK2ia/zq/sm0H+mXNcMn8t//7atEaYd5Kfn8+IESNITU0lLS2NN954A4Bu//6aHv0H0717d4DuHi7A5RK25p1hxMuZ/H7glKdFcRkqtSR9zhqGv5TpaVFcysS31zPyld98+qb2d+4Zrnt/Axc+u6bJr1WEzd7bGPriOgAuT4nzsCTOISgoiFdeeYV+/fpRUVFB//79ORPdg/KN3xCW2JcDmZ8ghKgAZgKPelpeZ/LK6v20ahHMgMTWnhbFZTz2/Q4A0jtF2xjpvVTW1LOzsAyAthGhHpbGdVz73gYAbh2c2OTX+pV9E/nDYAY4uX+ClZHeQ3x8PPHxmpL0kZGRpKSk8N6KLZw7uInFP+oqMXMauBofUvZ/HSrmr0OneeDy7kSE+uZPQUrJ11sKAPhwWoaN0d7LjR9qqiDfLVwgiQAAIABJREFUMSzJw5K4jvd/ayhq+dT41Ca/3m/GaSI3fbQZgHtHdPOwJK4hNzeXPzf+TWiHHqgqS5k8tLduVx1g9lFGCHGHtrb2llOnvMMcIqXk1dX7CQ8J5JZLEj0tjst4bvkeAPp2iiYsONDD0riG6joVWfmajn2PXdHTw9I4h0/WHyFx5nIMy9nMW7lXvxwQIJp8TL+ybwK/7W9QZA+P8Y0vlSFnz55l8uTJBF5yKwGhLWnZWDmY9WJKKT+QUmZIKTPatbMr5Nfj/Ly7iC15JQzs2proliGeFsclSCn56M8jAHzxz0EelsZ13PHZVgCmZHRCiKYrQSXyzE85AJworwbgvkXb9ft+eWhYs47pm8+uLmLax5pZ/dQBnWyM9D7q6uqYPHky7dJHcrrdJQB07NCe48eP60w8wWi6OXkt1dXVDBs2jJqaGvYUltKyx2DWcSNHjhxh6tSpnD59GiBJCBEiNa3ivJo3fj0AQFK7cJ81U9Wr1PyunYQ9f01vG6O9A8OIovhWLQBYmn1Mvy05NrJZx/XP7O3kJ4MPe/7kPh6UxPlIKbntttvo2bMne9tdCkDvjq2YOHEiCxcu1A1rAyzxlIzOIDQ0lLVr1/LcpyuJv/VNqo5s5eNxrXj00Ud58MEHOXjwIGi6iN3mYVEdRkrJ679olP33d1/iYWlchy5YYmxaewKbYdpQIiNf+c1o/eklDY3Tnhyf0uzj+ubt3slIKfmXwWOUr7F+/Xo+++wz4rpcwJmqnwBY8PFbXHTDTK677jo++ugjgChgvifldBQhBC1bhvOvRb8h1fWgVhEeGszatWv58ssvdcN0juh3PSep4zz63Q79sq+aqdRqyfEyjZnjrRtsN2ryBgxt9BP7dgBg4YaGJNmr0pvf6tiv7O1g+c7j+uU9c+zpWuddDBkyhHqVmm6Pr6CDdtv48eMB+PXXXwEQQuyXUp7xkIhO47O/DnPsf/+ivuQ4M2bcS7du3YiOjiYoSP9TqMX9Dd+dji4Cx5dJenyFfjk40DeMFDqHOsBDoy7gslcy9esDu7amXWTzw0r9yt4GarVkxpcNs/oWIb4X0fBT9jGjJ5e2Eb45E6xXqXl62V463PoW6uqz7Nz+Hnv37rX9QjQRR8AdAJ07d3almA7z+A879ctH5o3zoCSuw3AG/M1dF3tQEueic6gDdGnTksOnKvXrNw5y7HvnG7dDF/Lx+oYPf8m9gz0oiWvYfayskYnq138P94wwLsZw1jQkrQsjRoxgw4YNlJaWUl+vL30RgpnGz94UcfTlpqP6ZV+JTjGl62MNs3pfSYjLLa40Wjc8R4DRqe0dOr5f2VtBpZZGCqKvD2YgTv1go9F6bGQorVoEe0ga11FTr+KjX7JRV2sjHVS1rFmzhpSUFEaMGMG3336rG+rVjuibtRFjAAfmXuFBSdzD+zf197QITmP4y5kW903ul+CwVcFvxrHCR38e1i/fP7K7ByVxDdV1KiqqjYu5/fbwCA9J41qufOtPVGfPULz8NZBqstuFM2XKdUyYMIHU1FSmTp3Kk08+CZrfxEceFrdZSCn1YYjgO3ZsQ6pqVaTMWqVfH5Pm2GxXKdSr1Fb3T7rQcTeSX9lboLZezfMrGuy5D466wIPSuAZdHLaOliGBPumTOHr6HPuLzhIS25UOt77J5SmxfDhtgH5/UlISmzdrZsRCiMNSSq+spGX42L/zmdEelMR1fL/dNx3Ppr9FQ+KiQrm4WxuH38P3bv1O4umlu/XLLaykmatUKoBUIcQy10vlPKSUvJt5yGhb1izfVBDDXlpntP7e//nOo7+O6jqV0XpkmO+Z4qSUPPFDQ8x57vzxHpTGuby19qDFfVend3RKDoFf2ZuhqlbFos0NTq49z1oOt9SWA65yvVTOZdORxlGUIUG+93VYtuOY0fqYtDiCfNC80fOpBtPGOzf286AkrmP9wdOeFsElHDxZYXX/1U4w4YBf2TdCrZYMfXGtXWMLCgpYvnw5QLFLhXIBpo7ZvVZuaN5KZU29Udgs+Oas/lSFsdWp3ocasQx6/hdm/6R5yv6/jzbpt/vSrP7yV3+3uK9n+0hS4qOc8j5+ZW/Aj9sLSXp8BcVnG8qiWItTfuCBB3jxxRetHlOJFSETZy5vtM0XKyI+tzzHaL1tRKhPhiIOmPuL0fq4Xr7htNx2tISi8hr+tz6XvNOVtl/ghdTUq6zud4ZjVodf2RvwwFdZjbZZUg7Lli0jNjaW/v2tzxSVFp992Ezbts2Pj/SAJK5lV2EZizbnG237+YGhHpLGdew9UW60/tb1F/qMmUr39HnLJYlcatBly5cSxd5Zd8jiPiEcK49gim98K5zA1rzGNuw/HrEchrh+/XqWLl1KYmIiQBJwmRDic1fJ5ywuMymyBBAbFeYBSVzDos1Hufnjzcz4clujfW18rIORlJKxr/9htG1873gPSeNc6lRqaus14YimNmtfejqzFoUzuFtb2rdy3m/TH3qpZfK7Gxpt69S6pcXx8+bNY968eYAmXA84IKX8P1fJ5wy2HS1ptO3z23yrzvlj3+80u92RaoFKZd0+44rTH9+S0aymFkrk1TX79ctXL1ivX173n+EekMY17Cwos7rfWY5ZHf6ZPXBS2yDAkG99qN4GaGZK17zzV6PtQ7q39YA0rmGxQQSVKY6mmiuBgyfP6pOm6lRqpn+yxWj/iB6xnhDLJZiGBevo2jbczZK4juvebzzB1BESFMBYJ/te/DN7YODzvzbaltG0ehsVUsoJThPIyajVkqveXt9ou6+0cNMx08Ksvmf7SDq3sfyU5i1c/qrGBJc7fzxfbMwz2rf4jot8xrxxoMh8KKIv9Zc9W1NPVZ1l5+wVvdo7veHMea/si8zM6meMSPaAJK7DsBSsIXde6jt9dA+ZcTzrGO0DKfVLsjS12Tq0CqPsXJ2+bZ2Oi5Icz7BUCv/8dIvZ7bcP9R1lv2Cd5SQqcL4JB1xoxhFCBAohtis9s3SQmVn9f8b08IAkzketltz5mfkfzsievvPIDzD2dfOxyqFBAVw/0PvbSN6/WBMp9tk/B1l16nk7tfVq8k6fM7svMiyIOhs1ZLwBc9nrpgxNdr551ZU2+/uBPTZHeZA1OUWNtoX6UBbp6pwift7d+BwBFvhQlmV1nYo6lflEolsHd9X38fRWys7V6ZcFxmW3wbcS4j43MU/pePrKVHo+tYp7vmgcZeVt/J3bOFDCkFsuSXRJ+KxLNJsQIgEYD3zoiuM7g7KqOm4387j495OXe0Aa1/Dwt9kW9/lSEpVhtIYhrVoEc7cPmKruW6zJAp4xItmoOJ8OX7qWc5blmN0+W2u2Sojx7hs3wG0L/7a6f3K/BJe8r6umsa8DjwAWn7k8nVnad/Zqs9sjnewU8RTHSqsalS/W8bsPlTGuV6nZe8K8Q++e4d1o1dL7C4L9po3AGZTUml/2GD+p+UqC0fGyKq5860+b456+Ms0N0riOkspai79LgPCQQHp1dE55BFOcruyFEBOAk1LKrdbGeTKz1Fy5AIBHx/b0mYiG//5x2OI+X4hM0TH4BfN1jNpHhTHtkkT3CuMCdKbG8JBAnlvW2Crqzd/Xsqo6Pt+Yh0ot+fCPI+wstB53/v09l7hJMtfxtg3H7D0jkl12TV0xsx8MTBRC5AKLUVhmqaH905S7h3v/Iz9oHLP/W59rdt9b11/YaNv06dOJjY2lV69e+m3PPPMMHTt2JD09nfT0dIBWrpHWMYrKzZeef2j0BT5h3tCZGq/pl8A+k5DEhdMHNhpfXV3NwIED6du3L2lpaTz99NMAHDlyhEGDBpGcnAyQJITwaKNhKSXzV+7lyR93sSanyKKtXkdsZCj9Ose4STrXoFZLox6z5rgqvYPL3t/pyl5K+ZiUMkFKmQhMBdYqKbO07xzz5htfYuMRy6Vgx5lJp7/llltYtWpVo+0PPvggWVlZZGVlAVifdnkAcyUuQJN44yq7pzs5W9PwuL80+1ij/ebmf6Ghoaxdu5bs7GyysrJYtWoVGzdu5NFHH+XBBx/k4MGDAPXAba6S2x72HK/QlxG/98tt1NRbj7L53UrpEm/h9wPWzdVhwQEkxLjuqdt3Qk8cZPfsMZ4WwWlYmtXfOjjRbBOEYcOG0bq19zVtNlfiAuDxcSlOafbgaR79bod+uazK8hOpIUIIIiIiAKirq6Ourg4hBGvXruUf//iHbthp4GrnSts0TlY05LeobJRkfmRsD594Srv7c+uRRK72R7hU2UspM5WUWbp69wmL+8J9xDFbdq7ObEgpwD3Dm5Ys9vbbb9OnTx+mT58OYPHX5gln+18HzbcQ6BEXyeUpvpFDsHzHcav7oyw0hlepVKSnpxMbG8uoUaPo1q0b0dHRBAXpv+O1gNmsHXddyzOVtbYHaWnq91aJHC+rspoxC+afup3JeTWzv+Mz8z5jXyrxq2v0YMqQ5La0i7S/6uPdd9/NoUOHyMrKIj4+HsBiZpInnO03fLjJ7Pa5k3p5tdNSx58HrPfDGdS1NX0TzLtRAgMDycrKoqCggM2bN7N3b+NwTUu461raq+x/eWiYy2RwJ88tt55yFBIYQCsLN29ncd4o+/Jqy4/BvlTi9/vthWa3/+uyps2O4uLiCAwMJCAggNtvvx1AMRWopDT/2J8aH9XUmkaKxbArkzmeGJ9i86YWHR3NiBEj2LBhA6WlpdTX630AIYD5L4qbsEfZhwUHkBwb6QZpXEudSm3zKe3l6/q6XI7zRtn3eca8Y/anGUPcLInruN6k1aCOthGhDOzaNCV4/HjDl/OHH34ABfXZvfDZNWa3vzE13c2SuAbT0sWmtAkPoU9CtNl9p06dorS0FICqqirWrFlDSkoKI0aM4Ntvv9UfAljiPImbTsk528p+5zO+4Ueba2NWDzDWDfWbfMNQ7QC9LTwKexsnyqrZcNh8FM59I63H7l5//fVkZmZSXFxMQkICs2fPJjMzk6ysLIQQugYt+RYP4GZKzYTPpsRH0T3O+2eBALf+z3qG5Y/3Dra47/jx40ybNg2VSoVarea6665jwoQJpKamMnXqVJ588knQ/O4/cqrQTeRUhXVlPyAxhmAf6LhVU6/ik79ybY4LcUOZlvNC2U/7eLPZ7a9P8Y2ZoFotmfqB5drYtvpYLlq0qNG2/2/vzMOjqLI+/F4gYReM7GEJgQgk7CCIIlsmbCquoyIKCA7Ch467xh035JMPBccVlxHRQUV0VAQUBFwQBGSXLSyRgBhQEAgIJOn7/dHdSaW7qrvTXVVdVdT7PHnSqa7ce2rp27fOPed3Ro8uG5knhIgsHMRgxqtUoAJ4c0RXky0xhnCRKUM6NApZVKd9+/asXbs2aHtqaiorV3o/B0KIXVJK9QQFE/jrdHFQJnAgH97ijHoS2XPUZbeVvGKSTpX9vzrDcPRkYUm6eSCD2tlf+hZg1IxV5GooBV53XhNqVrG/ZADA8p1/qPo+z61fg0a17a+ZAjBvY2jf7pOXtw35vh14ep66/o2fFnWrO2KRfdfBAj7RWENToneREi0cP9iP0ngkvqVXKpUr2T929+jJQpZu0w6Ru7FHMxOtMZahr6uvSXwwxhmzQIDbZgXPyv08OLi14REbRiOl5N0V2hXFADwS3vhul+Yibl5eHn379iU9PZ2MjAymTZsGwKFDh8jKyiItLY2srCwIES5sNFJKxv9H+1r6uenCFNO+2Bw92L+/cg+rf1GXEx0eo26KMi0dyBBCPB5Tg1Hy5Ofas6TOTWuT0cgZaxLKJBwliZUqcHb1uGb+68bxU9oCWQAjL2hukiXG8cNO7exuP9USK/LUF1t4UKPyWKVKlZgyZQqbN29mxYoVvPTSS2zevJlJkyaRmZlJTk4OmZmZAHF7dJ+zZh9b9h8Nuc/IC1L4vSDyfINYcexgL6XULFPXrXkSyTE+9ivT0oHNwEAhxPkxNRoFs3/aq/neDec7Z1Y/ViNH4ofsfiZbYhw3z1AvNAPw6g2dTVnEMxqtgvB+OjSuxfXdmwJwSQf1JKOGDRvSubPXz12zZk3atGnDvn37+PTTTxkxYgSA/3dcxHQOHT/NPbO15cUBWtarwds/5NKolnlh3/a/ezRQ0xLxU96YczWUael4ZUoSgNCrazqzR8NPD3B2tQTDM/LMQkrJmj1/qr5Xp0bkiWJWRkqpGU0FMMABpRX3Hj7BnkPa9yzAqJ7N+b8vt9G9eRIXR3D/5ubmsnbtWrp3705+fr4/AZAGDRqARgCK0VnCkYRa7jhQQMcmtbm7v3lV8Rw72PvLuKlxYQt9Sn7509KBDsBCKWVQJoyRN9bEedo31TVdmzhCTwTgI42nl9UOKjTz7JfbNN/75H8ucMSCZZ/JS8Pus3bPnxz5q5DHLs0Ie8wFBQVcddVVTJ06lbPOKqsBH+p/jcwS/mHn78xZo/207ad2tQReGmbu05ojB/uCEL7PRy9Jp4JOIln+tHRgA9BNCBEUKmHUjVXskSwIofXjfxR2Avd+tEF1u1Nm9aeKijVrkrasV4NONpf2Bdi49whFYcJKXxjaiZkrfmFot6akNwpdwKOwsJCrrrqKYcOGceWVVwLerG9/MqDvd+hFEJ05WVjM3R+Gdt/4ee6aDjG7ksuLIwf7GRpJDFUTKnJVF0Okb4uBJYBpxUB/2KmtndL73Lo0O8cy6gYx8euf6om739zbx1xDDCRU8el/jzzPREuM4XSRh0tfDF+F6sNVeVRPrBjWtSGlZPTo0bRp04a77rqrZPuQIUOYMWMGgP+3uu/PIF5eupP9R9QDCZSM7d2Cfq3rm2BRWRw52E/WeCS+vFOybqFryrR0vD77LCByxakYeSlExRsnLczeqKER45Qvsz8KTjF1UY7m+6ESqOxCuOpM4BXq+37H79zdvxVJYaKrli1bxsyZM1m8eHFJcZ158+aRnZ3NwoULSUtLY9GiRQChkxZ05LcjJ3nha+3rqOSe/ucabI06jsugDRXudKOOg6AyLR1IB56RUs7VrYMQHDlRyIpd6oU7GtWqQr/WzpD4PV3kYefB40Hboy1Pl5eXx/Dhw8nPz0cIwZgxY7j99ts5dOgQ1157Lbm5uQBpQoizpZTqMbs6E2qgN7JqkVls3HskokFwW/4xWtWvybAI3I89e/bUFMP7+uuvS14LIUJrCutI5Qh978uy+1EpTjIQjhvstUK7ujY7O6wfsDwo09KFED9LKZ/QrfEwPL9ou+Z713dv6ojCHQAzNUrVRVuezh+f3blzZ44dO0aXLl3Iysri7bffJjMzk+zsbIQQx4Bs4P7oLY+MnPxjmscI9pfzOFVUzN2ztQMllBw8dopp13aM20AYK/M2RfYQYbafXomjBvuThcWsy1N30zkpk1RLWKlSBcE152nKztuOJ+cGJ4y9O7p71O01bNiwJDQvMD576dKl/t38VZwMH+yfDhFNNf3GLraPwJm6KIft+QUR7TswowEXtNQnSs5sTpwu4qFPNoXd7/lrjZcxDoU9v0Y1uEUj8aZOjcoMauuMmPM1e7S9C3/v2oR6NZ2hza9WvKNG5Ur0TNNnQNCKzwYKgaDVM71DaL/ZfjCkzEV/m8fVr9lzmNe+0V54VpJQUfDQxW0Mtsg4Mqd8E9F+l3cMLUhoNI4Z7Pcf+UtT8Gx4j2aOyD4EuPLlH1S316lRmeyBrU22xjjUindE66sPJFR8to8gh7CeIbRFxR4mfKZeUQxgxQP2rpx2srCYe2avp0GERYHG9m5h24Xo73N+jygCZ+fEwXF/UnPECCilpMczi1XfS61TnTG9Uk22yBhCKSJOGJJOrWr2Fsnys17FFZdatzrn6qBXHy4+G28mdOjqITEya1Ueu38PXnj208DEFHojmPzlNnYdPM6vEQyCAOP6tDDYImPweGTYimIAu58ZbIl1NEcM9vM2aicXPXNlO8dkkv7Pe+pa7pmt60WUWm4XLntpWdC298fELjsUSXw2BldxKiz28Mh/tf27W580LVXDEFbuPsRby3ZHvP+06zpSLdGeS4dan0cljw8JnwlsFo4Y7LUKWlzfvSndU88x2Rpj2J5/TPO9Jy53RpFtgNW5wSGlbRqepctaRCTx2cBZwKSYO9Mg3FWy88TkxOki7v1oPRpRkaoM6WDP8NITp4tCZrD7GW6hwBB7fqUqOHBU+1Exe5BzfNj9n/9Wdfujl6THNZxLb65+Nbji1sfj9PHVRxKfLYTYLqVUT2LQgfV7j2i/91h/o7o1hUnzt/JLCHG+QLqlJNl2khIqqdFPcu2qljo+28/stVbCp9/YhbMcUqEpVKLYiBh1+a2EWgRO8zrVqZpo39muEo9Hcv8cdZ0fwNaFSf4oOMU7y38ho9FZVEmIbFi5b6B5io96sutgAdO/3RV2v5mju5lgTeTYerAv9kiOaYie2T10TcnIf6vX0J17W09LLPzogdZi16K7esfBGmN44/td7DgQWdy53Uiqnsh7N3dnYEYDThZ6Ivofu0pePL8ohwoRzNhT69YIu4+Z2Hqw15L4tXvompLDx0+Tf1S9NnTbZH2qUI0aNYp69erRtm2paKfZJd6uUyk5WCWhgmO+zE4VFTNxnrZ00ppHsky0Rn+EEDRNqsaUhdrZ3YHUqWG/CmOFxR6Wbj3AqaLQX2hje1svwsjWg/2b36uv+ts9dE3JlIXqom6rHtJPy33kyJEsWLCgzDYzS7xJKVm5O9hNvvlxe0emKAm1aNmnVd2w4l924PMN2gWDAklveJal/NmR8tMvhzW9CUriJXYWCtsO9loJVBsn2HuRS0mxR7s4c92a+mm59+rVi6SkpDLbzCzxplVVTK+6A1bgr9PamlxOkDEGeHaBdgGWQFLq2DOJasm2yFIwrKjxYz2LImTEW+p+7JoOWZQFmKsxU1p8t/F+7EhLvEHsUgJqVcXsHm8eSKcnF6puf+Iy68Rhm4ld/fWvfaO+MHuRQsZjjk7RY3pjy8H+4DF1H/Z39/U12RJj0SqtaPbCT7jBKBYpAa1raed48/Kgp+x2PNmnUWRGi5Rz7DezD1Uw6LrzSqWZuzSzZmUxWw72WrN6u+prqKGVRPXKsM6m9G9WiTe1a6nneoQV0LqWVsqujJVVKmsuSkYqQoSbJlXjvJQk7Z0tSFGxh+tf15ZG8Cd2ZqWbX4EqUmw32BcVe9isEnf+9k3O8Hv6uetD9Vn9IJNkEcwo8SalVL2Weq5HWAGthLhICnXYhXdD6PID3DugNKb+2/v6Wi4sMRxqEh5++rYqfZqddp11axDYbrCfplH1pk8rc6sz5eXl0bdvX9LT0wEyhBC369X20ZOFbNoXPAjecL4xg8PQoUPp0aMH27Zto3Hjxrz55pumlHh778fgxecv/tlT727iyonT2g9EVlzEi5bVv2hLb0+9tiPVK3uXfJQDo50IVdx+iUKq2so6P9a1TIN/LQ5OU76tX0vT7VBWPRJCbAHGCyEWSimDK26Uk1c1ClA/MaSt6vZYmTVrlup2o0u8PawiCJbRSJ/cAavQe/JS1e3rHjUmrl6r9OKECRN4/fXX8a2ppAshBksp5+nR597D2hIJSdUTS8or/vz4AKradC0mQeOL+blrOnDXh+sBeGtkVzNNKje2GuyX7VBfILkry/yYVmXVI8ADbAGSgZgGe49H8rLKYJ9UPdFRoYh5h4IHiJdNWo8wi8PHT6suQDeqVYXa1YyJq9cqvQhw5513cs899yCE2KzXQA+EVPGccVO3knUJ/+zebuw9fIJFW/KDtr88rDPnK4QW+7W2rr8ebDbYD3sjeIGkbbIlkjMSgU5AeHHrMCxUuanA6+d0EmrSCIMdJNMM8J3G5OST8Rca1qdW6UWjkFKWcWMouaxjI9o1tveTmpSSRz8NLjRTJaECg9s1ZLxP5vgfFzU327RyY4jTUAjRRAixRAixWQjxsx7+7P1H1EO74h3TWlBQANACuENKGeRoL28MulZpxRo2nRWpUeyRQeqI4/taL708Vv45a63q9voRVnCKFWXpRYAXX3yR9u3bA6QIIVTjA8t7v4by1T9ySXpUdluJBZt+Y/HW4ESqnx72Pi194SsodHd/64u6GbVCVATcLaVMB87H68+O6cprVaKqXCl+PkB/1SPgkJTyY7V9yhODvvU3dXVLJ4mBAbygssh+7wDnyFGHYsEdF5nST2DpxXHjxrFz507WrVsH3jq7U9T+r7w5Ey9rSP0+fHGbkIuaduDYyUImfB48q7+ldyrVK1cqU1HNDnkhhgz2Usr9Uso1vtfHKPVnR0WBhhbFygfjJ3imrHoEqPteysnAqd+pbm9Zz15hauEIjKjq1txeMdeRsGCTemGL1g1Ua97qilbpxYoVK1KhQgWAg0DM+ru/F5xSdeEkVqrA8B4psTYfd6Z8tV1VhPDOv3nXCEf41GjfHGHthVk/hsd+CSFSiNGf3faxL1W31zPpcVgNZdUjvNEN64QQg6NtT8tN9dw1HaJt0pJs2hdcvOP9f8RectBqjH032B336g3GL0BrlV5U1NgFqA1or6pGyAer8lS3vz68K4mV7B1WumHvn8xYnktiQBTOP/u1pEpCRaSU/HmiEIDMNtZemPVjqCNYCFEDmIOKP1sIMQYYA9C0afnjxz/QoSZpLCirHvmiG2L6etdyU13ZuXEszVqOS/71fZm/u6UkOSrKCLyFPNQYYEKNBf8kpF27dnTs6E3wmThxIrNmzWLdunX+YIazgDtj6afYI5n8ZbDw2UVpdeh9rj1j6f0UFXt48JONSOnNiPX75QFu7ZcGwL+X5QLQsUnteJgYFYYN9kKIBLwD/Xtq/mwp5XRgOkDXrl01BWALi9V1o51SW9ZPn1Z1WRrwSPzs1e3jZI0xHFdxx8282VrVfPQg8AsNvNEaZkSNaZVeHDy49KFTCLFDShlTktxSDfXHJy8zJhfETN5Z/gub9h3l710aM/unvSXbb+3bsuSJxf8k/ooJT2t6YVQ0jgDeBLZIKZ+Cf7f/AAAMUUlEQVSLpa2vVUIRZznssV9KGTTQA1zTtUkcrDGOwJJ8LepWj+sCuxF4PJL9R4LrIt8/0FkL0K+o5IKM7d2ClDr2VLP0c+DoSaZ8tY1W9WsyYUhGmffuVOTzZA9qw5pHsmhYyz71n41yrF0I3Aj08/myo/Znj313TdC2Hi2cNatfsStYROoqh7lvpJTM3VB2MvnhLT3iZI1xvLUsuKDOBS3OcZQ0wp4/TqiGXN4ah0x2vTl8opCm51TnpWGdyVCsFY7plVqmalrFCsJ2BWcMceNIKb8HYn5m3XkwuF7nQ4PbxNqs5RiqUpLvmSvbxcES41ArUHKOzUPz1Hjqi+BSmdOH2yNaI1Le+zFY9GzK3zs4IhekVYOazL/9oiBXWLYDnswsPd2Y8FlwjOvNNshUKw9qUTjJtavaPpohkEBt/i/v6BUnS4xjxwF1KWMnDIJ+ThYW89q3ZQt4tEuuxRWdoo6stiTNHyhVkxjarakjgggsO6KcOF3Edzll080HZjSwgjSCrlzz2vKgbR+Nc5Z7Q+0LrVWDmnGwxFj+9lywlPG39zpL5mLexuB13acub+uIwdBP4Kx+4hX2X3QGC2vjvLM8+FFxqoW1oqPhdJGHvEPBA6GdFn0iITCs1C5JKOXhVJG6KGhTG1ZkCoVf4RHgik7JTL66vaPWI6DsrP6KTsmOmWBa8ipJKZk0f2uZbXVqVLZFSnJ5+I+K73PmaGeFIqoNgnZJQikPrR5eELTtDYf56gMT4h4Y3NpxA33grN5JSY2WnNmvVJQ4e/bq9ng8kiE+TWwnMeHzUjXkSzs0YsnWA1yUZu+ElEBunrG6zN8PDLL/Qlek/M3CJeqiQZk/MOHSdOrVjF8Gu1EMmlYqWfLezd0dM6sHiw72104vjU5xWqy5n7V7SkPXtjwxkKqJFVWTYexO4LrLmF6pcbLEONTkPK7u4qzQ2aMnC8v8fYNDCqUHsvW30kX2C1vWiaMl+mO5wf7Q8dMlr522uKXkipd/KHldNdHrnnLSLAJgdW7pE9qOpwdRQQjHHSOoC/VNdlj2c/sJX5W8njPuAse5b6BsEZZ4y7EYgeWu2KBppRENTlvc8nNY8YW2/rH+cbTEWK5+tTTSqFLFCo6K2PCjVLf0X8vLOjZy1Jda4BNnl2aqUvi2Z6aiaLrT5FjAYjN7j0eWSIrOvc1ZhaeVdHlqYcnrWlUT4miJcRw4VioZsP2pQXG0xFiU6pa1qiaQO+niOFpjDINfKPXVr3nEmNq58Wbm8tyS1x+NdVbosx9LzewfUjxGtU22dzkzLTweicc3Ufoxjnr8kZKSkkK7du3AK+O8Otz+fro9XVqs3GkJYn4OHC39Qtv65MA4WmIsW/aXCtbaQSJgwYIFtGrVCqCtECI7kv95RFF6sGuK8+orgMUG+1kr9wDwzihnhR8qGTVjVclrs8rTxcqSJUsAIpZxViqVbn5igEFWxZ9uE0u/0JwWFuznfd9nEmDXxKjLNZhGcXEx48ePZ/78+QA/A0PLUyVvjsMSGpVYZrBXZub1srkedij86pZf3+2sUoNK+kxeWvK6WqJ1PIWjRo2iXr16tG1bmhF56NAhsrKySEtLIysrCyCiUfvIidLolB1PO9dNlf3xRgAan13VFmsuK1eupGXLlqSmpgJI4H3gslD/s0oRSNClmTNn9WChwX7vYW8BaqdpuGvRoq49Sg0KIejfvz9AG1/BmQj+x/t7/aPWWnweOXIkCxaUTX6aNGkSmZmZ5OTkkJmZCRBRhZHDJ7yL7ELgyMgUPxelecMPv7+/X5wtiYx9+/bRpEmZcO29BJREDSyq7n8SdaqvvgQpZdx/unTpIqWUsqjYI+3CTTfdJOvWrSszMjIksFpGeJx2OkYppdy7d6+UUkpgHbAe6CUDjgtvxbHVwOqmTZtKKa17LXfv3i0zMjJK/j733HPlr7/+KqWU8tdff5XASenA+1VJee5Xj8cjPR77HOfs2bPl6NGjpZTe48Qrtf6iDHMtix1+LaWU1pnZA2X0oq2O2iwxEux0jADJySWToiLgE1QKVUspp0spu0opu9at63XB2eU48/PzadiwIQANGjQAjQi1wNkg2OcYY0HYLDciOTmZvLwytXEbA/vC/Z8dXFSxYqnB3k706tWLpCTn+vcAjh8/zrFjJRmFFYD+6FCo2qqEGtTUvtBcrMd5551HTk4Ou3fvBm9NjeuAz+JrlTVwB3sXTfLz8+nZsycdOnQAaAN8IaUs/+OMhalfvz7793uDA3y/g9NhXWxDpUqVePHFFxkwYABABvChlDK4MMYZiJAW0GMRQhwEgiUgI6cO8HvYvfTD318ikAYUSClVp3u+RU3/wmYrYFsU/RhJpH000zpGJTa4lv5r5h8AGgOVgZ14F2eTpJQhU7cVx6i3rWa1Z9S11Mt+vdoJe5wGXstI0KPPiK4lWGSwjxUhxGoZYQy4nv0JIVKAuVJKQ6obmHFcZp+7cBhpjxBiFtAH74csH3gM+C+w2/f3L8A1UsrgosAm2Gr19szqLx735JnQp3WCoF1cDEZKOVRtuxBiu5W+8FxcjMD12UeJb5a4HGglhNgrhBgdb5tcXFxctHDKzH662f1JKc3o0yl9lId42BNtn3rbavX2zOrPTveAbfp0hM/excXFxSU0rhvHxcXF5QzAHexdXFxczgBsM9gLIZoIIZYIITYLIX4WQtyusk8fIcQRIcQ638+jMfaZK4TY6GsrSMtdeHlBCLFDCLFBCNE5lv4U7YY9Vr0QQlQUQqwVQsw1qo8Qfb8lhDgghNik2JYkhFgohMjx/datLJLWeQ3Xp952RmtHmDarCCFWCiHW+9p83Le9uRDiR989+oEQImZB+hD2TxBC7FN8/sJqIptpdwgbBgohtvn6ikj/Poo+dDtnUROpiE68f4CGQGff65rAdiA9YJ8+eOPe9eozF6gT4v3BwHy8adnnAz+adaw6HuNdwH/0PG/l6LsX0BnYpNj2LJDte50N/K/R91C4PvW2M1o7wrQpgBq+1wnAj7578kPgOt/2V4FxBp7HCcA95WzLNLs1+q+IN6EuFW/C3XojPmt6nrNof2wzs5dS7pdSrvG9PgZsIUC6NA5cBrwjvawAagshGsbaqFnHKoRoDFwMvKF325EgpfwWCExgugyY4Xs9A7hcx/60zmvIPvW2M1o7wrQppZQFvj8TfD8S6Ad8FE2bUdgfTVum2a1BN2CHlHKXlPI0EejfR4MVxi/bDPZKhDdztRPeWUAgPXyPhPOFEBkxdiWBr4QQPwl1LfdkQCmxF6SdHSthjjVWpgL3AZ5wO5pIfSmlv5LNb0B9IzoJOK/R9KmLnTrYoWyrohBiHXAAWIh3xvqnlNKv92PG/Xmrz6X5VqRuqHjYrcDwz3AgepyzaLDdYC+EqAHMAe6QUh4NeHsNXq2IDsC/8KbCx0JPKWVnYBAwXgjRK8b2ykWYY4217UuAA1LKn8LuHCek95lX99jgUOc1mj6jtdMAO4qllB3x6v10A1qX16byoGL/K0ALoCOwH5gSSTtm2x1P9Dpn0WCrwV4IkYD3RL0npfw48H0p5VH/I6GUch6QIISoE21/Usp9vt8HUNdy3wcoy+JEpJ0dCeGOVQcuBIYIIXLxPrr2E0K8a0A/5SXf7wrz/T6gZ+Ma5zWaPmOyU0c7gpBS/gksAXrgdS36kycNvT+llPm+gdsDvI5K7YN4262CYZ/hQIw4Z+XBNoO9EEIAbwJbpJTPaezTwLcfQohueI/vjyj7qy6EqOl/jbqW+2fAcOHlfOCI4jE8aiI51liRUj4gpWwspUzBq/m9WEp5gxF9lZPPgBG+1yOAT/VqOMR5jabPqO3U2Q5/m3WFELV9r6sCWXj9wkuAq6NpM0RfqvYHrFddQQS1D8y0W4NVQJov+icRg/Tv9TxnUWPGKrAeP0BPvI+1G/CWyFuHNxpmLDDWt8+teKVr1wMrgAti6C/V1856X5sP+bYr+xPAS3h9jBuBrkYeq4Hntg/xicaZhffRtRCvr3Q0cA7wNZADLMIrOWz0PRSyT73tjNaOMG22B9b62twEPKq4j1cCO4DZQGUDz+NM3+dgA94Bs6GV7A5hw2C80TE7/Z9zA/rQ7ZxF++PKJbi4uLicAdjGjePi4uLiEj3uYO/i4uJyBuAO9i4uLi5nAO5g7+Li4nIG4A72Li4uLmcA7mDv4uLicgbgDvYuLi4uZwD/D6f5x5zzEXJxAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 12 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8O5QNsBGafq"
      },
      "source": [
        "plt.scatter(predictions[\"22\"],test_need[\"22\"])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}